schema {
  query: query_root
  mutation: mutation_root
  subscription: subscription_root
}

"""whether this query should be cached (Hasura Cloud only)"""
directive @cached(
  """measured in seconds"""
  ttl: Int! = 60

  """refresh the cache entry"""
  refresh: Boolean! = false
) on QUERY

"""
Boolean expression to compare columns of type "Boolean". All fields are combined with logical 'AND'.
"""
input Boolean_comparison_exp {
  _eq: Boolean
  _gt: Boolean
  _gte: Boolean
  _in: [Boolean!]
  _is_null: Boolean
  _lt: Boolean
  _lte: Boolean
  _neq: Boolean
  _nin: [Boolean!]
}

"""
Boolean expression to compare columns of type "Int". All fields are combined with logical 'AND'.
"""
input Int_comparison_exp {
  _eq: Int
  _gt: Int
  _gte: Int
  _in: [Int!]
  _is_null: Boolean
  _lt: Int
  _lte: Int
  _neq: Int
  _nin: [Int!]
}

"""
Boolean expression to compare columns of type "String". All fields are combined with logical 'AND'.
"""
input String_comparison_exp {
  _eq: String
  _gt: String
  _gte: String

  """does the column match the given case-insensitive pattern"""
  _ilike: String
  _in: [String!]

  """
  does the column match the given POSIX regular expression, case insensitive
  """
  _iregex: String
  _is_null: Boolean

  """does the column match the given pattern"""
  _like: String
  _lt: String
  _lte: String
  _neq: String

  """does the column NOT match the given case-insensitive pattern"""
  _nilike: String
  _nin: [String!]

  """
  does the column NOT match the given POSIX regular expression, case insensitive
  """
  _niregex: String

  """does the column NOT match the given pattern"""
  _nlike: String

  """
  does the column NOT match the given POSIX regular expression, case sensitive
  """
  _nregex: String

  """does the column NOT match the given SQL regular expression"""
  _nsimilar: String

  """
  does the column match the given POSIX regular expression, case sensitive
  """
  _regex: String

  """does the column match the given SQL regular expression"""
  _similar: String
}

"""
columns and relationships of "aggregator_event"
"""
type aggregator_event {
  id: bigint!
  level: Int!
  operation_hash: String!
  operation_timestamp: timestamptz
  tracker: String!
}

"""
aggregated selection of "aggregator_event"
"""
type aggregator_event_aggregate {
  aggregate: aggregator_event_aggregate_fields
  nodes: [aggregator_event!]!
}

"""
aggregate fields of "aggregator_event"
"""
type aggregator_event_aggregate_fields {
  avg: aggregator_event_avg_fields
  count(columns: [aggregator_event_select_column!], distinct: Boolean): Int!
  max: aggregator_event_max_fields
  min: aggregator_event_min_fields
  stddev: aggregator_event_stddev_fields
  stddev_pop: aggregator_event_stddev_pop_fields
  stddev_samp: aggregator_event_stddev_samp_fields
  sum: aggregator_event_sum_fields
  var_pop: aggregator_event_var_pop_fields
  var_samp: aggregator_event_var_samp_fields
  variance: aggregator_event_variance_fields
}

"""aggregate avg on columns"""
type aggregator_event_avg_fields {
  id: Float
  level: Float
}

"""
Boolean expression to filter rows from the table "aggregator_event". All fields are combined with a logical 'AND'.
"""
input aggregator_event_bool_exp {
  _and: [aggregator_event_bool_exp!]
  _not: aggregator_event_bool_exp
  _or: [aggregator_event_bool_exp!]
  id: bigint_comparison_exp
  level: Int_comparison_exp
  operation_hash: String_comparison_exp
  operation_timestamp: timestamptz_comparison_exp
  tracker: String_comparison_exp
}

"""
unique or primary key constraints on table "aggregator_event"
"""
enum aggregator_event_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  aggregator_event_pkey
}

"""
input type for incrementing numeric columns in table "aggregator_event"
"""
input aggregator_event_inc_input {
  id: bigint
  level: Int
}

"""
input type for inserting data into table "aggregator_event"
"""
input aggregator_event_insert_input {
  id: bigint
  level: Int
  operation_hash: String
  operation_timestamp: timestamptz
  tracker: String
}

"""aggregate max on columns"""
type aggregator_event_max_fields {
  id: bigint
  level: Int
  operation_hash: String
  operation_timestamp: timestamptz
  tracker: String
}

"""aggregate min on columns"""
type aggregator_event_min_fields {
  id: bigint
  level: Int
  operation_hash: String
  operation_timestamp: timestamptz
  tracker: String
}

"""
response of any mutation on the table "aggregator_event"
"""
type aggregator_event_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [aggregator_event!]!
}

"""
on_conflict condition type for table "aggregator_event"
"""
input aggregator_event_on_conflict {
  constraint: aggregator_event_constraint!
  update_columns: [aggregator_event_update_column!]! = []
  where: aggregator_event_bool_exp
}

"""Ordering options when selecting data from "aggregator_event"."""
input aggregator_event_order_by {
  id: order_by
  level: order_by
  operation_hash: order_by
  operation_timestamp: order_by
  tracker: order_by
}

"""primary key columns input for table: aggregator_event"""
input aggregator_event_pk_columns_input {
  id: bigint!
}

"""
select columns of table "aggregator_event"
"""
enum aggregator_event_select_column {
  """column name"""
  id

  """column name"""
  level

  """column name"""
  operation_hash

  """column name"""
  operation_timestamp

  """column name"""
  tracker
}

"""
input type for updating data in table "aggregator_event"
"""
input aggregator_event_set_input {
  id: bigint
  level: Int
  operation_hash: String
  operation_timestamp: timestamptz
  tracker: String
}

"""aggregate stddev on columns"""
type aggregator_event_stddev_fields {
  id: Float
  level: Float
}

"""aggregate stddev_pop on columns"""
type aggregator_event_stddev_pop_fields {
  id: Float
  level: Float
}

"""aggregate stddev_samp on columns"""
type aggregator_event_stddev_samp_fields {
  id: Float
  level: Float
}

"""
Streaming cursor of the table "aggregator_event"
"""
input aggregator_event_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: aggregator_event_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input aggregator_event_stream_cursor_value_input {
  id: bigint
  level: Int
  operation_hash: String
  operation_timestamp: timestamptz
  tracker: String
}

"""aggregate sum on columns"""
type aggregator_event_sum_fields {
  id: bigint
  level: Int
}

"""
update columns of table "aggregator_event"
"""
enum aggregator_event_update_column {
  """column name"""
  id

  """column name"""
  level

  """column name"""
  operation_hash

  """column name"""
  operation_timestamp

  """column name"""
  tracker
}

input aggregator_event_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: aggregator_event_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: aggregator_event_set_input

  """filter the rows which have to be updated"""
  where: aggregator_event_bool_exp!
}

"""aggregate var_pop on columns"""
type aggregator_event_var_pop_fields {
  id: Float
  level: Float
}

"""aggregate var_samp on columns"""
type aggregator_event_var_samp_fields {
  id: Float
  level: Float
}

"""aggregate variance on columns"""
type aggregator_event_variance_fields {
  id: Float
  level: Float
}

scalar bigint

"""
Boolean expression to compare columns of type "bigint". All fields are combined with logical 'AND'.
"""
input bigint_comparison_exp {
  _eq: bigint
  _gt: bigint
  _gte: bigint
  _in: [bigint!]
  _is_null: Boolean
  _lt: bigint
  _lte: bigint
  _neq: bigint
  _nin: [bigint!]
}

"""
columns and relationships of "collection"
"""
type collection {
  db_updated_at: timestamptz!
  id: String!
  minters(
    """JSON select path"""
    path: String
  ): jsonb!
  owner: String!
  standard: String!
  symbol: String
}

"""
aggregated selection of "collection"
"""
type collection_aggregate {
  aggregate: collection_aggregate_fields
  nodes: [collection!]!
}

"""
aggregate fields of "collection"
"""
type collection_aggregate_fields {
  count(columns: [collection_select_column!], distinct: Boolean): Int!
  max: collection_max_fields
  min: collection_min_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input collection_append_input {
  minters: jsonb
}

"""
Boolean expression to filter rows from the table "collection". All fields are combined with a logical 'AND'.
"""
input collection_bool_exp {
  _and: [collection_bool_exp!]
  _not: collection_bool_exp
  _or: [collection_bool_exp!]
  db_updated_at: timestamptz_comparison_exp
  id: String_comparison_exp
  minters: jsonb_comparison_exp
  owner: String_comparison_exp
  standard: String_comparison_exp
  symbol: String_comparison_exp
}

"""
unique or primary key constraints on table "collection"
"""
enum collection_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  collection_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input collection_delete_at_path_input {
  minters: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input collection_delete_elem_input {
  minters: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input collection_delete_key_input {
  minters: String
}

"""
input type for inserting data into table "collection"
"""
input collection_insert_input {
  db_updated_at: timestamptz
  id: String
  minters: jsonb
  owner: String
  standard: String
  symbol: String
}

"""aggregate max on columns"""
type collection_max_fields {
  db_updated_at: timestamptz
  id: String
  owner: String
  standard: String
  symbol: String
}

"""aggregate min on columns"""
type collection_min_fields {
  db_updated_at: timestamptz
  id: String
  owner: String
  standard: String
  symbol: String
}

"""
response of any mutation on the table "collection"
"""
type collection_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [collection!]!
}

"""
on_conflict condition type for table "collection"
"""
input collection_on_conflict {
  constraint: collection_constraint!
  update_columns: [collection_update_column!]! = []
  where: collection_bool_exp
}

"""Ordering options when selecting data from "collection"."""
input collection_order_by {
  db_updated_at: order_by
  id: order_by
  minters: order_by
  owner: order_by
  standard: order_by
  symbol: order_by
}

"""primary key columns input for table: collection"""
input collection_pk_columns_input {
  id: String!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input collection_prepend_input {
  minters: jsonb
}

"""
select columns of table "collection"
"""
enum collection_select_column {
  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  minters

  """column name"""
  owner

  """column name"""
  standard

  """column name"""
  symbol
}

"""
input type for updating data in table "collection"
"""
input collection_set_input {
  db_updated_at: timestamptz
  id: String
  minters: jsonb
  owner: String
  standard: String
  symbol: String
}

"""
Streaming cursor of the table "collection"
"""
input collection_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: collection_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input collection_stream_cursor_value_input {
  db_updated_at: timestamptz
  id: String
  minters: jsonb
  owner: String
  standard: String
  symbol: String
}

"""
update columns of table "collection"
"""
enum collection_update_column {
  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  minters

  """column name"""
  owner

  """column name"""
  standard

  """column name"""
  symbol
}

input collection_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: collection_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: collection_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: collection_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: collection_delete_key_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: collection_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: collection_set_input

  """filter the rows which have to be updated"""
  where: collection_bool_exp!
}

"""
columns and relationships of "collection_with_meta"
"""
type collection_with_meta {
  db_updated_at: timestamptz
  id: String
  metadata: String
  minters(
    """JSON select path"""
    path: String
  ): jsonb
  owner: String
  standard: String
  symbol: String
}

"""
aggregated selection of "collection_with_meta"
"""
type collection_with_meta_aggregate {
  aggregate: collection_with_meta_aggregate_fields
  nodes: [collection_with_meta!]!
}

"""
aggregate fields of "collection_with_meta"
"""
type collection_with_meta_aggregate_fields {
  count(columns: [collection_with_meta_select_column!], distinct: Boolean): Int!
  max: collection_with_meta_max_fields
  min: collection_with_meta_min_fields
}

"""
Boolean expression to filter rows from the table "collection_with_meta". All fields are combined with a logical 'AND'.
"""
input collection_with_meta_bool_exp {
  _and: [collection_with_meta_bool_exp!]
  _not: collection_with_meta_bool_exp
  _or: [collection_with_meta_bool_exp!]
  db_updated_at: timestamptz_comparison_exp
  id: String_comparison_exp
  metadata: String_comparison_exp
  minters: jsonb_comparison_exp
  owner: String_comparison_exp
  standard: String_comparison_exp
  symbol: String_comparison_exp
}

"""aggregate max on columns"""
type collection_with_meta_max_fields {
  db_updated_at: timestamptz
  id: String
  metadata: String
  owner: String
  standard: String
  symbol: String
}

"""aggregate min on columns"""
type collection_with_meta_min_fields {
  db_updated_at: timestamptz
  id: String
  metadata: String
  owner: String
  standard: String
  symbol: String
}

"""Ordering options when selecting data from "collection_with_meta"."""
input collection_with_meta_order_by {
  db_updated_at: order_by
  id: order_by
  metadata: order_by
  minters: order_by
  owner: order_by
  standard: order_by
  symbol: order_by
}

"""
select columns of table "collection_with_meta"
"""
enum collection_with_meta_select_column {
  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  minters

  """column name"""
  owner

  """column name"""
  standard

  """column name"""
  symbol
}

"""
Streaming cursor of the table "collection_with_meta"
"""
input collection_with_meta_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: collection_with_meta_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input collection_with_meta_stream_cursor_value_input {
  db_updated_at: timestamptz
  id: String
  metadata: String
  minters: jsonb
  owner: String
  standard: String
  symbol: String
}

"""ordering argument of a cursor"""
enum cursor_ordering {
  """ascending ordering of the cursor"""
  ASC

  """descending ordering of the cursor"""
  DESC
}

"""
columns and relationships of "dipdup_contract"
"""
type dipdup_contract {
  address: String!
  created_at: timestamptz!
  name: String!
  typename: String
  updated_at: timestamptz!
}

"""
aggregated selection of "dipdup_contract"
"""
type dipdup_contract_aggregate {
  aggregate: dipdup_contract_aggregate_fields
  nodes: [dipdup_contract!]!
}

"""
aggregate fields of "dipdup_contract"
"""
type dipdup_contract_aggregate_fields {
  count(columns: [dipdup_contract_select_column!], distinct: Boolean): Int!
  max: dipdup_contract_max_fields
  min: dipdup_contract_min_fields
}

"""
Boolean expression to filter rows from the table "dipdup_contract". All fields are combined with a logical 'AND'.
"""
input dipdup_contract_bool_exp {
  _and: [dipdup_contract_bool_exp!]
  _not: dipdup_contract_bool_exp
  _or: [dipdup_contract_bool_exp!]
  address: String_comparison_exp
  created_at: timestamptz_comparison_exp
  name: String_comparison_exp
  typename: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "dipdup_contract"
"""
enum dipdup_contract_constraint {
  """
  unique or primary key constraint on columns "name"
  """
  dipdup_contract_pkey
}

"""
input type for inserting data into table "dipdup_contract"
"""
input dipdup_contract_insert_input {
  address: String
  created_at: timestamptz
  name: String
  typename: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type dipdup_contract_max_fields {
  address: String
  created_at: timestamptz
  name: String
  typename: String
  updated_at: timestamptz
}

"""
columns and relationships of "dipdup_contract_metadata"
"""
type dipdup_contract_metadata {
  contract: String!
  created_at: timestamptz!
  id: Int!
  metadata(
    """JSON select path"""
    path: String
  ): jsonb!
  network: String!
  update_id: Int!
  updated_at: timestamptz!
}

"""
aggregated selection of "dipdup_contract_metadata"
"""
type dipdup_contract_metadata_aggregate {
  aggregate: dipdup_contract_metadata_aggregate_fields
  nodes: [dipdup_contract_metadata!]!
}

"""
aggregate fields of "dipdup_contract_metadata"
"""
type dipdup_contract_metadata_aggregate_fields {
  avg: dipdup_contract_metadata_avg_fields
  count(columns: [dipdup_contract_metadata_select_column!], distinct: Boolean): Int!
  max: dipdup_contract_metadata_max_fields
  min: dipdup_contract_metadata_min_fields
  stddev: dipdup_contract_metadata_stddev_fields
  stddev_pop: dipdup_contract_metadata_stddev_pop_fields
  stddev_samp: dipdup_contract_metadata_stddev_samp_fields
  sum: dipdup_contract_metadata_sum_fields
  var_pop: dipdup_contract_metadata_var_pop_fields
  var_samp: dipdup_contract_metadata_var_samp_fields
  variance: dipdup_contract_metadata_variance_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input dipdup_contract_metadata_append_input {
  metadata: jsonb
}

"""aggregate avg on columns"""
type dipdup_contract_metadata_avg_fields {
  id: Float
  update_id: Float
}

"""
Boolean expression to filter rows from the table "dipdup_contract_metadata". All fields are combined with a logical 'AND'.
"""
input dipdup_contract_metadata_bool_exp {
  _and: [dipdup_contract_metadata_bool_exp!]
  _not: dipdup_contract_metadata_bool_exp
  _or: [dipdup_contract_metadata_bool_exp!]
  contract: String_comparison_exp
  created_at: timestamptz_comparison_exp
  id: Int_comparison_exp
  metadata: jsonb_comparison_exp
  network: String_comparison_exp
  update_id: Int_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "dipdup_contract_metadata"
"""
enum dipdup_contract_metadata_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  dipdup_contract_metadata_pkey

  """
  unique or primary key constraint on columns "network", "contract"
  """
  uid_dipdup_cont_network_1ae32f
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input dipdup_contract_metadata_delete_at_path_input {
  metadata: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input dipdup_contract_metadata_delete_elem_input {
  metadata: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input dipdup_contract_metadata_delete_key_input {
  metadata: String
}

"""
input type for incrementing numeric columns in table "dipdup_contract_metadata"
"""
input dipdup_contract_metadata_inc_input {
  id: Int
  update_id: Int
}

"""
input type for inserting data into table "dipdup_contract_metadata"
"""
input dipdup_contract_metadata_insert_input {
  contract: String
  created_at: timestamptz
  id: Int
  metadata: jsonb
  network: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate max on columns"""
type dipdup_contract_metadata_max_fields {
  contract: String
  created_at: timestamptz
  id: Int
  network: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate min on columns"""
type dipdup_contract_metadata_min_fields {
  contract: String
  created_at: timestamptz
  id: Int
  network: String
  update_id: Int
  updated_at: timestamptz
}

"""
response of any mutation on the table "dipdup_contract_metadata"
"""
type dipdup_contract_metadata_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_contract_metadata!]!
}

"""
on_conflict condition type for table "dipdup_contract_metadata"
"""
input dipdup_contract_metadata_on_conflict {
  constraint: dipdup_contract_metadata_constraint!
  update_columns: [dipdup_contract_metadata_update_column!]! = []
  where: dipdup_contract_metadata_bool_exp
}

"""Ordering options when selecting data from "dipdup_contract_metadata"."""
input dipdup_contract_metadata_order_by {
  contract: order_by
  created_at: order_by
  id: order_by
  metadata: order_by
  network: order_by
  update_id: order_by
  updated_at: order_by
}

"""primary key columns input for table: dipdup_contract_metadata"""
input dipdup_contract_metadata_pk_columns_input {
  id: Int!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input dipdup_contract_metadata_prepend_input {
  metadata: jsonb
}

"""
select columns of table "dipdup_contract_metadata"
"""
enum dipdup_contract_metadata_select_column {
  """column name"""
  contract

  """column name"""
  created_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  network

  """column name"""
  update_id

  """column name"""
  updated_at
}

"""
input type for updating data in table "dipdup_contract_metadata"
"""
input dipdup_contract_metadata_set_input {
  contract: String
  created_at: timestamptz
  id: Int
  metadata: jsonb
  network: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type dipdup_contract_metadata_stddev_fields {
  id: Float
  update_id: Float
}

"""aggregate stddev_pop on columns"""
type dipdup_contract_metadata_stddev_pop_fields {
  id: Float
  update_id: Float
}

"""aggregate stddev_samp on columns"""
type dipdup_contract_metadata_stddev_samp_fields {
  id: Float
  update_id: Float
}

"""
Streaming cursor of the table "dipdup_contract_metadata"
"""
input dipdup_contract_metadata_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_contract_metadata_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_contract_metadata_stream_cursor_value_input {
  contract: String
  created_at: timestamptz
  id: Int
  metadata: jsonb
  network: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type dipdup_contract_metadata_sum_fields {
  id: Int
  update_id: Int
}

"""
update columns of table "dipdup_contract_metadata"
"""
enum dipdup_contract_metadata_update_column {
  """column name"""
  contract

  """column name"""
  created_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  network

  """column name"""
  update_id

  """column name"""
  updated_at
}

input dipdup_contract_metadata_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: dipdup_contract_metadata_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: dipdup_contract_metadata_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: dipdup_contract_metadata_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: dipdup_contract_metadata_delete_key_input

  """increments the numeric columns with given value of the filtered values"""
  _inc: dipdup_contract_metadata_inc_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: dipdup_contract_metadata_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_contract_metadata_set_input

  """filter the rows which have to be updated"""
  where: dipdup_contract_metadata_bool_exp!
}

"""aggregate var_pop on columns"""
type dipdup_contract_metadata_var_pop_fields {
  id: Float
  update_id: Float
}

"""aggregate var_samp on columns"""
type dipdup_contract_metadata_var_samp_fields {
  id: Float
  update_id: Float
}

"""aggregate variance on columns"""
type dipdup_contract_metadata_variance_fields {
  id: Float
  update_id: Float
}

"""aggregate min on columns"""
type dipdup_contract_min_fields {
  address: String
  created_at: timestamptz
  name: String
  typename: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "dipdup_contract"
"""
type dipdup_contract_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_contract!]!
}

"""
on_conflict condition type for table "dipdup_contract"
"""
input dipdup_contract_on_conflict {
  constraint: dipdup_contract_constraint!
  update_columns: [dipdup_contract_update_column!]! = []
  where: dipdup_contract_bool_exp
}

"""Ordering options when selecting data from "dipdup_contract"."""
input dipdup_contract_order_by {
  address: order_by
  created_at: order_by
  name: order_by
  typename: order_by
  updated_at: order_by
}

"""primary key columns input for table: dipdup_contract"""
input dipdup_contract_pk_columns_input {
  name: String!
}

"""
select columns of table "dipdup_contract"
"""
enum dipdup_contract_select_column {
  """column name"""
  address

  """column name"""
  created_at

  """column name"""
  name

  """column name"""
  typename

  """column name"""
  updated_at
}

"""
input type for updating data in table "dipdup_contract"
"""
input dipdup_contract_set_input {
  address: String
  created_at: timestamptz
  name: String
  typename: String
  updated_at: timestamptz
}

"""
Streaming cursor of the table "dipdup_contract"
"""
input dipdup_contract_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_contract_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_contract_stream_cursor_value_input {
  address: String
  created_at: timestamptz
  name: String
  typename: String
  updated_at: timestamptz
}

"""
update columns of table "dipdup_contract"
"""
enum dipdup_contract_update_column {
  """column name"""
  address

  """column name"""
  created_at

  """column name"""
  name

  """column name"""
  typename

  """column name"""
  updated_at
}

input dipdup_contract_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_contract_set_input

  """filter the rows which have to be updated"""
  where: dipdup_contract_bool_exp!
}

"""
columns and relationships of "dipdup_head"
"""
type dipdup_head {
  created_at: timestamptz!
  hash: String!
  level: Int!
  name: String!
  timestamp: timestamptz!
  updated_at: timestamptz!
}

"""
aggregated selection of "dipdup_head"
"""
type dipdup_head_aggregate {
  aggregate: dipdup_head_aggregate_fields
  nodes: [dipdup_head!]!
}

"""
aggregate fields of "dipdup_head"
"""
type dipdup_head_aggregate_fields {
  avg: dipdup_head_avg_fields
  count(columns: [dipdup_head_select_column!], distinct: Boolean): Int!
  max: dipdup_head_max_fields
  min: dipdup_head_min_fields
  stddev: dipdup_head_stddev_fields
  stddev_pop: dipdup_head_stddev_pop_fields
  stddev_samp: dipdup_head_stddev_samp_fields
  sum: dipdup_head_sum_fields
  var_pop: dipdup_head_var_pop_fields
  var_samp: dipdup_head_var_samp_fields
  variance: dipdup_head_variance_fields
}

"""aggregate avg on columns"""
type dipdup_head_avg_fields {
  level: Float
}

"""
Boolean expression to filter rows from the table "dipdup_head". All fields are combined with a logical 'AND'.
"""
input dipdup_head_bool_exp {
  _and: [dipdup_head_bool_exp!]
  _not: dipdup_head_bool_exp
  _or: [dipdup_head_bool_exp!]
  created_at: timestamptz_comparison_exp
  hash: String_comparison_exp
  level: Int_comparison_exp
  name: String_comparison_exp
  timestamp: timestamptz_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "dipdup_head"
"""
enum dipdup_head_constraint {
  """
  unique or primary key constraint on columns "name"
  """
  dipdup_head_pkey
}

"""
input type for incrementing numeric columns in table "dipdup_head"
"""
input dipdup_head_inc_input {
  level: Int
}

"""
input type for inserting data into table "dipdup_head"
"""
input dipdup_head_insert_input {
  created_at: timestamptz
  hash: String
  level: Int
  name: String
  timestamp: timestamptz
  updated_at: timestamptz
}

"""aggregate max on columns"""
type dipdup_head_max_fields {
  created_at: timestamptz
  hash: String
  level: Int
  name: String
  timestamp: timestamptz
  updated_at: timestamptz
}

"""aggregate min on columns"""
type dipdup_head_min_fields {
  created_at: timestamptz
  hash: String
  level: Int
  name: String
  timestamp: timestamptz
  updated_at: timestamptz
}

"""
response of any mutation on the table "dipdup_head"
"""
type dipdup_head_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_head!]!
}

"""
on_conflict condition type for table "dipdup_head"
"""
input dipdup_head_on_conflict {
  constraint: dipdup_head_constraint!
  update_columns: [dipdup_head_update_column!]! = []
  where: dipdup_head_bool_exp
}

"""Ordering options when selecting data from "dipdup_head"."""
input dipdup_head_order_by {
  created_at: order_by
  hash: order_by
  level: order_by
  name: order_by
  timestamp: order_by
  updated_at: order_by
}

"""primary key columns input for table: dipdup_head"""
input dipdup_head_pk_columns_input {
  name: String!
}

"""
select columns of table "dipdup_head"
"""
enum dipdup_head_select_column {
  """column name"""
  created_at

  """column name"""
  hash

  """column name"""
  level

  """column name"""
  name

  """column name"""
  timestamp

  """column name"""
  updated_at
}

"""
input type for updating data in table "dipdup_head"
"""
input dipdup_head_set_input {
  created_at: timestamptz
  hash: String
  level: Int
  name: String
  timestamp: timestamptz
  updated_at: timestamptz
}

"""
columns and relationships of "dipdup_head_status"
"""
type dipdup_head_status {
  name: String
  status: String
}

"""
aggregated selection of "dipdup_head_status"
"""
type dipdup_head_status_aggregate {
  aggregate: dipdup_head_status_aggregate_fields
  nodes: [dipdup_head_status!]!
}

"""
aggregate fields of "dipdup_head_status"
"""
type dipdup_head_status_aggregate_fields {
  count(columns: [dipdup_head_status_select_column!], distinct: Boolean): Int!
  max: dipdup_head_status_max_fields
  min: dipdup_head_status_min_fields
}

"""
Boolean expression to filter rows from the table "dipdup_head_status". All fields are combined with a logical 'AND'.
"""
input dipdup_head_status_bool_exp {
  _and: [dipdup_head_status_bool_exp!]
  _not: dipdup_head_status_bool_exp
  _or: [dipdup_head_status_bool_exp!]
  name: String_comparison_exp
  status: String_comparison_exp
}

"""
input type for inserting data into table "dipdup_head_status"
"""
input dipdup_head_status_insert_input {
  name: String
  status: String
}

"""aggregate max on columns"""
type dipdup_head_status_max_fields {
  name: String
  status: String
}

"""aggregate min on columns"""
type dipdup_head_status_min_fields {
  name: String
  status: String
}

"""
response of any mutation on the table "dipdup_head_status"
"""
type dipdup_head_status_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_head_status!]!
}

"""Ordering options when selecting data from "dipdup_head_status"."""
input dipdup_head_status_order_by {
  name: order_by
  status: order_by
}

"""
select columns of table "dipdup_head_status"
"""
enum dipdup_head_status_select_column {
  """column name"""
  name

  """column name"""
  status
}

"""
input type for updating data in table "dipdup_head_status"
"""
input dipdup_head_status_set_input {
  name: String
  status: String
}

"""
Streaming cursor of the table "dipdup_head_status"
"""
input dipdup_head_status_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_head_status_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_head_status_stream_cursor_value_input {
  name: String
  status: String
}

input dipdup_head_status_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_head_status_set_input

  """filter the rows which have to be updated"""
  where: dipdup_head_status_bool_exp!
}

"""aggregate stddev on columns"""
type dipdup_head_stddev_fields {
  level: Float
}

"""aggregate stddev_pop on columns"""
type dipdup_head_stddev_pop_fields {
  level: Float
}

"""aggregate stddev_samp on columns"""
type dipdup_head_stddev_samp_fields {
  level: Float
}

"""
Streaming cursor of the table "dipdup_head"
"""
input dipdup_head_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_head_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_head_stream_cursor_value_input {
  created_at: timestamptz
  hash: String
  level: Int
  name: String
  timestamp: timestamptz
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type dipdup_head_sum_fields {
  level: Int
}

"""
update columns of table "dipdup_head"
"""
enum dipdup_head_update_column {
  """column name"""
  created_at

  """column name"""
  hash

  """column name"""
  level

  """column name"""
  name

  """column name"""
  timestamp

  """column name"""
  updated_at
}

input dipdup_head_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: dipdup_head_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_head_set_input

  """filter the rows which have to be updated"""
  where: dipdup_head_bool_exp!
}

"""aggregate var_pop on columns"""
type dipdup_head_var_pop_fields {
  level: Float
}

"""aggregate var_samp on columns"""
type dipdup_head_var_samp_fields {
  level: Float
}

"""aggregate variance on columns"""
type dipdup_head_variance_fields {
  level: Float
}

"""
columns and relationships of "dipdup_index"
"""
type dipdup_index {
  config_hash: String!
  created_at: timestamptz!
  level: Int!
  name: String!

  """
  NEW: NEW\nSYNCING: SYNCING\nREALTIME: REALTIME\nROLLBACK: ROLLBACK\nONESHOT: ONESHOT
  """
  status: String!
  template: String
  template_values(
    """JSON select path"""
    path: String
  ): jsonb

  """
  operation: operation\nbig_map: big_map\nhead: head\ntoken_transfer: token_transfer\nevent: event
  """
  type: String!
  updated_at: timestamptz!
}

"""
aggregated selection of "dipdup_index"
"""
type dipdup_index_aggregate {
  aggregate: dipdup_index_aggregate_fields
  nodes: [dipdup_index!]!
}

"""
aggregate fields of "dipdup_index"
"""
type dipdup_index_aggregate_fields {
  avg: dipdup_index_avg_fields
  count(columns: [dipdup_index_select_column!], distinct: Boolean): Int!
  max: dipdup_index_max_fields
  min: dipdup_index_min_fields
  stddev: dipdup_index_stddev_fields
  stddev_pop: dipdup_index_stddev_pop_fields
  stddev_samp: dipdup_index_stddev_samp_fields
  sum: dipdup_index_sum_fields
  var_pop: dipdup_index_var_pop_fields
  var_samp: dipdup_index_var_samp_fields
  variance: dipdup_index_variance_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input dipdup_index_append_input {
  template_values: jsonb
}

"""aggregate avg on columns"""
type dipdup_index_avg_fields {
  level: Float
}

"""
Boolean expression to filter rows from the table "dipdup_index". All fields are combined with a logical 'AND'.
"""
input dipdup_index_bool_exp {
  _and: [dipdup_index_bool_exp!]
  _not: dipdup_index_bool_exp
  _or: [dipdup_index_bool_exp!]
  config_hash: String_comparison_exp
  created_at: timestamptz_comparison_exp
  level: Int_comparison_exp
  name: String_comparison_exp
  status: String_comparison_exp
  template: String_comparison_exp
  template_values: jsonb_comparison_exp
  type: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "dipdup_index"
"""
enum dipdup_index_constraint {
  """
  unique or primary key constraint on columns "name"
  """
  dipdup_index_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input dipdup_index_delete_at_path_input {
  template_values: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input dipdup_index_delete_elem_input {
  template_values: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input dipdup_index_delete_key_input {
  template_values: String
}

"""
input type for incrementing numeric columns in table "dipdup_index"
"""
input dipdup_index_inc_input {
  level: Int
}

"""
input type for inserting data into table "dipdup_index"
"""
input dipdup_index_insert_input {
  config_hash: String
  created_at: timestamptz
  level: Int
  name: String

  """
  NEW: NEW\nSYNCING: SYNCING\nREALTIME: REALTIME\nROLLBACK: ROLLBACK\nONESHOT: ONESHOT
  """
  status: String
  template: String
  template_values: jsonb

  """
  operation: operation\nbig_map: big_map\nhead: head\ntoken_transfer: token_transfer\nevent: event
  """
  type: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type dipdup_index_max_fields {
  config_hash: String
  created_at: timestamptz
  level: Int
  name: String

  """
  NEW: NEW\nSYNCING: SYNCING\nREALTIME: REALTIME\nROLLBACK: ROLLBACK\nONESHOT: ONESHOT
  """
  status: String
  template: String

  """
  operation: operation\nbig_map: big_map\nhead: head\ntoken_transfer: token_transfer\nevent: event
  """
  type: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type dipdup_index_min_fields {
  config_hash: String
  created_at: timestamptz
  level: Int
  name: String

  """
  NEW: NEW\nSYNCING: SYNCING\nREALTIME: REALTIME\nROLLBACK: ROLLBACK\nONESHOT: ONESHOT
  """
  status: String
  template: String

  """
  operation: operation\nbig_map: big_map\nhead: head\ntoken_transfer: token_transfer\nevent: event
  """
  type: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "dipdup_index"
"""
type dipdup_index_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_index!]!
}

"""
on_conflict condition type for table "dipdup_index"
"""
input dipdup_index_on_conflict {
  constraint: dipdup_index_constraint!
  update_columns: [dipdup_index_update_column!]! = []
  where: dipdup_index_bool_exp
}

"""Ordering options when selecting data from "dipdup_index"."""
input dipdup_index_order_by {
  config_hash: order_by
  created_at: order_by
  level: order_by
  name: order_by
  status: order_by
  template: order_by
  template_values: order_by
  type: order_by
  updated_at: order_by
}

"""primary key columns input for table: dipdup_index"""
input dipdup_index_pk_columns_input {
  name: String!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input dipdup_index_prepend_input {
  template_values: jsonb
}

"""
select columns of table "dipdup_index"
"""
enum dipdup_index_select_column {
  """column name"""
  config_hash

  """column name"""
  created_at

  """column name"""
  level

  """column name"""
  name

  """column name"""
  status

  """column name"""
  template

  """column name"""
  template_values

  """column name"""
  type

  """column name"""
  updated_at
}

"""
input type for updating data in table "dipdup_index"
"""
input dipdup_index_set_input {
  config_hash: String
  created_at: timestamptz
  level: Int
  name: String

  """
  NEW: NEW\nSYNCING: SYNCING\nREALTIME: REALTIME\nROLLBACK: ROLLBACK\nONESHOT: ONESHOT
  """
  status: String
  template: String
  template_values: jsonb

  """
  operation: operation\nbig_map: big_map\nhead: head\ntoken_transfer: token_transfer\nevent: event
  """
  type: String
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type dipdup_index_stddev_fields {
  level: Float
}

"""aggregate stddev_pop on columns"""
type dipdup_index_stddev_pop_fields {
  level: Float
}

"""aggregate stddev_samp on columns"""
type dipdup_index_stddev_samp_fields {
  level: Float
}

"""
Streaming cursor of the table "dipdup_index"
"""
input dipdup_index_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_index_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_index_stream_cursor_value_input {
  config_hash: String
  created_at: timestamptz
  level: Int
  name: String

  """
  NEW: NEW\nSYNCING: SYNCING\nREALTIME: REALTIME\nROLLBACK: ROLLBACK\nONESHOT: ONESHOT
  """
  status: String
  template: String
  template_values: jsonb

  """
  operation: operation\nbig_map: big_map\nhead: head\ntoken_transfer: token_transfer\nevent: event
  """
  type: String
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type dipdup_index_sum_fields {
  level: Int
}

"""
update columns of table "dipdup_index"
"""
enum dipdup_index_update_column {
  """column name"""
  config_hash

  """column name"""
  created_at

  """column name"""
  level

  """column name"""
  name

  """column name"""
  status

  """column name"""
  template

  """column name"""
  template_values

  """column name"""
  type

  """column name"""
  updated_at
}

input dipdup_index_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: dipdup_index_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: dipdup_index_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: dipdup_index_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: dipdup_index_delete_key_input

  """increments the numeric columns with given value of the filtered values"""
  _inc: dipdup_index_inc_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: dipdup_index_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_index_set_input

  """filter the rows which have to be updated"""
  where: dipdup_index_bool_exp!
}

"""aggregate var_pop on columns"""
type dipdup_index_var_pop_fields {
  level: Float
}

"""aggregate var_samp on columns"""
type dipdup_index_var_samp_fields {
  level: Float
}

"""aggregate variance on columns"""
type dipdup_index_variance_fields {
  level: Float
}

"""Model update created within versioned transactions"""
type dipdup_model_update {
  """INSERT: INSERT\nUPDATE: UPDATE\nDELETE: DELETE"""
  action: String!
  created_at: timestamptz!
  data(
    """JSON select path"""
    path: String
  ): jsonb
  id: Int!
  index: String!
  level: Int!
  model_name: String!
  model_pk: String!
  updated_at: timestamptz!
}

"""
aggregated selection of "dipdup_model_update"
"""
type dipdup_model_update_aggregate {
  aggregate: dipdup_model_update_aggregate_fields
  nodes: [dipdup_model_update!]!
}

"""
aggregate fields of "dipdup_model_update"
"""
type dipdup_model_update_aggregate_fields {
  avg: dipdup_model_update_avg_fields
  count(columns: [dipdup_model_update_select_column!], distinct: Boolean): Int!
  max: dipdup_model_update_max_fields
  min: dipdup_model_update_min_fields
  stddev: dipdup_model_update_stddev_fields
  stddev_pop: dipdup_model_update_stddev_pop_fields
  stddev_samp: dipdup_model_update_stddev_samp_fields
  sum: dipdup_model_update_sum_fields
  var_pop: dipdup_model_update_var_pop_fields
  var_samp: dipdup_model_update_var_samp_fields
  variance: dipdup_model_update_variance_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input dipdup_model_update_append_input {
  data: jsonb
}

"""aggregate avg on columns"""
type dipdup_model_update_avg_fields {
  id: Float
  level: Float
}

"""
Boolean expression to filter rows from the table "dipdup_model_update". All fields are combined with a logical 'AND'.
"""
input dipdup_model_update_bool_exp {
  _and: [dipdup_model_update_bool_exp!]
  _not: dipdup_model_update_bool_exp
  _or: [dipdup_model_update_bool_exp!]
  action: String_comparison_exp
  created_at: timestamptz_comparison_exp
  data: jsonb_comparison_exp
  id: Int_comparison_exp
  index: String_comparison_exp
  level: Int_comparison_exp
  model_name: String_comparison_exp
  model_pk: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "dipdup_model_update"
"""
enum dipdup_model_update_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  dipdup_model_update_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input dipdup_model_update_delete_at_path_input {
  data: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input dipdup_model_update_delete_elem_input {
  data: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input dipdup_model_update_delete_key_input {
  data: String
}

"""
input type for incrementing numeric columns in table "dipdup_model_update"
"""
input dipdup_model_update_inc_input {
  id: Int
  level: Int
}

"""
input type for inserting data into table "dipdup_model_update"
"""
input dipdup_model_update_insert_input {
  """INSERT: INSERT\nUPDATE: UPDATE\nDELETE: DELETE"""
  action: String
  created_at: timestamptz
  data: jsonb
  id: Int
  index: String
  level: Int
  model_name: String
  model_pk: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type dipdup_model_update_max_fields {
  """INSERT: INSERT\nUPDATE: UPDATE\nDELETE: DELETE"""
  action: String
  created_at: timestamptz
  id: Int
  index: String
  level: Int
  model_name: String
  model_pk: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type dipdup_model_update_min_fields {
  """INSERT: INSERT\nUPDATE: UPDATE\nDELETE: DELETE"""
  action: String
  created_at: timestamptz
  id: Int
  index: String
  level: Int
  model_name: String
  model_pk: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "dipdup_model_update"
"""
type dipdup_model_update_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_model_update!]!
}

"""
on_conflict condition type for table "dipdup_model_update"
"""
input dipdup_model_update_on_conflict {
  constraint: dipdup_model_update_constraint!
  update_columns: [dipdup_model_update_update_column!]! = []
  where: dipdup_model_update_bool_exp
}

"""Ordering options when selecting data from "dipdup_model_update"."""
input dipdup_model_update_order_by {
  action: order_by
  created_at: order_by
  data: order_by
  id: order_by
  index: order_by
  level: order_by
  model_name: order_by
  model_pk: order_by
  updated_at: order_by
}

"""primary key columns input for table: dipdup_model_update"""
input dipdup_model_update_pk_columns_input {
  id: Int!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input dipdup_model_update_prepend_input {
  data: jsonb
}

"""
select columns of table "dipdup_model_update"
"""
enum dipdup_model_update_select_column {
  """column name"""
  action

  """column name"""
  created_at

  """column name"""
  data

  """column name"""
  id

  """column name"""
  index

  """column name"""
  level

  """column name"""
  model_name

  """column name"""
  model_pk

  """column name"""
  updated_at
}

"""
input type for updating data in table "dipdup_model_update"
"""
input dipdup_model_update_set_input {
  """INSERT: INSERT\nUPDATE: UPDATE\nDELETE: DELETE"""
  action: String
  created_at: timestamptz
  data: jsonb
  id: Int
  index: String
  level: Int
  model_name: String
  model_pk: String
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type dipdup_model_update_stddev_fields {
  id: Float
  level: Float
}

"""aggregate stddev_pop on columns"""
type dipdup_model_update_stddev_pop_fields {
  id: Float
  level: Float
}

"""aggregate stddev_samp on columns"""
type dipdup_model_update_stddev_samp_fields {
  id: Float
  level: Float
}

"""
Streaming cursor of the table "dipdup_model_update"
"""
input dipdup_model_update_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_model_update_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_model_update_stream_cursor_value_input {
  """INSERT: INSERT\nUPDATE: UPDATE\nDELETE: DELETE"""
  action: String
  created_at: timestamptz
  data: jsonb
  id: Int
  index: String
  level: Int
  model_name: String
  model_pk: String
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type dipdup_model_update_sum_fields {
  id: Int
  level: Int
}

"""
update columns of table "dipdup_model_update"
"""
enum dipdup_model_update_update_column {
  """column name"""
  action

  """column name"""
  created_at

  """column name"""
  data

  """column name"""
  id

  """column name"""
  index

  """column name"""
  level

  """column name"""
  model_name

  """column name"""
  model_pk

  """column name"""
  updated_at
}

input dipdup_model_update_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: dipdup_model_update_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: dipdup_model_update_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: dipdup_model_update_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: dipdup_model_update_delete_key_input

  """increments the numeric columns with given value of the filtered values"""
  _inc: dipdup_model_update_inc_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: dipdup_model_update_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_model_update_set_input

  """filter the rows which have to be updated"""
  where: dipdup_model_update_bool_exp!
}

"""aggregate var_pop on columns"""
type dipdup_model_update_var_pop_fields {
  id: Float
  level: Float
}

"""aggregate var_samp on columns"""
type dipdup_model_update_var_samp_fields {
  id: Float
  level: Float
}

"""aggregate variance on columns"""
type dipdup_model_update_variance_fields {
  id: Float
  level: Float
}

"""
columns and relationships of "dipdup_schema"
"""
type dipdup_schema {
  created_at: timestamptz!
  hash: String!
  name: String!

  """
  manual: manual\nmigration: migration\nrollback: rollback\nconfig_modified: config_modified\nschema_modified: schema_modified
  """
  reindex: String
  updated_at: timestamptz!
}

"""
aggregated selection of "dipdup_schema"
"""
type dipdup_schema_aggregate {
  aggregate: dipdup_schema_aggregate_fields
  nodes: [dipdup_schema!]!
}

"""
aggregate fields of "dipdup_schema"
"""
type dipdup_schema_aggregate_fields {
  count(columns: [dipdup_schema_select_column!], distinct: Boolean): Int!
  max: dipdup_schema_max_fields
  min: dipdup_schema_min_fields
}

"""
Boolean expression to filter rows from the table "dipdup_schema". All fields are combined with a logical 'AND'.
"""
input dipdup_schema_bool_exp {
  _and: [dipdup_schema_bool_exp!]
  _not: dipdup_schema_bool_exp
  _or: [dipdup_schema_bool_exp!]
  created_at: timestamptz_comparison_exp
  hash: String_comparison_exp
  name: String_comparison_exp
  reindex: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "dipdup_schema"
"""
enum dipdup_schema_constraint {
  """
  unique or primary key constraint on columns "name"
  """
  dipdup_schema_pkey
}

"""
input type for inserting data into table "dipdup_schema"
"""
input dipdup_schema_insert_input {
  created_at: timestamptz
  hash: String
  name: String

  """
  manual: manual\nmigration: migration\nrollback: rollback\nconfig_modified: config_modified\nschema_modified: schema_modified
  """
  reindex: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type dipdup_schema_max_fields {
  created_at: timestamptz
  hash: String
  name: String

  """
  manual: manual\nmigration: migration\nrollback: rollback\nconfig_modified: config_modified\nschema_modified: schema_modified
  """
  reindex: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type dipdup_schema_min_fields {
  created_at: timestamptz
  hash: String
  name: String

  """
  manual: manual\nmigration: migration\nrollback: rollback\nconfig_modified: config_modified\nschema_modified: schema_modified
  """
  reindex: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "dipdup_schema"
"""
type dipdup_schema_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_schema!]!
}

"""
on_conflict condition type for table "dipdup_schema"
"""
input dipdup_schema_on_conflict {
  constraint: dipdup_schema_constraint!
  update_columns: [dipdup_schema_update_column!]! = []
  where: dipdup_schema_bool_exp
}

"""Ordering options when selecting data from "dipdup_schema"."""
input dipdup_schema_order_by {
  created_at: order_by
  hash: order_by
  name: order_by
  reindex: order_by
  updated_at: order_by
}

"""primary key columns input for table: dipdup_schema"""
input dipdup_schema_pk_columns_input {
  name: String!
}

"""
select columns of table "dipdup_schema"
"""
enum dipdup_schema_select_column {
  """column name"""
  created_at

  """column name"""
  hash

  """column name"""
  name

  """column name"""
  reindex

  """column name"""
  updated_at
}

"""
input type for updating data in table "dipdup_schema"
"""
input dipdup_schema_set_input {
  created_at: timestamptz
  hash: String
  name: String

  """
  manual: manual\nmigration: migration\nrollback: rollback\nconfig_modified: config_modified\nschema_modified: schema_modified
  """
  reindex: String
  updated_at: timestamptz
}

"""
Streaming cursor of the table "dipdup_schema"
"""
input dipdup_schema_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_schema_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_schema_stream_cursor_value_input {
  created_at: timestamptz
  hash: String
  name: String

  """
  manual: manual\nmigration: migration\nrollback: rollback\nconfig_modified: config_modified\nschema_modified: schema_modified
  """
  reindex: String
  updated_at: timestamptz
}

"""
update columns of table "dipdup_schema"
"""
enum dipdup_schema_update_column {
  """column name"""
  created_at

  """column name"""
  hash

  """column name"""
  name

  """column name"""
  reindex

  """column name"""
  updated_at
}

input dipdup_schema_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_schema_set_input

  """filter the rows which have to be updated"""
  where: dipdup_schema_bool_exp!
}

"""
columns and relationships of "dipdup_token_metadata"
"""
type dipdup_token_metadata {
  contract: String!
  created_at: timestamptz!
  id: Int!
  metadata(
    """JSON select path"""
    path: String
  ): jsonb!
  network: String!
  token_id: String!
  update_id: Int!
  updated_at: timestamptz!
}

"""
aggregated selection of "dipdup_token_metadata"
"""
type dipdup_token_metadata_aggregate {
  aggregate: dipdup_token_metadata_aggregate_fields
  nodes: [dipdup_token_metadata!]!
}

"""
aggregate fields of "dipdup_token_metadata"
"""
type dipdup_token_metadata_aggregate_fields {
  avg: dipdup_token_metadata_avg_fields
  count(columns: [dipdup_token_metadata_select_column!], distinct: Boolean): Int!
  max: dipdup_token_metadata_max_fields
  min: dipdup_token_metadata_min_fields
  stddev: dipdup_token_metadata_stddev_fields
  stddev_pop: dipdup_token_metadata_stddev_pop_fields
  stddev_samp: dipdup_token_metadata_stddev_samp_fields
  sum: dipdup_token_metadata_sum_fields
  var_pop: dipdup_token_metadata_var_pop_fields
  var_samp: dipdup_token_metadata_var_samp_fields
  variance: dipdup_token_metadata_variance_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input dipdup_token_metadata_append_input {
  metadata: jsonb
}

"""aggregate avg on columns"""
type dipdup_token_metadata_avg_fields {
  id: Float
  update_id: Float
}

"""
Boolean expression to filter rows from the table "dipdup_token_metadata". All fields are combined with a logical 'AND'.
"""
input dipdup_token_metadata_bool_exp {
  _and: [dipdup_token_metadata_bool_exp!]
  _not: dipdup_token_metadata_bool_exp
  _or: [dipdup_token_metadata_bool_exp!]
  contract: String_comparison_exp
  created_at: timestamptz_comparison_exp
  id: Int_comparison_exp
  metadata: jsonb_comparison_exp
  network: String_comparison_exp
  token_id: String_comparison_exp
  update_id: Int_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "dipdup_token_metadata"
"""
enum dipdup_token_metadata_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  dipdup_token_metadata_pkey

  """
  unique or primary key constraint on columns "network", "token_id", "contract"
  """
  uid_dipdup_toke_network_5d1a25
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input dipdup_token_metadata_delete_at_path_input {
  metadata: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input dipdup_token_metadata_delete_elem_input {
  metadata: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input dipdup_token_metadata_delete_key_input {
  metadata: String
}

"""
input type for incrementing numeric columns in table "dipdup_token_metadata"
"""
input dipdup_token_metadata_inc_input {
  id: Int
  update_id: Int
}

"""
input type for inserting data into table "dipdup_token_metadata"
"""
input dipdup_token_metadata_insert_input {
  contract: String
  created_at: timestamptz
  id: Int
  metadata: jsonb
  network: String
  token_id: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate max on columns"""
type dipdup_token_metadata_max_fields {
  contract: String
  created_at: timestamptz
  id: Int
  network: String
  token_id: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate min on columns"""
type dipdup_token_metadata_min_fields {
  contract: String
  created_at: timestamptz
  id: Int
  network: String
  token_id: String
  update_id: Int
  updated_at: timestamptz
}

"""
response of any mutation on the table "dipdup_token_metadata"
"""
type dipdup_token_metadata_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [dipdup_token_metadata!]!
}

"""
on_conflict condition type for table "dipdup_token_metadata"
"""
input dipdup_token_metadata_on_conflict {
  constraint: dipdup_token_metadata_constraint!
  update_columns: [dipdup_token_metadata_update_column!]! = []
  where: dipdup_token_metadata_bool_exp
}

"""Ordering options when selecting data from "dipdup_token_metadata"."""
input dipdup_token_metadata_order_by {
  contract: order_by
  created_at: order_by
  id: order_by
  metadata: order_by
  network: order_by
  token_id: order_by
  update_id: order_by
  updated_at: order_by
}

"""primary key columns input for table: dipdup_token_metadata"""
input dipdup_token_metadata_pk_columns_input {
  id: Int!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input dipdup_token_metadata_prepend_input {
  metadata: jsonb
}

"""
select columns of table "dipdup_token_metadata"
"""
enum dipdup_token_metadata_select_column {
  """column name"""
  contract

  """column name"""
  created_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  network

  """column name"""
  token_id

  """column name"""
  update_id

  """column name"""
  updated_at
}

"""
input type for updating data in table "dipdup_token_metadata"
"""
input dipdup_token_metadata_set_input {
  contract: String
  created_at: timestamptz
  id: Int
  metadata: jsonb
  network: String
  token_id: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type dipdup_token_metadata_stddev_fields {
  id: Float
  update_id: Float
}

"""aggregate stddev_pop on columns"""
type dipdup_token_metadata_stddev_pop_fields {
  id: Float
  update_id: Float
}

"""aggregate stddev_samp on columns"""
type dipdup_token_metadata_stddev_samp_fields {
  id: Float
  update_id: Float
}

"""
Streaming cursor of the table "dipdup_token_metadata"
"""
input dipdup_token_metadata_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: dipdup_token_metadata_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input dipdup_token_metadata_stream_cursor_value_input {
  contract: String
  created_at: timestamptz
  id: Int
  metadata: jsonb
  network: String
  token_id: String
  update_id: Int
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type dipdup_token_metadata_sum_fields {
  id: Int
  update_id: Int
}

"""
update columns of table "dipdup_token_metadata"
"""
enum dipdup_token_metadata_update_column {
  """column name"""
  contract

  """column name"""
  created_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  network

  """column name"""
  token_id

  """column name"""
  update_id

  """column name"""
  updated_at
}

input dipdup_token_metadata_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: dipdup_token_metadata_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: dipdup_token_metadata_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: dipdup_token_metadata_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: dipdup_token_metadata_delete_key_input

  """increments the numeric columns with given value of the filtered values"""
  _inc: dipdup_token_metadata_inc_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: dipdup_token_metadata_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: dipdup_token_metadata_set_input

  """filter the rows which have to be updated"""
  where: dipdup_token_metadata_bool_exp!
}

"""aggregate var_pop on columns"""
type dipdup_token_metadata_var_pop_fields {
  id: Float
  update_id: Float
}

"""aggregate var_samp on columns"""
type dipdup_token_metadata_var_samp_fields {
  id: Float
  update_id: Float
}

"""aggregate variance on columns"""
type dipdup_token_metadata_variance_fields {
  id: Float
  update_id: Float
}

"""
columns and relationships of "indexing_status"
"""
type indexing_status {
  """
  COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
  """
  index: String!
  last_level: String!
}

"""
aggregated selection of "indexing_status"
"""
type indexing_status_aggregate {
  aggregate: indexing_status_aggregate_fields
  nodes: [indexing_status!]!
}

"""
aggregate fields of "indexing_status"
"""
type indexing_status_aggregate_fields {
  count(columns: [indexing_status_select_column!], distinct: Boolean): Int!
  max: indexing_status_max_fields
  min: indexing_status_min_fields
}

"""
Boolean expression to filter rows from the table "indexing_status". All fields are combined with a logical 'AND'.
"""
input indexing_status_bool_exp {
  _and: [indexing_status_bool_exp!]
  _not: indexing_status_bool_exp
  _or: [indexing_status_bool_exp!]
  index: String_comparison_exp
  last_level: String_comparison_exp
}

"""
unique or primary key constraints on table "indexing_status"
"""
enum indexing_status_constraint {
  """
  unique or primary key constraint on columns "index"
  """
  indexing_status_pkey
}

"""
input type for inserting data into table "indexing_status"
"""
input indexing_status_insert_input {
  """
  COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
  """
  index: String
  last_level: String
}

"""aggregate max on columns"""
type indexing_status_max_fields {
  """
  COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
  """
  index: String
  last_level: String
}

"""aggregate min on columns"""
type indexing_status_min_fields {
  """
  COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
  """
  index: String
  last_level: String
}

"""
response of any mutation on the table "indexing_status"
"""
type indexing_status_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [indexing_status!]!
}

"""
on_conflict condition type for table "indexing_status"
"""
input indexing_status_on_conflict {
  constraint: indexing_status_constraint!
  update_columns: [indexing_status_update_column!]! = []
  where: indexing_status_bool_exp
}

"""Ordering options when selecting data from "indexing_status"."""
input indexing_status_order_by {
  index: order_by
  last_level: order_by
}

"""primary key columns input for table: indexing_status"""
input indexing_status_pk_columns_input {
  """
  COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
  """
  index: String!
}

"""
select columns of table "indexing_status"
"""
enum indexing_status_select_column {
  """column name"""
  index

  """column name"""
  last_level
}

"""
input type for updating data in table "indexing_status"
"""
input indexing_status_set_input {
  """
  COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
  """
  index: String
  last_level: String
}

"""
Streaming cursor of the table "indexing_status"
"""
input indexing_status_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: indexing_status_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input indexing_status_stream_cursor_value_input {
  """
  COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
  """
  index: String
  last_level: String
}

"""
update columns of table "indexing_status"
"""
enum indexing_status_update_column {
  """column name"""
  index

  """column name"""
  last_level
}

input indexing_status_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: indexing_status_set_input

  """filter the rows which have to be updated"""
  where: indexing_status_bool_exp!
}

scalar jsonb

input jsonb_cast_exp {
  String: String_comparison_exp
}

"""
Boolean expression to compare columns of type "jsonb". All fields are combined with logical 'AND'.
"""
input jsonb_comparison_exp {
  _cast: jsonb_cast_exp

  """is the column contained in the given json value"""
  _contained_in: jsonb

  """does the column contain the given json value at the top level"""
  _contains: jsonb
  _eq: jsonb
  _gt: jsonb
  _gte: jsonb

  """does the string exist as a top-level key in the column"""
  _has_key: String

  """do all of these strings exist as top-level keys in the column"""
  _has_keys_all: [String!]

  """do any of these strings exist as top-level keys in the column"""
  _has_keys_any: [String!]
  _in: [jsonb!]
  _is_null: Boolean
  _lt: jsonb
  _lte: jsonb
  _neq: jsonb
  _nin: [jsonb!]
}

"""
columns and relationships of "legacy_orders"
"""
type legacy_orders {
  data(
    """JSON select path"""
    path: String
  ): jsonb!
  hash: String!
  id: uuid!
}

"""
aggregated selection of "legacy_orders"
"""
type legacy_orders_aggregate {
  aggregate: legacy_orders_aggregate_fields
  nodes: [legacy_orders!]!
}

"""
aggregate fields of "legacy_orders"
"""
type legacy_orders_aggregate_fields {
  count(columns: [legacy_orders_select_column!], distinct: Boolean): Int!
  max: legacy_orders_max_fields
  min: legacy_orders_min_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input legacy_orders_append_input {
  data: jsonb
}

"""
Boolean expression to filter rows from the table "legacy_orders". All fields are combined with a logical 'AND'.
"""
input legacy_orders_bool_exp {
  _and: [legacy_orders_bool_exp!]
  _not: legacy_orders_bool_exp
  _or: [legacy_orders_bool_exp!]
  data: jsonb_comparison_exp
  hash: String_comparison_exp
  id: uuid_comparison_exp
}

"""
unique or primary key constraints on table "legacy_orders"
"""
enum legacy_orders_constraint {
  """
  unique or primary key constraint on columns "hash"
  """
  legacy_orders_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input legacy_orders_delete_at_path_input {
  data: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input legacy_orders_delete_elem_input {
  data: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input legacy_orders_delete_key_input {
  data: String
}

"""
input type for inserting data into table "legacy_orders"
"""
input legacy_orders_insert_input {
  data: jsonb
  hash: String
  id: uuid
}

"""aggregate max on columns"""
type legacy_orders_max_fields {
  hash: String
  id: uuid
}

"""aggregate min on columns"""
type legacy_orders_min_fields {
  hash: String
  id: uuid
}

"""
response of any mutation on the table "legacy_orders"
"""
type legacy_orders_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [legacy_orders!]!
}

"""
on_conflict condition type for table "legacy_orders"
"""
input legacy_orders_on_conflict {
  constraint: legacy_orders_constraint!
  update_columns: [legacy_orders_update_column!]! = []
  where: legacy_orders_bool_exp
}

"""Ordering options when selecting data from "legacy_orders"."""
input legacy_orders_order_by {
  data: order_by
  hash: order_by
  id: order_by
}

"""primary key columns input for table: legacy_orders"""
input legacy_orders_pk_columns_input {
  hash: String!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input legacy_orders_prepend_input {
  data: jsonb
}

"""
select columns of table "legacy_orders"
"""
enum legacy_orders_select_column {
  """column name"""
  data

  """column name"""
  hash

  """column name"""
  id
}

"""
input type for updating data in table "legacy_orders"
"""
input legacy_orders_set_input {
  data: jsonb
  hash: String
  id: uuid
}

"""
Streaming cursor of the table "legacy_orders"
"""
input legacy_orders_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: legacy_orders_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input legacy_orders_stream_cursor_value_input {
  data: jsonb
  hash: String
  id: uuid
}

"""
update columns of table "legacy_orders"
"""
enum legacy_orders_update_column {
  """column name"""
  data

  """column name"""
  hash

  """column name"""
  id
}

input legacy_orders_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: legacy_orders_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: legacy_orders_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: legacy_orders_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: legacy_orders_delete_key_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: legacy_orders_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: legacy_orders_set_input

  """filter the rows which have to be updated"""
  where: legacy_orders_bool_exp!
}

"""
columns and relationships of "marketplace_activity"
"""
type marketplace_activity {
  db_updated_at: timestamptz
  id: uuid!
  internal_order_id: String!

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String!
  make_contract: String
  make_price: numeric
  make_token_id: String
  make_value: numeric!
  maker: String
  network: String!
  operation_counter: Int!
  operation_hash: String!
  operation_level: Int!
  operation_nonce: Int
  operation_timestamp: timestamptz!
  order_id: uuid!

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String!

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String!
}

"""
aggregated selection of "marketplace_activity"
"""
type marketplace_activity_aggregate {
  aggregate: marketplace_activity_aggregate_fields
  nodes: [marketplace_activity!]!
}

"""
aggregate fields of "marketplace_activity"
"""
type marketplace_activity_aggregate_fields {
  avg: marketplace_activity_avg_fields
  count(columns: [marketplace_activity_select_column!], distinct: Boolean): Int!
  max: marketplace_activity_max_fields
  min: marketplace_activity_min_fields
  stddev: marketplace_activity_stddev_fields
  stddev_pop: marketplace_activity_stddev_pop_fields
  stddev_samp: marketplace_activity_stddev_samp_fields
  sum: marketplace_activity_sum_fields
  var_pop: marketplace_activity_var_pop_fields
  var_samp: marketplace_activity_var_samp_fields
  variance: marketplace_activity_variance_fields
}

"""aggregate avg on columns"""
type marketplace_activity_avg_fields {
  make_price: Float
  make_value: Float
  operation_counter: Float
  operation_level: Float
  operation_nonce: Float
  take_price: Float
  take_value: Float
}

"""
Boolean expression to filter rows from the table "marketplace_activity". All fields are combined with a logical 'AND'.
"""
input marketplace_activity_bool_exp {
  _and: [marketplace_activity_bool_exp!]
  _not: marketplace_activity_bool_exp
  _or: [marketplace_activity_bool_exp!]
  db_updated_at: timestamptz_comparison_exp
  id: uuid_comparison_exp
  internal_order_id: String_comparison_exp
  make_asset_class: String_comparison_exp
  make_contract: String_comparison_exp
  make_price: numeric_comparison_exp
  make_token_id: String_comparison_exp
  make_value: numeric_comparison_exp
  maker: String_comparison_exp
  network: String_comparison_exp
  operation_counter: Int_comparison_exp
  operation_hash: String_comparison_exp
  operation_level: Int_comparison_exp
  operation_nonce: Int_comparison_exp
  operation_timestamp: timestamptz_comparison_exp
  order_id: uuid_comparison_exp
  platform: String_comparison_exp
  take_asset_class: String_comparison_exp
  take_contract: String_comparison_exp
  take_price: numeric_comparison_exp
  take_token_id: String_comparison_exp
  take_value: numeric_comparison_exp
  taker: String_comparison_exp
  type: String_comparison_exp
}

"""
unique or primary key constraints on table "marketplace_activity"
"""
enum marketplace_activity_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  marketplace_activity_pkey
}

"""
input type for incrementing numeric columns in table "marketplace_activity"
"""
input marketplace_activity_inc_input {
  make_price: numeric
  make_value: numeric
  operation_counter: Int
  operation_level: Int
  operation_nonce: Int
  take_price: numeric
  take_value: numeric
}

"""
input type for inserting data into table "marketplace_activity"
"""
input marketplace_activity_insert_input {
  db_updated_at: timestamptz
  id: uuid
  internal_order_id: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  operation_counter: Int
  operation_hash: String
  operation_level: Int
  operation_nonce: Int
  operation_timestamp: timestamptz
  order_id: uuid

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
}

"""aggregate max on columns"""
type marketplace_activity_max_fields {
  db_updated_at: timestamptz
  id: uuid
  internal_order_id: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  operation_counter: Int
  operation_hash: String
  operation_level: Int
  operation_nonce: Int
  operation_timestamp: timestamptz
  order_id: uuid

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
}

"""aggregate min on columns"""
type marketplace_activity_min_fields {
  db_updated_at: timestamptz
  id: uuid
  internal_order_id: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  operation_counter: Int
  operation_hash: String
  operation_level: Int
  operation_nonce: Int
  operation_timestamp: timestamptz
  order_id: uuid

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
}

"""
response of any mutation on the table "marketplace_activity"
"""
type marketplace_activity_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [marketplace_activity!]!
}

"""
on_conflict condition type for table "marketplace_activity"
"""
input marketplace_activity_on_conflict {
  constraint: marketplace_activity_constraint!
  update_columns: [marketplace_activity_update_column!]! = []
  where: marketplace_activity_bool_exp
}

"""Ordering options when selecting data from "marketplace_activity"."""
input marketplace_activity_order_by {
  db_updated_at: order_by
  id: order_by
  internal_order_id: order_by
  make_asset_class: order_by
  make_contract: order_by
  make_price: order_by
  make_token_id: order_by
  make_value: order_by
  maker: order_by
  network: order_by
  operation_counter: order_by
  operation_hash: order_by
  operation_level: order_by
  operation_nonce: order_by
  operation_timestamp: order_by
  order_id: order_by
  platform: order_by
  take_asset_class: order_by
  take_contract: order_by
  take_price: order_by
  take_token_id: order_by
  take_value: order_by
  taker: order_by
  type: order_by
}

"""primary key columns input for table: marketplace_activity"""
input marketplace_activity_pk_columns_input {
  id: uuid!
}

"""
select columns of table "marketplace_activity"
"""
enum marketplace_activity_select_column {
  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  internal_order_id

  """column name"""
  make_asset_class

  """column name"""
  make_contract

  """column name"""
  make_price

  """column name"""
  make_token_id

  """column name"""
  make_value

  """column name"""
  maker

  """column name"""
  network

  """column name"""
  operation_counter

  """column name"""
  operation_hash

  """column name"""
  operation_level

  """column name"""
  operation_nonce

  """column name"""
  operation_timestamp

  """column name"""
  order_id

  """column name"""
  platform

  """column name"""
  take_asset_class

  """column name"""
  take_contract

  """column name"""
  take_price

  """column name"""
  take_token_id

  """column name"""
  take_value

  """column name"""
  taker

  """column name"""
  type
}

"""
input type for updating data in table "marketplace_activity"
"""
input marketplace_activity_set_input {
  db_updated_at: timestamptz
  id: uuid
  internal_order_id: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  operation_counter: Int
  operation_hash: String
  operation_level: Int
  operation_nonce: Int
  operation_timestamp: timestamptz
  order_id: uuid

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
}

"""aggregate stddev on columns"""
type marketplace_activity_stddev_fields {
  make_price: Float
  make_value: Float
  operation_counter: Float
  operation_level: Float
  operation_nonce: Float
  take_price: Float
  take_value: Float
}

"""aggregate stddev_pop on columns"""
type marketplace_activity_stddev_pop_fields {
  make_price: Float
  make_value: Float
  operation_counter: Float
  operation_level: Float
  operation_nonce: Float
  take_price: Float
  take_value: Float
}

"""aggregate stddev_samp on columns"""
type marketplace_activity_stddev_samp_fields {
  make_price: Float
  make_value: Float
  operation_counter: Float
  operation_level: Float
  operation_nonce: Float
  take_price: Float
  take_value: Float
}

"""
Streaming cursor of the table "marketplace_activity"
"""
input marketplace_activity_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: marketplace_activity_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input marketplace_activity_stream_cursor_value_input {
  db_updated_at: timestamptz
  id: uuid
  internal_order_id: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  operation_counter: Int
  operation_hash: String
  operation_level: Int
  operation_nonce: Int
  operation_timestamp: timestamptz
  order_id: uuid

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
}

"""aggregate sum on columns"""
type marketplace_activity_sum_fields {
  make_price: numeric
  make_value: numeric
  operation_counter: Int
  operation_level: Int
  operation_nonce: Int
  take_price: numeric
  take_value: numeric
}

"""
update columns of table "marketplace_activity"
"""
enum marketplace_activity_update_column {
  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  internal_order_id

  """column name"""
  make_asset_class

  """column name"""
  make_contract

  """column name"""
  make_price

  """column name"""
  make_token_id

  """column name"""
  make_value

  """column name"""
  maker

  """column name"""
  network

  """column name"""
  operation_counter

  """column name"""
  operation_hash

  """column name"""
  operation_level

  """column name"""
  operation_nonce

  """column name"""
  operation_timestamp

  """column name"""
  order_id

  """column name"""
  platform

  """column name"""
  take_asset_class

  """column name"""
  take_contract

  """column name"""
  take_price

  """column name"""
  take_token_id

  """column name"""
  take_value

  """column name"""
  taker

  """column name"""
  type
}

input marketplace_activity_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: marketplace_activity_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: marketplace_activity_set_input

  """filter the rows which have to be updated"""
  where: marketplace_activity_bool_exp!
}

"""aggregate var_pop on columns"""
type marketplace_activity_var_pop_fields {
  make_price: Float
  make_value: Float
  operation_counter: Float
  operation_level: Float
  operation_nonce: Float
  take_price: Float
  take_value: Float
}

"""aggregate var_samp on columns"""
type marketplace_activity_var_samp_fields {
  make_price: Float
  make_value: Float
  operation_counter: Float
  operation_level: Float
  operation_nonce: Float
  take_price: Float
  take_value: Float
}

"""aggregate variance on columns"""
type marketplace_activity_variance_fields {
  make_price: Float
  make_value: Float
  operation_counter: Float
  operation_level: Float
  operation_nonce: Float
  take_price: Float
  take_value: Float
}

"""
columns and relationships of "marketplace_order"
"""
type marketplace_order {
  cancelled: Boolean!
  created_at: timestamptz!
  end_at: timestamptz
  ended_at: timestamptz
  fill: numeric!
  id: uuid!
  internal_order_id: String!
  is_bid: Boolean!
  last_updated_at: timestamptz!

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String!
  make_contract: String
  make_price: numeric
  make_stock: numeric
  make_token_id: String
  make_value: numeric!
  maker: String!
  network: String!
  origin_fees(
    """JSON select path"""
    path: String
  ): jsonb!
  payouts(
    """JSON select path"""
    path: String
  ): jsonb!

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String!
  salt: String!
  start_at: timestamptz!

  """
  ACTIVE: ACTIVE\nFILLED: FILLED\nHISTORICAL: HISTORICAL\nINACTIVE: INACTIVE\nCANCELLED: CANCELLED
  """
  status: String!

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String
}

"""
aggregated selection of "marketplace_order"
"""
type marketplace_order_aggregate {
  aggregate: marketplace_order_aggregate_fields
  nodes: [marketplace_order!]!
}

"""
aggregate fields of "marketplace_order"
"""
type marketplace_order_aggregate_fields {
  avg: marketplace_order_avg_fields
  count(columns: [marketplace_order_select_column!], distinct: Boolean): Int!
  max: marketplace_order_max_fields
  min: marketplace_order_min_fields
  stddev: marketplace_order_stddev_fields
  stddev_pop: marketplace_order_stddev_pop_fields
  stddev_samp: marketplace_order_stddev_samp_fields
  sum: marketplace_order_sum_fields
  var_pop: marketplace_order_var_pop_fields
  var_samp: marketplace_order_var_samp_fields
  variance: marketplace_order_variance_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input marketplace_order_append_input {
  origin_fees: jsonb
  payouts: jsonb
}

"""aggregate avg on columns"""
type marketplace_order_avg_fields {
  fill: Float
  make_price: Float
  make_stock: Float
  make_value: Float
  take_price: Float
  take_value: Float
}

"""
Boolean expression to filter rows from the table "marketplace_order". All fields are combined with a logical 'AND'.
"""
input marketplace_order_bool_exp {
  _and: [marketplace_order_bool_exp!]
  _not: marketplace_order_bool_exp
  _or: [marketplace_order_bool_exp!]
  cancelled: Boolean_comparison_exp
  created_at: timestamptz_comparison_exp
  end_at: timestamptz_comparison_exp
  ended_at: timestamptz_comparison_exp
  fill: numeric_comparison_exp
  id: uuid_comparison_exp
  internal_order_id: String_comparison_exp
  is_bid: Boolean_comparison_exp
  last_updated_at: timestamptz_comparison_exp
  make_asset_class: String_comparison_exp
  make_contract: String_comparison_exp
  make_price: numeric_comparison_exp
  make_stock: numeric_comparison_exp
  make_token_id: String_comparison_exp
  make_value: numeric_comparison_exp
  maker: String_comparison_exp
  network: String_comparison_exp
  origin_fees: jsonb_comparison_exp
  payouts: jsonb_comparison_exp
  platform: String_comparison_exp
  salt: String_comparison_exp
  start_at: timestamptz_comparison_exp
  status: String_comparison_exp
  take_asset_class: String_comparison_exp
  take_contract: String_comparison_exp
  take_price: numeric_comparison_exp
  take_token_id: String_comparison_exp
  take_value: numeric_comparison_exp
  taker: String_comparison_exp
}

"""
unique or primary key constraints on table "marketplace_order"
"""
enum marketplace_order_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  marketplace_order_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input marketplace_order_delete_at_path_input {
  origin_fees: [String!]
  payouts: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input marketplace_order_delete_elem_input {
  origin_fees: Int
  payouts: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input marketplace_order_delete_key_input {
  origin_fees: String
  payouts: String
}

"""
input type for incrementing numeric columns in table "marketplace_order"
"""
input marketplace_order_inc_input {
  fill: numeric
  make_price: numeric
  make_stock: numeric
  make_value: numeric
  take_price: numeric
  take_value: numeric
}

"""
input type for inserting data into table "marketplace_order"
"""
input marketplace_order_insert_input {
  cancelled: Boolean
  created_at: timestamptz
  end_at: timestamptz
  ended_at: timestamptz
  fill: numeric
  id: uuid
  internal_order_id: String
  is_bid: Boolean
  last_updated_at: timestamptz

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_stock: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  origin_fees: jsonb
  payouts: jsonb

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String
  salt: String
  start_at: timestamptz

  """
  ACTIVE: ACTIVE\nFILLED: FILLED\nHISTORICAL: HISTORICAL\nINACTIVE: INACTIVE\nCANCELLED: CANCELLED
  """
  status: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String
}

"""aggregate max on columns"""
type marketplace_order_max_fields {
  created_at: timestamptz
  end_at: timestamptz
  ended_at: timestamptz
  fill: numeric
  id: uuid
  internal_order_id: String
  last_updated_at: timestamptz

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_stock: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String
  salt: String
  start_at: timestamptz

  """
  ACTIVE: ACTIVE\nFILLED: FILLED\nHISTORICAL: HISTORICAL\nINACTIVE: INACTIVE\nCANCELLED: CANCELLED
  """
  status: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String
}

"""aggregate min on columns"""
type marketplace_order_min_fields {
  created_at: timestamptz
  end_at: timestamptz
  ended_at: timestamptz
  fill: numeric
  id: uuid
  internal_order_id: String
  last_updated_at: timestamptz

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_stock: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String
  salt: String
  start_at: timestamptz

  """
  ACTIVE: ACTIVE\nFILLED: FILLED\nHISTORICAL: HISTORICAL\nINACTIVE: INACTIVE\nCANCELLED: CANCELLED
  """
  status: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String
}

"""
response of any mutation on the table "marketplace_order"
"""
type marketplace_order_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [marketplace_order!]!
}

"""
on_conflict condition type for table "marketplace_order"
"""
input marketplace_order_on_conflict {
  constraint: marketplace_order_constraint!
  update_columns: [marketplace_order_update_column!]! = []
  where: marketplace_order_bool_exp
}

"""Ordering options when selecting data from "marketplace_order"."""
input marketplace_order_order_by {
  cancelled: order_by
  created_at: order_by
  end_at: order_by
  ended_at: order_by
  fill: order_by
  id: order_by
  internal_order_id: order_by
  is_bid: order_by
  last_updated_at: order_by
  make_asset_class: order_by
  make_contract: order_by
  make_price: order_by
  make_stock: order_by
  make_token_id: order_by
  make_value: order_by
  maker: order_by
  network: order_by
  origin_fees: order_by
  payouts: order_by
  platform: order_by
  salt: order_by
  start_at: order_by
  status: order_by
  take_asset_class: order_by
  take_contract: order_by
  take_price: order_by
  take_token_id: order_by
  take_value: order_by
  taker: order_by
}

"""primary key columns input for table: marketplace_order"""
input marketplace_order_pk_columns_input {
  id: uuid!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input marketplace_order_prepend_input {
  origin_fees: jsonb
  payouts: jsonb
}

"""
select columns of table "marketplace_order"
"""
enum marketplace_order_select_column {
  """column name"""
  cancelled

  """column name"""
  created_at

  """column name"""
  end_at

  """column name"""
  ended_at

  """column name"""
  fill

  """column name"""
  id

  """column name"""
  internal_order_id

  """column name"""
  is_bid

  """column name"""
  last_updated_at

  """column name"""
  make_asset_class

  """column name"""
  make_contract

  """column name"""
  make_price

  """column name"""
  make_stock

  """column name"""
  make_token_id

  """column name"""
  make_value

  """column name"""
  maker

  """column name"""
  network

  """column name"""
  origin_fees

  """column name"""
  payouts

  """column name"""
  platform

  """column name"""
  salt

  """column name"""
  start_at

  """column name"""
  status

  """column name"""
  take_asset_class

  """column name"""
  take_contract

  """column name"""
  take_price

  """column name"""
  take_token_id

  """column name"""
  take_value

  """column name"""
  taker
}

"""
input type for updating data in table "marketplace_order"
"""
input marketplace_order_set_input {
  cancelled: Boolean
  created_at: timestamptz
  end_at: timestamptz
  ended_at: timestamptz
  fill: numeric
  id: uuid
  internal_order_id: String
  is_bid: Boolean
  last_updated_at: timestamptz

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_stock: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  origin_fees: jsonb
  payouts: jsonb

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String
  salt: String
  start_at: timestamptz

  """
  ACTIVE: ACTIVE\nFILLED: FILLED\nHISTORICAL: HISTORICAL\nINACTIVE: INACTIVE\nCANCELLED: CANCELLED
  """
  status: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String
}

"""aggregate stddev on columns"""
type marketplace_order_stddev_fields {
  fill: Float
  make_price: Float
  make_stock: Float
  make_value: Float
  take_price: Float
  take_value: Float
}

"""aggregate stddev_pop on columns"""
type marketplace_order_stddev_pop_fields {
  fill: Float
  make_price: Float
  make_stock: Float
  make_value: Float
  take_price: Float
  take_value: Float
}

"""aggregate stddev_samp on columns"""
type marketplace_order_stddev_samp_fields {
  fill: Float
  make_price: Float
  make_stock: Float
  make_value: Float
  take_price: Float
  take_value: Float
}

"""
Streaming cursor of the table "marketplace_order"
"""
input marketplace_order_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: marketplace_order_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input marketplace_order_stream_cursor_value_input {
  cancelled: Boolean
  created_at: timestamptz
  end_at: timestamptz
  ended_at: timestamptz
  fill: numeric
  id: uuid
  internal_order_id: String
  is_bid: Boolean
  last_updated_at: timestamptz

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  make_asset_class: String
  make_contract: String
  make_price: numeric
  make_stock: numeric
  make_token_id: String
  make_value: numeric
  maker: String
  network: String
  origin_fees: jsonb
  payouts: jsonb

  """
  HEN: HEN\nTEIA_V1: TEIA_V1\nVERSUM_V1: VERSUM_V1\nOBJKT_V1: OBJKT_V1\nOBJKT_V2: OBJKT_V2\nRARIBLE_V1: RARIBLE_V1\nRARIBLE_V2: RARIBLE_V2\nFXHASH_V1: FXHASH_V1\nFXHASH_V2: FXHASH_V2
  """
  platform: String
  salt: String
  start_at: timestamptz

  """
  ACTIVE: ACTIVE\nFILLED: FILLED\nHISTORICAL: HISTORICAL\nINACTIVE: INACTIVE\nCANCELLED: CANCELLED
  """
  status: String

  """
  ETH: ETH\nXTZ: XTZ\nFUNGIBLE_TOKEN: TEZOS_FT\nNON_FUNGIBLE_TOKEN: TEZOS_NFT\nMULTI_TOKEN: TEZOS_MT\nERC20: ERC20\nERC721: ERC721\nERC1155: ERC1155\nERC721_LAZY: ERC721_LAZY\nERC1155_LAZY: ERC1155_LAZY\nCOLLECTION: COLLECTION\nGEN_ART: GEN_ART
  """
  take_asset_class: String
  take_contract: String
  take_price: numeric
  take_token_id: String
  take_value: numeric
  taker: String
}

"""aggregate sum on columns"""
type marketplace_order_sum_fields {
  fill: numeric
  make_price: numeric
  make_stock: numeric
  make_value: numeric
  take_price: numeric
  take_value: numeric
}

"""
update columns of table "marketplace_order"
"""
enum marketplace_order_update_column {
  """column name"""
  cancelled

  """column name"""
  created_at

  """column name"""
  end_at

  """column name"""
  ended_at

  """column name"""
  fill

  """column name"""
  id

  """column name"""
  internal_order_id

  """column name"""
  is_bid

  """column name"""
  last_updated_at

  """column name"""
  make_asset_class

  """column name"""
  make_contract

  """column name"""
  make_price

  """column name"""
  make_stock

  """column name"""
  make_token_id

  """column name"""
  make_value

  """column name"""
  maker

  """column name"""
  network

  """column name"""
  origin_fees

  """column name"""
  payouts

  """column name"""
  platform

  """column name"""
  salt

  """column name"""
  start_at

  """column name"""
  status

  """column name"""
  take_asset_class

  """column name"""
  take_contract

  """column name"""
  take_price

  """column name"""
  take_token_id

  """column name"""
  take_value

  """column name"""
  taker
}

input marketplace_order_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: marketplace_order_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: marketplace_order_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: marketplace_order_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: marketplace_order_delete_key_input

  """increments the numeric columns with given value of the filtered values"""
  _inc: marketplace_order_inc_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: marketplace_order_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: marketplace_order_set_input

  """filter the rows which have to be updated"""
  where: marketplace_order_bool_exp!
}

"""aggregate var_pop on columns"""
type marketplace_order_var_pop_fields {
  fill: Float
  make_price: Float
  make_stock: Float
  make_value: Float
  take_price: Float
  take_value: Float
}

"""aggregate var_samp on columns"""
type marketplace_order_var_samp_fields {
  fill: Float
  make_price: Float
  make_stock: Float
  make_value: Float
  take_price: Float
  take_value: Float
}

"""aggregate variance on columns"""
type marketplace_order_variance_fields {
  fill: Float
  make_price: Float
  make_stock: Float
  make_value: Float
  take_price: Float
  take_value: Float
}

"""
columns and relationships of "metadata_collection"
"""
type metadata_collection {
  db_updated_at: timestamptz!
  id: String!
  metadata: String
  metadata_retries: Int!
  metadata_synced: Boolean!
}

"""
aggregated selection of "metadata_collection"
"""
type metadata_collection_aggregate {
  aggregate: metadata_collection_aggregate_fields
  nodes: [metadata_collection!]!
}

"""
aggregate fields of "metadata_collection"
"""
type metadata_collection_aggregate_fields {
  avg: metadata_collection_avg_fields
  count(columns: [metadata_collection_select_column!], distinct: Boolean): Int!
  max: metadata_collection_max_fields
  min: metadata_collection_min_fields
  stddev: metadata_collection_stddev_fields
  stddev_pop: metadata_collection_stddev_pop_fields
  stddev_samp: metadata_collection_stddev_samp_fields
  sum: metadata_collection_sum_fields
  var_pop: metadata_collection_var_pop_fields
  var_samp: metadata_collection_var_samp_fields
  variance: metadata_collection_variance_fields
}

"""aggregate avg on columns"""
type metadata_collection_avg_fields {
  metadata_retries: Float
}

"""
Boolean expression to filter rows from the table "metadata_collection". All fields are combined with a logical 'AND'.
"""
input metadata_collection_bool_exp {
  _and: [metadata_collection_bool_exp!]
  _not: metadata_collection_bool_exp
  _or: [metadata_collection_bool_exp!]
  db_updated_at: timestamptz_comparison_exp
  id: String_comparison_exp
  metadata: String_comparison_exp
  metadata_retries: Int_comparison_exp
  metadata_synced: Boolean_comparison_exp
}

"""
unique or primary key constraints on table "metadata_collection"
"""
enum metadata_collection_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  metadata_collection_pkey
}

"""
input type for incrementing numeric columns in table "metadata_collection"
"""
input metadata_collection_inc_input {
  metadata_retries: Int
}

"""
input type for inserting data into table "metadata_collection"
"""
input metadata_collection_insert_input {
  db_updated_at: timestamptz
  id: String
  metadata: String
  metadata_retries: Int
  metadata_synced: Boolean
}

"""aggregate max on columns"""
type metadata_collection_max_fields {
  db_updated_at: timestamptz
  id: String
  metadata: String
  metadata_retries: Int
}

"""aggregate min on columns"""
type metadata_collection_min_fields {
  db_updated_at: timestamptz
  id: String
  metadata: String
  metadata_retries: Int
}

"""
response of any mutation on the table "metadata_collection"
"""
type metadata_collection_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [metadata_collection!]!
}

"""
on_conflict condition type for table "metadata_collection"
"""
input metadata_collection_on_conflict {
  constraint: metadata_collection_constraint!
  update_columns: [metadata_collection_update_column!]! = []
  where: metadata_collection_bool_exp
}

"""Ordering options when selecting data from "metadata_collection"."""
input metadata_collection_order_by {
  db_updated_at: order_by
  id: order_by
  metadata: order_by
  metadata_retries: order_by
  metadata_synced: order_by
}

"""primary key columns input for table: metadata_collection"""
input metadata_collection_pk_columns_input {
  id: String!
}

"""
select columns of table "metadata_collection"
"""
enum metadata_collection_select_column {
  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  metadata_retries

  """column name"""
  metadata_synced
}

"""
input type for updating data in table "metadata_collection"
"""
input metadata_collection_set_input {
  db_updated_at: timestamptz
  id: String
  metadata: String
  metadata_retries: Int
  metadata_synced: Boolean
}

"""aggregate stddev on columns"""
type metadata_collection_stddev_fields {
  metadata_retries: Float
}

"""aggregate stddev_pop on columns"""
type metadata_collection_stddev_pop_fields {
  metadata_retries: Float
}

"""aggregate stddev_samp on columns"""
type metadata_collection_stddev_samp_fields {
  metadata_retries: Float
}

"""
Streaming cursor of the table "metadata_collection"
"""
input metadata_collection_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: metadata_collection_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input metadata_collection_stream_cursor_value_input {
  db_updated_at: timestamptz
  id: String
  metadata: String
  metadata_retries: Int
  metadata_synced: Boolean
}

"""aggregate sum on columns"""
type metadata_collection_sum_fields {
  metadata_retries: Int
}

"""
update columns of table "metadata_collection"
"""
enum metadata_collection_update_column {
  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  metadata_retries

  """column name"""
  metadata_synced
}

input metadata_collection_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: metadata_collection_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: metadata_collection_set_input

  """filter the rows which have to be updated"""
  where: metadata_collection_bool_exp!
}

"""aggregate var_pop on columns"""
type metadata_collection_var_pop_fields {
  metadata_retries: Float
}

"""aggregate var_samp on columns"""
type metadata_collection_var_samp_fields {
  metadata_retries: Float
}

"""aggregate variance on columns"""
type metadata_collection_variance_fields {
  metadata_retries: Float
}

"""
columns and relationships of "metadata_token"
"""
type metadata_token {
  contract: String!
  db_updated_at: timestamptz!
  id: uuid!
  metadata: String
  metadata_retries: Int!
  metadata_synced: Boolean!
  token_id: String!
}

"""
aggregated selection of "metadata_token"
"""
type metadata_token_aggregate {
  aggregate: metadata_token_aggregate_fields
  nodes: [metadata_token!]!
}

"""
aggregate fields of "metadata_token"
"""
type metadata_token_aggregate_fields {
  avg: metadata_token_avg_fields
  count(columns: [metadata_token_select_column!], distinct: Boolean): Int!
  max: metadata_token_max_fields
  min: metadata_token_min_fields
  stddev: metadata_token_stddev_fields
  stddev_pop: metadata_token_stddev_pop_fields
  stddev_samp: metadata_token_stddev_samp_fields
  sum: metadata_token_sum_fields
  var_pop: metadata_token_var_pop_fields
  var_samp: metadata_token_var_samp_fields
  variance: metadata_token_variance_fields
}

"""aggregate avg on columns"""
type metadata_token_avg_fields {
  metadata_retries: Float
}

"""
Boolean expression to filter rows from the table "metadata_token". All fields are combined with a logical 'AND'.
"""
input metadata_token_bool_exp {
  _and: [metadata_token_bool_exp!]
  _not: metadata_token_bool_exp
  _or: [metadata_token_bool_exp!]
  contract: String_comparison_exp
  db_updated_at: timestamptz_comparison_exp
  id: uuid_comparison_exp
  metadata: String_comparison_exp
  metadata_retries: Int_comparison_exp
  metadata_synced: Boolean_comparison_exp
  token_id: String_comparison_exp
}

"""
unique or primary key constraints on table "metadata_token"
"""
enum metadata_token_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  metadata_token_pkey
}

"""
input type for incrementing numeric columns in table "metadata_token"
"""
input metadata_token_inc_input {
  metadata_retries: Int
}

"""
input type for inserting data into table "metadata_token"
"""
input metadata_token_insert_input {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  metadata: String
  metadata_retries: Int
  metadata_synced: Boolean
  token_id: String
}

"""aggregate max on columns"""
type metadata_token_max_fields {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  metadata: String
  metadata_retries: Int
  token_id: String
}

"""aggregate min on columns"""
type metadata_token_min_fields {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  metadata: String
  metadata_retries: Int
  token_id: String
}

"""
response of any mutation on the table "metadata_token"
"""
type metadata_token_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [metadata_token!]!
}

"""
on_conflict condition type for table "metadata_token"
"""
input metadata_token_on_conflict {
  constraint: metadata_token_constraint!
  update_columns: [metadata_token_update_column!]! = []
  where: metadata_token_bool_exp
}

"""Ordering options when selecting data from "metadata_token"."""
input metadata_token_order_by {
  contract: order_by
  db_updated_at: order_by
  id: order_by
  metadata: order_by
  metadata_retries: order_by
  metadata_synced: order_by
  token_id: order_by
}

"""primary key columns input for table: metadata_token"""
input metadata_token_pk_columns_input {
  id: uuid!
}

"""
select columns of table "metadata_token"
"""
enum metadata_token_select_column {
  """column name"""
  contract

  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  metadata_retries

  """column name"""
  metadata_synced

  """column name"""
  token_id
}

"""
input type for updating data in table "metadata_token"
"""
input metadata_token_set_input {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  metadata: String
  metadata_retries: Int
  metadata_synced: Boolean
  token_id: String
}

"""aggregate stddev on columns"""
type metadata_token_stddev_fields {
  metadata_retries: Float
}

"""aggregate stddev_pop on columns"""
type metadata_token_stddev_pop_fields {
  metadata_retries: Float
}

"""aggregate stddev_samp on columns"""
type metadata_token_stddev_samp_fields {
  metadata_retries: Float
}

"""
Streaming cursor of the table "metadata_token"
"""
input metadata_token_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: metadata_token_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input metadata_token_stream_cursor_value_input {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  metadata: String
  metadata_retries: Int
  metadata_synced: Boolean
  token_id: String
}

"""aggregate sum on columns"""
type metadata_token_sum_fields {
  metadata_retries: Int
}

"""
update columns of table "metadata_token"
"""
enum metadata_token_update_column {
  """column name"""
  contract

  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  metadata_retries

  """column name"""
  metadata_synced

  """column name"""
  token_id
}

input metadata_token_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: metadata_token_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: metadata_token_set_input

  """filter the rows which have to be updated"""
  where: metadata_token_bool_exp!
}

"""aggregate var_pop on columns"""
type metadata_token_var_pop_fields {
  metadata_retries: Float
}

"""aggregate var_samp on columns"""
type metadata_token_var_samp_fields {
  metadata_retries: Float
}

"""aggregate variance on columns"""
type metadata_token_variance_fields {
  metadata_retries: Float
}

"""mutation root"""
type mutation_root {
  """
  delete data from the table: "aggregator_event"
  """
  delete_aggregator_event(
    """filter the rows which have to be deleted"""
    where: aggregator_event_bool_exp!
  ): aggregator_event_mutation_response

  """
  delete single row from the table: "aggregator_event"
  """
  delete_aggregator_event_by_pk(id: bigint!): aggregator_event

  """
  delete data from the table: "collection"
  """
  delete_collection(
    """filter the rows which have to be deleted"""
    where: collection_bool_exp!
  ): collection_mutation_response

  """
  delete single row from the table: "collection"
  """
  delete_collection_by_pk(id: String!): collection

  """
  delete data from the table: "dipdup_contract"
  """
  delete_dipdup_contract(
    """filter the rows which have to be deleted"""
    where: dipdup_contract_bool_exp!
  ): dipdup_contract_mutation_response

  """
  delete single row from the table: "dipdup_contract"
  """
  delete_dipdup_contract_by_pk(name: String!): dipdup_contract

  """
  delete data from the table: "dipdup_contract_metadata"
  """
  delete_dipdup_contract_metadata(
    """filter the rows which have to be deleted"""
    where: dipdup_contract_metadata_bool_exp!
  ): dipdup_contract_metadata_mutation_response

  """
  delete single row from the table: "dipdup_contract_metadata"
  """
  delete_dipdup_contract_metadata_by_pk(id: Int!): dipdup_contract_metadata

  """
  delete data from the table: "dipdup_head"
  """
  delete_dipdup_head(
    """filter the rows which have to be deleted"""
    where: dipdup_head_bool_exp!
  ): dipdup_head_mutation_response

  """
  delete single row from the table: "dipdup_head"
  """
  delete_dipdup_head_by_pk(name: String!): dipdup_head

  """
  delete data from the table: "dipdup_head_status"
  """
  delete_dipdup_head_status(
    """filter the rows which have to be deleted"""
    where: dipdup_head_status_bool_exp!
  ): dipdup_head_status_mutation_response

  """
  delete data from the table: "dipdup_index"
  """
  delete_dipdup_index(
    """filter the rows which have to be deleted"""
    where: dipdup_index_bool_exp!
  ): dipdup_index_mutation_response

  """
  delete single row from the table: "dipdup_index"
  """
  delete_dipdup_index_by_pk(name: String!): dipdup_index

  """
  delete data from the table: "dipdup_model_update"
  """
  delete_dipdup_model_update(
    """filter the rows which have to be deleted"""
    where: dipdup_model_update_bool_exp!
  ): dipdup_model_update_mutation_response

  """
  delete single row from the table: "dipdup_model_update"
  """
  delete_dipdup_model_update_by_pk(id: Int!): dipdup_model_update

  """
  delete data from the table: "dipdup_schema"
  """
  delete_dipdup_schema(
    """filter the rows which have to be deleted"""
    where: dipdup_schema_bool_exp!
  ): dipdup_schema_mutation_response

  """
  delete single row from the table: "dipdup_schema"
  """
  delete_dipdup_schema_by_pk(name: String!): dipdup_schema

  """
  delete data from the table: "dipdup_token_metadata"
  """
  delete_dipdup_token_metadata(
    """filter the rows which have to be deleted"""
    where: dipdup_token_metadata_bool_exp!
  ): dipdup_token_metadata_mutation_response

  """
  delete single row from the table: "dipdup_token_metadata"
  """
  delete_dipdup_token_metadata_by_pk(id: Int!): dipdup_token_metadata

  """
  delete data from the table: "indexing_status"
  """
  delete_indexing_status(
    """filter the rows which have to be deleted"""
    where: indexing_status_bool_exp!
  ): indexing_status_mutation_response

  """
  delete single row from the table: "indexing_status"
  """
  delete_indexing_status_by_pk(
    """
    COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
    """
    index: String!
  ): indexing_status

  """
  delete data from the table: "legacy_orders"
  """
  delete_legacy_orders(
    """filter the rows which have to be deleted"""
    where: legacy_orders_bool_exp!
  ): legacy_orders_mutation_response

  """
  delete single row from the table: "legacy_orders"
  """
  delete_legacy_orders_by_pk(hash: String!): legacy_orders

  """
  delete data from the table: "marketplace_activity"
  """
  delete_marketplace_activity(
    """filter the rows which have to be deleted"""
    where: marketplace_activity_bool_exp!
  ): marketplace_activity_mutation_response

  """
  delete single row from the table: "marketplace_activity"
  """
  delete_marketplace_activity_by_pk(id: uuid!): marketplace_activity

  """
  delete data from the table: "marketplace_order"
  """
  delete_marketplace_order(
    """filter the rows which have to be deleted"""
    where: marketplace_order_bool_exp!
  ): marketplace_order_mutation_response

  """
  delete single row from the table: "marketplace_order"
  """
  delete_marketplace_order_by_pk(id: uuid!): marketplace_order

  """
  delete data from the table: "metadata_collection"
  """
  delete_metadata_collection(
    """filter the rows which have to be deleted"""
    where: metadata_collection_bool_exp!
  ): metadata_collection_mutation_response

  """
  delete single row from the table: "metadata_collection"
  """
  delete_metadata_collection_by_pk(id: String!): metadata_collection

  """
  delete data from the table: "metadata_token"
  """
  delete_metadata_token(
    """filter the rows which have to be deleted"""
    where: metadata_token_bool_exp!
  ): metadata_token_mutation_response

  """
  delete single row from the table: "metadata_token"
  """
  delete_metadata_token_by_pk(id: uuid!): metadata_token

  """
  delete data from the table: "ownership"
  """
  delete_ownership(
    """filter the rows which have to be deleted"""
    where: ownership_bool_exp!
  ): ownership_mutation_response

  """
  delete single row from the table: "ownership"
  """
  delete_ownership_by_pk(id: uuid!): ownership

  """
  delete data from the table: "royalties"
  """
  delete_royalties(
    """filter the rows which have to be deleted"""
    where: royalties_bool_exp!
  ): royalties_mutation_response

  """
  delete single row from the table: "royalties"
  """
  delete_royalties_by_pk(id: uuid!): royalties

  """
  delete data from the table: "tasks"
  """
  delete_tasks(
    """filter the rows which have to be deleted"""
    where: tasks_bool_exp!
  ): tasks_mutation_response

  """
  delete single row from the table: "tasks"
  """
  delete_tasks_by_pk(id: bigint!): tasks

  """
  delete data from the table: "tezos_domains_domain"
  """
  delete_tezos_domains_domain(
    """filter the rows which have to be deleted"""
    where: tezos_domains_domain_bool_exp!
  ): tezos_domains_domain_mutation_response

  """
  delete single row from the table: "tezos_domains_domain"
  """
  delete_tezos_domains_domain_by_pk(id: String!): tezos_domains_domain

  """
  delete data from the table: "tezos_domains_record"
  """
  delete_tezos_domains_record(
    """filter the rows which have to be deleted"""
    where: tezos_domains_record_bool_exp!
  ): tezos_domains_record_mutation_response

  """
  delete single row from the table: "tezos_domains_record"
  """
  delete_tezos_domains_record_by_pk(id: String!): tezos_domains_record

  """
  delete data from the table: "tezos_domains_tld"
  """
  delete_tezos_domains_tld(
    """filter the rows which have to be deleted"""
    where: tezos_domains_tld_bool_exp!
  ): tezos_domains_tld_mutation_response

  """
  delete single row from the table: "tezos_domains_tld"
  """
  delete_tezos_domains_tld_by_pk(id: String!): tezos_domains_tld

  """
  delete data from the table: "token"
  """
  delete_token(
    """filter the rows which have to be deleted"""
    where: token_bool_exp!
  ): token_mutation_response

  """
  delete single row from the table: "token"
  """
  delete_token_by_pk(id: uuid!): token

  """
  delete data from the table: "token_transfer"
  """
  delete_token_transfer(
    """filter the rows which have to be deleted"""
    where: token_transfer_bool_exp!
  ): token_transfer_mutation_response

  """
  delete single row from the table: "token_transfer"
  """
  delete_token_transfer_by_pk(id: bigint!): token_transfer

  """
  delete data from the table: "tzprofiles"
  """
  delete_tzprofiles(
    """filter the rows which have to be deleted"""
    where: tzprofiles_bool_exp!
  ): tzprofiles_mutation_response

  """
  delete single row from the table: "tzprofiles"
  """
  delete_tzprofiles_by_pk(account: String!): tzprofiles

  """
  insert data into the table: "aggregator_event"
  """
  insert_aggregator_event(
    """the rows to be inserted"""
    objects: [aggregator_event_insert_input!]!

    """upsert condition"""
    on_conflict: aggregator_event_on_conflict
  ): aggregator_event_mutation_response

  """
  insert a single row into the table: "aggregator_event"
  """
  insert_aggregator_event_one(
    """the row to be inserted"""
    object: aggregator_event_insert_input!

    """upsert condition"""
    on_conflict: aggregator_event_on_conflict
  ): aggregator_event

  """
  insert data into the table: "collection"
  """
  insert_collection(
    """the rows to be inserted"""
    objects: [collection_insert_input!]!

    """upsert condition"""
    on_conflict: collection_on_conflict
  ): collection_mutation_response

  """
  insert a single row into the table: "collection"
  """
  insert_collection_one(
    """the row to be inserted"""
    object: collection_insert_input!

    """upsert condition"""
    on_conflict: collection_on_conflict
  ): collection

  """
  insert data into the table: "dipdup_contract"
  """
  insert_dipdup_contract(
    """the rows to be inserted"""
    objects: [dipdup_contract_insert_input!]!

    """upsert condition"""
    on_conflict: dipdup_contract_on_conflict
  ): dipdup_contract_mutation_response

  """
  insert data into the table: "dipdup_contract_metadata"
  """
  insert_dipdup_contract_metadata(
    """the rows to be inserted"""
    objects: [dipdup_contract_metadata_insert_input!]!

    """upsert condition"""
    on_conflict: dipdup_contract_metadata_on_conflict
  ): dipdup_contract_metadata_mutation_response

  """
  insert a single row into the table: "dipdup_contract_metadata"
  """
  insert_dipdup_contract_metadata_one(
    """the row to be inserted"""
    object: dipdup_contract_metadata_insert_input!

    """upsert condition"""
    on_conflict: dipdup_contract_metadata_on_conflict
  ): dipdup_contract_metadata

  """
  insert a single row into the table: "dipdup_contract"
  """
  insert_dipdup_contract_one(
    """the row to be inserted"""
    object: dipdup_contract_insert_input!

    """upsert condition"""
    on_conflict: dipdup_contract_on_conflict
  ): dipdup_contract

  """
  insert data into the table: "dipdup_head"
  """
  insert_dipdup_head(
    """the rows to be inserted"""
    objects: [dipdup_head_insert_input!]!

    """upsert condition"""
    on_conflict: dipdup_head_on_conflict
  ): dipdup_head_mutation_response

  """
  insert a single row into the table: "dipdup_head"
  """
  insert_dipdup_head_one(
    """the row to be inserted"""
    object: dipdup_head_insert_input!

    """upsert condition"""
    on_conflict: dipdup_head_on_conflict
  ): dipdup_head

  """
  insert data into the table: "dipdup_head_status"
  """
  insert_dipdup_head_status(
    """the rows to be inserted"""
    objects: [dipdup_head_status_insert_input!]!
  ): dipdup_head_status_mutation_response

  """
  insert a single row into the table: "dipdup_head_status"
  """
  insert_dipdup_head_status_one(
    """the row to be inserted"""
    object: dipdup_head_status_insert_input!
  ): dipdup_head_status

  """
  insert data into the table: "dipdup_index"
  """
  insert_dipdup_index(
    """the rows to be inserted"""
    objects: [dipdup_index_insert_input!]!

    """upsert condition"""
    on_conflict: dipdup_index_on_conflict
  ): dipdup_index_mutation_response

  """
  insert a single row into the table: "dipdup_index"
  """
  insert_dipdup_index_one(
    """the row to be inserted"""
    object: dipdup_index_insert_input!

    """upsert condition"""
    on_conflict: dipdup_index_on_conflict
  ): dipdup_index

  """
  insert data into the table: "dipdup_model_update"
  """
  insert_dipdup_model_update(
    """the rows to be inserted"""
    objects: [dipdup_model_update_insert_input!]!

    """upsert condition"""
    on_conflict: dipdup_model_update_on_conflict
  ): dipdup_model_update_mutation_response

  """
  insert a single row into the table: "dipdup_model_update"
  """
  insert_dipdup_model_update_one(
    """the row to be inserted"""
    object: dipdup_model_update_insert_input!

    """upsert condition"""
    on_conflict: dipdup_model_update_on_conflict
  ): dipdup_model_update

  """
  insert data into the table: "dipdup_schema"
  """
  insert_dipdup_schema(
    """the rows to be inserted"""
    objects: [dipdup_schema_insert_input!]!

    """upsert condition"""
    on_conflict: dipdup_schema_on_conflict
  ): dipdup_schema_mutation_response

  """
  insert a single row into the table: "dipdup_schema"
  """
  insert_dipdup_schema_one(
    """the row to be inserted"""
    object: dipdup_schema_insert_input!

    """upsert condition"""
    on_conflict: dipdup_schema_on_conflict
  ): dipdup_schema

  """
  insert data into the table: "dipdup_token_metadata"
  """
  insert_dipdup_token_metadata(
    """the rows to be inserted"""
    objects: [dipdup_token_metadata_insert_input!]!

    """upsert condition"""
    on_conflict: dipdup_token_metadata_on_conflict
  ): dipdup_token_metadata_mutation_response

  """
  insert a single row into the table: "dipdup_token_metadata"
  """
  insert_dipdup_token_metadata_one(
    """the row to be inserted"""
    object: dipdup_token_metadata_insert_input!

    """upsert condition"""
    on_conflict: dipdup_token_metadata_on_conflict
  ): dipdup_token_metadata

  """
  insert data into the table: "indexing_status"
  """
  insert_indexing_status(
    """the rows to be inserted"""
    objects: [indexing_status_insert_input!]!

    """upsert condition"""
    on_conflict: indexing_status_on_conflict
  ): indexing_status_mutation_response

  """
  insert a single row into the table: "indexing_status"
  """
  insert_indexing_status_one(
    """the row to be inserted"""
    object: indexing_status_insert_input!

    """upsert condition"""
    on_conflict: indexing_status_on_conflict
  ): indexing_status

  """
  insert data into the table: "legacy_orders"
  """
  insert_legacy_orders(
    """the rows to be inserted"""
    objects: [legacy_orders_insert_input!]!

    """upsert condition"""
    on_conflict: legacy_orders_on_conflict
  ): legacy_orders_mutation_response

  """
  insert a single row into the table: "legacy_orders"
  """
  insert_legacy_orders_one(
    """the row to be inserted"""
    object: legacy_orders_insert_input!

    """upsert condition"""
    on_conflict: legacy_orders_on_conflict
  ): legacy_orders

  """
  insert data into the table: "marketplace_activity"
  """
  insert_marketplace_activity(
    """the rows to be inserted"""
    objects: [marketplace_activity_insert_input!]!

    """upsert condition"""
    on_conflict: marketplace_activity_on_conflict
  ): marketplace_activity_mutation_response

  """
  insert a single row into the table: "marketplace_activity"
  """
  insert_marketplace_activity_one(
    """the row to be inserted"""
    object: marketplace_activity_insert_input!

    """upsert condition"""
    on_conflict: marketplace_activity_on_conflict
  ): marketplace_activity

  """
  insert data into the table: "marketplace_order"
  """
  insert_marketplace_order(
    """the rows to be inserted"""
    objects: [marketplace_order_insert_input!]!

    """upsert condition"""
    on_conflict: marketplace_order_on_conflict
  ): marketplace_order_mutation_response

  """
  insert a single row into the table: "marketplace_order"
  """
  insert_marketplace_order_one(
    """the row to be inserted"""
    object: marketplace_order_insert_input!

    """upsert condition"""
    on_conflict: marketplace_order_on_conflict
  ): marketplace_order

  """
  insert data into the table: "metadata_collection"
  """
  insert_metadata_collection(
    """the rows to be inserted"""
    objects: [metadata_collection_insert_input!]!

    """upsert condition"""
    on_conflict: metadata_collection_on_conflict
  ): metadata_collection_mutation_response

  """
  insert a single row into the table: "metadata_collection"
  """
  insert_metadata_collection_one(
    """the row to be inserted"""
    object: metadata_collection_insert_input!

    """upsert condition"""
    on_conflict: metadata_collection_on_conflict
  ): metadata_collection

  """
  insert data into the table: "metadata_token"
  """
  insert_metadata_token(
    """the rows to be inserted"""
    objects: [metadata_token_insert_input!]!

    """upsert condition"""
    on_conflict: metadata_token_on_conflict
  ): metadata_token_mutation_response

  """
  insert a single row into the table: "metadata_token"
  """
  insert_metadata_token_one(
    """the row to be inserted"""
    object: metadata_token_insert_input!

    """upsert condition"""
    on_conflict: metadata_token_on_conflict
  ): metadata_token

  """
  insert data into the table: "ownership"
  """
  insert_ownership(
    """the rows to be inserted"""
    objects: [ownership_insert_input!]!

    """upsert condition"""
    on_conflict: ownership_on_conflict
  ): ownership_mutation_response

  """
  insert a single row into the table: "ownership"
  """
  insert_ownership_one(
    """the row to be inserted"""
    object: ownership_insert_input!

    """upsert condition"""
    on_conflict: ownership_on_conflict
  ): ownership

  """
  insert data into the table: "royalties"
  """
  insert_royalties(
    """the rows to be inserted"""
    objects: [royalties_insert_input!]!

    """upsert condition"""
    on_conflict: royalties_on_conflict
  ): royalties_mutation_response

  """
  insert a single row into the table: "royalties"
  """
  insert_royalties_one(
    """the row to be inserted"""
    object: royalties_insert_input!

    """upsert condition"""
    on_conflict: royalties_on_conflict
  ): royalties

  """
  insert data into the table: "tasks"
  """
  insert_tasks(
    """the rows to be inserted"""
    objects: [tasks_insert_input!]!

    """upsert condition"""
    on_conflict: tasks_on_conflict
  ): tasks_mutation_response

  """
  insert a single row into the table: "tasks"
  """
  insert_tasks_one(
    """the row to be inserted"""
    object: tasks_insert_input!

    """upsert condition"""
    on_conflict: tasks_on_conflict
  ): tasks

  """
  insert data into the table: "tezos_domains_domain"
  """
  insert_tezos_domains_domain(
    """the rows to be inserted"""
    objects: [tezos_domains_domain_insert_input!]!

    """upsert condition"""
    on_conflict: tezos_domains_domain_on_conflict
  ): tezos_domains_domain_mutation_response

  """
  insert a single row into the table: "tezos_domains_domain"
  """
  insert_tezos_domains_domain_one(
    """the row to be inserted"""
    object: tezos_domains_domain_insert_input!

    """upsert condition"""
    on_conflict: tezos_domains_domain_on_conflict
  ): tezos_domains_domain

  """
  insert data into the table: "tezos_domains_record"
  """
  insert_tezos_domains_record(
    """the rows to be inserted"""
    objects: [tezos_domains_record_insert_input!]!

    """upsert condition"""
    on_conflict: tezos_domains_record_on_conflict
  ): tezos_domains_record_mutation_response

  """
  insert a single row into the table: "tezos_domains_record"
  """
  insert_tezos_domains_record_one(
    """the row to be inserted"""
    object: tezos_domains_record_insert_input!

    """upsert condition"""
    on_conflict: tezos_domains_record_on_conflict
  ): tezos_domains_record

  """
  insert data into the table: "tezos_domains_tld"
  """
  insert_tezos_domains_tld(
    """the rows to be inserted"""
    objects: [tezos_domains_tld_insert_input!]!

    """upsert condition"""
    on_conflict: tezos_domains_tld_on_conflict
  ): tezos_domains_tld_mutation_response

  """
  insert a single row into the table: "tezos_domains_tld"
  """
  insert_tezos_domains_tld_one(
    """the row to be inserted"""
    object: tezos_domains_tld_insert_input!

    """upsert condition"""
    on_conflict: tezos_domains_tld_on_conflict
  ): tezos_domains_tld

  """
  insert data into the table: "token"
  """
  insert_token(
    """the rows to be inserted"""
    objects: [token_insert_input!]!

    """upsert condition"""
    on_conflict: token_on_conflict
  ): token_mutation_response

  """
  insert a single row into the table: "token"
  """
  insert_token_one(
    """the row to be inserted"""
    object: token_insert_input!

    """upsert condition"""
    on_conflict: token_on_conflict
  ): token

  """
  insert data into the table: "token_transfer"
  """
  insert_token_transfer(
    """the rows to be inserted"""
    objects: [token_transfer_insert_input!]!

    """upsert condition"""
    on_conflict: token_transfer_on_conflict
  ): token_transfer_mutation_response

  """
  insert a single row into the table: "token_transfer"
  """
  insert_token_transfer_one(
    """the row to be inserted"""
    object: token_transfer_insert_input!

    """upsert condition"""
    on_conflict: token_transfer_on_conflict
  ): token_transfer

  """
  insert data into the table: "tzprofiles"
  """
  insert_tzprofiles(
    """the rows to be inserted"""
    objects: [tzprofiles_insert_input!]!

    """upsert condition"""
    on_conflict: tzprofiles_on_conflict
  ): tzprofiles_mutation_response

  """
  insert a single row into the table: "tzprofiles"
  """
  insert_tzprofiles_one(
    """the row to be inserted"""
    object: tzprofiles_insert_input!

    """upsert condition"""
    on_conflict: tzprofiles_on_conflict
  ): tzprofiles

  """
  update data of the table: "aggregator_event"
  """
  update_aggregator_event(
    """increments the numeric columns with given value of the filtered values"""
    _inc: aggregator_event_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: aggregator_event_set_input

    """filter the rows which have to be updated"""
    where: aggregator_event_bool_exp!
  ): aggregator_event_mutation_response

  """
  update single row of the table: "aggregator_event"
  """
  update_aggregator_event_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: aggregator_event_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: aggregator_event_set_input
    pk_columns: aggregator_event_pk_columns_input!
  ): aggregator_event

  """
  update multiples rows of table: "aggregator_event"
  """
  update_aggregator_event_many(
    """updates to execute, in order"""
    updates: [aggregator_event_updates!]!
  ): [aggregator_event_mutation_response]

  """
  update data of the table: "collection"
  """
  update_collection(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: collection_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: collection_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: collection_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: collection_delete_key_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: collection_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: collection_set_input

    """filter the rows which have to be updated"""
    where: collection_bool_exp!
  ): collection_mutation_response

  """
  update single row of the table: "collection"
  """
  update_collection_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: collection_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: collection_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: collection_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: collection_delete_key_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: collection_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: collection_set_input
    pk_columns: collection_pk_columns_input!
  ): collection

  """
  update multiples rows of table: "collection"
  """
  update_collection_many(
    """updates to execute, in order"""
    updates: [collection_updates!]!
  ): [collection_mutation_response]

  """
  update data of the table: "dipdup_contract"
  """
  update_dipdup_contract(
    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_contract_set_input

    """filter the rows which have to be updated"""
    where: dipdup_contract_bool_exp!
  ): dipdup_contract_mutation_response

  """
  update single row of the table: "dipdup_contract"
  """
  update_dipdup_contract_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_contract_set_input
    pk_columns: dipdup_contract_pk_columns_input!
  ): dipdup_contract

  """
  update multiples rows of table: "dipdup_contract"
  """
  update_dipdup_contract_many(
    """updates to execute, in order"""
    updates: [dipdup_contract_updates!]!
  ): [dipdup_contract_mutation_response]

  """
  update data of the table: "dipdup_contract_metadata"
  """
  update_dipdup_contract_metadata(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_contract_metadata_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_contract_metadata_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_contract_metadata_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_contract_metadata_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_contract_metadata_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_contract_metadata_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_contract_metadata_set_input

    """filter the rows which have to be updated"""
    where: dipdup_contract_metadata_bool_exp!
  ): dipdup_contract_metadata_mutation_response

  """
  update single row of the table: "dipdup_contract_metadata"
  """
  update_dipdup_contract_metadata_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_contract_metadata_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_contract_metadata_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_contract_metadata_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_contract_metadata_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_contract_metadata_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_contract_metadata_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_contract_metadata_set_input
    pk_columns: dipdup_contract_metadata_pk_columns_input!
  ): dipdup_contract_metadata

  """
  update multiples rows of table: "dipdup_contract_metadata"
  """
  update_dipdup_contract_metadata_many(
    """updates to execute, in order"""
    updates: [dipdup_contract_metadata_updates!]!
  ): [dipdup_contract_metadata_mutation_response]

  """
  update data of the table: "dipdup_head"
  """
  update_dipdup_head(
    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_head_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_head_set_input

    """filter the rows which have to be updated"""
    where: dipdup_head_bool_exp!
  ): dipdup_head_mutation_response

  """
  update single row of the table: "dipdup_head"
  """
  update_dipdup_head_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_head_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_head_set_input
    pk_columns: dipdup_head_pk_columns_input!
  ): dipdup_head

  """
  update multiples rows of table: "dipdup_head"
  """
  update_dipdup_head_many(
    """updates to execute, in order"""
    updates: [dipdup_head_updates!]!
  ): [dipdup_head_mutation_response]

  """
  update data of the table: "dipdup_head_status"
  """
  update_dipdup_head_status(
    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_head_status_set_input

    """filter the rows which have to be updated"""
    where: dipdup_head_status_bool_exp!
  ): dipdup_head_status_mutation_response

  """
  update multiples rows of table: "dipdup_head_status"
  """
  update_dipdup_head_status_many(
    """updates to execute, in order"""
    updates: [dipdup_head_status_updates!]!
  ): [dipdup_head_status_mutation_response]

  """
  update data of the table: "dipdup_index"
  """
  update_dipdup_index(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_index_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_index_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_index_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_index_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_index_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_index_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_index_set_input

    """filter the rows which have to be updated"""
    where: dipdup_index_bool_exp!
  ): dipdup_index_mutation_response

  """
  update single row of the table: "dipdup_index"
  """
  update_dipdup_index_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_index_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_index_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_index_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_index_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_index_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_index_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_index_set_input
    pk_columns: dipdup_index_pk_columns_input!
  ): dipdup_index

  """
  update multiples rows of table: "dipdup_index"
  """
  update_dipdup_index_many(
    """updates to execute, in order"""
    updates: [dipdup_index_updates!]!
  ): [dipdup_index_mutation_response]

  """
  update data of the table: "dipdup_model_update"
  """
  update_dipdup_model_update(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_model_update_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_model_update_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_model_update_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_model_update_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_model_update_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_model_update_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_model_update_set_input

    """filter the rows which have to be updated"""
    where: dipdup_model_update_bool_exp!
  ): dipdup_model_update_mutation_response

  """
  update single row of the table: "dipdup_model_update"
  """
  update_dipdup_model_update_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_model_update_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_model_update_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_model_update_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_model_update_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_model_update_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_model_update_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_model_update_set_input
    pk_columns: dipdup_model_update_pk_columns_input!
  ): dipdup_model_update

  """
  update multiples rows of table: "dipdup_model_update"
  """
  update_dipdup_model_update_many(
    """updates to execute, in order"""
    updates: [dipdup_model_update_updates!]!
  ): [dipdup_model_update_mutation_response]

  """
  update data of the table: "dipdup_schema"
  """
  update_dipdup_schema(
    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_schema_set_input

    """filter the rows which have to be updated"""
    where: dipdup_schema_bool_exp!
  ): dipdup_schema_mutation_response

  """
  update single row of the table: "dipdup_schema"
  """
  update_dipdup_schema_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_schema_set_input
    pk_columns: dipdup_schema_pk_columns_input!
  ): dipdup_schema

  """
  update multiples rows of table: "dipdup_schema"
  """
  update_dipdup_schema_many(
    """updates to execute, in order"""
    updates: [dipdup_schema_updates!]!
  ): [dipdup_schema_mutation_response]

  """
  update data of the table: "dipdup_token_metadata"
  """
  update_dipdup_token_metadata(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_token_metadata_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_token_metadata_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_token_metadata_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_token_metadata_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_token_metadata_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_token_metadata_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_token_metadata_set_input

    """filter the rows which have to be updated"""
    where: dipdup_token_metadata_bool_exp!
  ): dipdup_token_metadata_mutation_response

  """
  update single row of the table: "dipdup_token_metadata"
  """
  update_dipdup_token_metadata_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: dipdup_token_metadata_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: dipdup_token_metadata_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: dipdup_token_metadata_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: dipdup_token_metadata_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: dipdup_token_metadata_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: dipdup_token_metadata_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: dipdup_token_metadata_set_input
    pk_columns: dipdup_token_metadata_pk_columns_input!
  ): dipdup_token_metadata

  """
  update multiples rows of table: "dipdup_token_metadata"
  """
  update_dipdup_token_metadata_many(
    """updates to execute, in order"""
    updates: [dipdup_token_metadata_updates!]!
  ): [dipdup_token_metadata_mutation_response]

  """
  update data of the table: "indexing_status"
  """
  update_indexing_status(
    """sets the columns of the filtered rows to the given values"""
    _set: indexing_status_set_input

    """filter the rows which have to be updated"""
    where: indexing_status_bool_exp!
  ): indexing_status_mutation_response

  """
  update single row of the table: "indexing_status"
  """
  update_indexing_status_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: indexing_status_set_input
    pk_columns: indexing_status_pk_columns_input!
  ): indexing_status

  """
  update multiples rows of table: "indexing_status"
  """
  update_indexing_status_many(
    """updates to execute, in order"""
    updates: [indexing_status_updates!]!
  ): [indexing_status_mutation_response]

  """
  update data of the table: "legacy_orders"
  """
  update_legacy_orders(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: legacy_orders_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: legacy_orders_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: legacy_orders_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: legacy_orders_delete_key_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: legacy_orders_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: legacy_orders_set_input

    """filter the rows which have to be updated"""
    where: legacy_orders_bool_exp!
  ): legacy_orders_mutation_response

  """
  update single row of the table: "legacy_orders"
  """
  update_legacy_orders_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: legacy_orders_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: legacy_orders_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: legacy_orders_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: legacy_orders_delete_key_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: legacy_orders_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: legacy_orders_set_input
    pk_columns: legacy_orders_pk_columns_input!
  ): legacy_orders

  """
  update multiples rows of table: "legacy_orders"
  """
  update_legacy_orders_many(
    """updates to execute, in order"""
    updates: [legacy_orders_updates!]!
  ): [legacy_orders_mutation_response]

  """
  update data of the table: "marketplace_activity"
  """
  update_marketplace_activity(
    """increments the numeric columns with given value of the filtered values"""
    _inc: marketplace_activity_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: marketplace_activity_set_input

    """filter the rows which have to be updated"""
    where: marketplace_activity_bool_exp!
  ): marketplace_activity_mutation_response

  """
  update single row of the table: "marketplace_activity"
  """
  update_marketplace_activity_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: marketplace_activity_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: marketplace_activity_set_input
    pk_columns: marketplace_activity_pk_columns_input!
  ): marketplace_activity

  """
  update multiples rows of table: "marketplace_activity"
  """
  update_marketplace_activity_many(
    """updates to execute, in order"""
    updates: [marketplace_activity_updates!]!
  ): [marketplace_activity_mutation_response]

  """
  update data of the table: "marketplace_order"
  """
  update_marketplace_order(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: marketplace_order_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: marketplace_order_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: marketplace_order_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: marketplace_order_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: marketplace_order_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: marketplace_order_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: marketplace_order_set_input

    """filter the rows which have to be updated"""
    where: marketplace_order_bool_exp!
  ): marketplace_order_mutation_response

  """
  update single row of the table: "marketplace_order"
  """
  update_marketplace_order_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: marketplace_order_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: marketplace_order_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: marketplace_order_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: marketplace_order_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: marketplace_order_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: marketplace_order_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: marketplace_order_set_input
    pk_columns: marketplace_order_pk_columns_input!
  ): marketplace_order

  """
  update multiples rows of table: "marketplace_order"
  """
  update_marketplace_order_many(
    """updates to execute, in order"""
    updates: [marketplace_order_updates!]!
  ): [marketplace_order_mutation_response]

  """
  update data of the table: "metadata_collection"
  """
  update_metadata_collection(
    """increments the numeric columns with given value of the filtered values"""
    _inc: metadata_collection_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: metadata_collection_set_input

    """filter the rows which have to be updated"""
    where: metadata_collection_bool_exp!
  ): metadata_collection_mutation_response

  """
  update single row of the table: "metadata_collection"
  """
  update_metadata_collection_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: metadata_collection_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: metadata_collection_set_input
    pk_columns: metadata_collection_pk_columns_input!
  ): metadata_collection

  """
  update multiples rows of table: "metadata_collection"
  """
  update_metadata_collection_many(
    """updates to execute, in order"""
    updates: [metadata_collection_updates!]!
  ): [metadata_collection_mutation_response]

  """
  update data of the table: "metadata_token"
  """
  update_metadata_token(
    """increments the numeric columns with given value of the filtered values"""
    _inc: metadata_token_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: metadata_token_set_input

    """filter the rows which have to be updated"""
    where: metadata_token_bool_exp!
  ): metadata_token_mutation_response

  """
  update single row of the table: "metadata_token"
  """
  update_metadata_token_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: metadata_token_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: metadata_token_set_input
    pk_columns: metadata_token_pk_columns_input!
  ): metadata_token

  """
  update multiples rows of table: "metadata_token"
  """
  update_metadata_token_many(
    """updates to execute, in order"""
    updates: [metadata_token_updates!]!
  ): [metadata_token_mutation_response]

  """
  update data of the table: "ownership"
  """
  update_ownership(
    """increments the numeric columns with given value of the filtered values"""
    _inc: ownership_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: ownership_set_input

    """filter the rows which have to be updated"""
    where: ownership_bool_exp!
  ): ownership_mutation_response

  """
  update single row of the table: "ownership"
  """
  update_ownership_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: ownership_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: ownership_set_input
    pk_columns: ownership_pk_columns_input!
  ): ownership

  """
  update multiples rows of table: "ownership"
  """
  update_ownership_many(
    """updates to execute, in order"""
    updates: [ownership_updates!]!
  ): [ownership_mutation_response]

  """
  update data of the table: "royalties"
  """
  update_royalties(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: royalties_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: royalties_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: royalties_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: royalties_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: royalties_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: royalties_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: royalties_set_input

    """filter the rows which have to be updated"""
    where: royalties_bool_exp!
  ): royalties_mutation_response

  """
  update single row of the table: "royalties"
  """
  update_royalties_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: royalties_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: royalties_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: royalties_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: royalties_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: royalties_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: royalties_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: royalties_set_input
    pk_columns: royalties_pk_columns_input!
  ): royalties

  """
  update multiples rows of table: "royalties"
  """
  update_royalties_many(
    """updates to execute, in order"""
    updates: [royalties_updates!]!
  ): [royalties_mutation_response]

  """
  update data of the table: "tasks"
  """
  update_tasks(
    """increments the numeric columns with given value of the filtered values"""
    _inc: tasks_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: tasks_set_input

    """filter the rows which have to be updated"""
    where: tasks_bool_exp!
  ): tasks_mutation_response

  """
  update single row of the table: "tasks"
  """
  update_tasks_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: tasks_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: tasks_set_input
    pk_columns: tasks_pk_columns_input!
  ): tasks

  """
  update multiples rows of table: "tasks"
  """
  update_tasks_many(
    """updates to execute, in order"""
    updates: [tasks_updates!]!
  ): [tasks_mutation_response]

  """
  update data of the table: "tezos_domains_domain"
  """
  update_tezos_domains_domain(
    """increments the numeric columns with given value of the filtered values"""
    _inc: tezos_domains_domain_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: tezos_domains_domain_set_input

    """filter the rows which have to be updated"""
    where: tezos_domains_domain_bool_exp!
  ): tezos_domains_domain_mutation_response

  """
  update single row of the table: "tezos_domains_domain"
  """
  update_tezos_domains_domain_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: tezos_domains_domain_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: tezos_domains_domain_set_input
    pk_columns: tezos_domains_domain_pk_columns_input!
  ): tezos_domains_domain

  """
  update multiples rows of table: "tezos_domains_domain"
  """
  update_tezos_domains_domain_many(
    """updates to execute, in order"""
    updates: [tezos_domains_domain_updates!]!
  ): [tezos_domains_domain_mutation_response]

  """
  update data of the table: "tezos_domains_record"
  """
  update_tezos_domains_record(
    """sets the columns of the filtered rows to the given values"""
    _set: tezos_domains_record_set_input

    """filter the rows which have to be updated"""
    where: tezos_domains_record_bool_exp!
  ): tezos_domains_record_mutation_response

  """
  update single row of the table: "tezos_domains_record"
  """
  update_tezos_domains_record_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: tezos_domains_record_set_input
    pk_columns: tezos_domains_record_pk_columns_input!
  ): tezos_domains_record

  """
  update multiples rows of table: "tezos_domains_record"
  """
  update_tezos_domains_record_many(
    """updates to execute, in order"""
    updates: [tezos_domains_record_updates!]!
  ): [tezos_domains_record_mutation_response]

  """
  update data of the table: "tezos_domains_tld"
  """
  update_tezos_domains_tld(
    """sets the columns of the filtered rows to the given values"""
    _set: tezos_domains_tld_set_input

    """filter the rows which have to be updated"""
    where: tezos_domains_tld_bool_exp!
  ): tezos_domains_tld_mutation_response

  """
  update single row of the table: "tezos_domains_tld"
  """
  update_tezos_domains_tld_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: tezos_domains_tld_set_input
    pk_columns: tezos_domains_tld_pk_columns_input!
  ): tezos_domains_tld

  """
  update multiples rows of table: "tezos_domains_tld"
  """
  update_tezos_domains_tld_many(
    """updates to execute, in order"""
    updates: [tezos_domains_tld_updates!]!
  ): [tezos_domains_tld_mutation_response]

  """
  update data of the table: "token"
  """
  update_token(
    """increments the numeric columns with given value of the filtered values"""
    _inc: token_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: token_set_input

    """filter the rows which have to be updated"""
    where: token_bool_exp!
  ): token_mutation_response

  """
  update single row of the table: "token"
  """
  update_token_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: token_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: token_set_input
    pk_columns: token_pk_columns_input!
  ): token

  """
  update multiples rows of table: "token"
  """
  update_token_many(
    """updates to execute, in order"""
    updates: [token_updates!]!
  ): [token_mutation_response]

  """
  update data of the table: "token_transfer"
  """
  update_token_transfer(
    """increments the numeric columns with given value of the filtered values"""
    _inc: token_transfer_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: token_transfer_set_input

    """filter the rows which have to be updated"""
    where: token_transfer_bool_exp!
  ): token_transfer_mutation_response

  """
  update single row of the table: "token_transfer"
  """
  update_token_transfer_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: token_transfer_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: token_transfer_set_input
    pk_columns: token_transfer_pk_columns_input!
  ): token_transfer

  """
  update multiples rows of table: "token_transfer"
  """
  update_token_transfer_many(
    """updates to execute, in order"""
    updates: [token_transfer_updates!]!
  ): [token_transfer_mutation_response]

  """
  update data of the table: "tzprofiles"
  """
  update_tzprofiles(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: tzprofiles_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: tzprofiles_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: tzprofiles_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: tzprofiles_delete_key_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: tzprofiles_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: tzprofiles_set_input

    """filter the rows which have to be updated"""
    where: tzprofiles_bool_exp!
  ): tzprofiles_mutation_response

  """
  update single row of the table: "tzprofiles"
  """
  update_tzprofiles_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: tzprofiles_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: tzprofiles_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: tzprofiles_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: tzprofiles_delete_key_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: tzprofiles_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: tzprofiles_set_input
    pk_columns: tzprofiles_pk_columns_input!
  ): tzprofiles

  """
  update multiples rows of table: "tzprofiles"
  """
  update_tzprofiles_many(
    """updates to execute, in order"""
    updates: [tzprofiles_updates!]!
  ): [tzprofiles_mutation_response]
}

scalar numeric

"""
Boolean expression to compare columns of type "numeric". All fields are combined with logical 'AND'.
"""
input numeric_comparison_exp {
  _eq: numeric
  _gt: numeric
  _gte: numeric
  _in: [numeric!]
  _is_null: Boolean
  _lt: numeric
  _lte: numeric
  _neq: numeric
  _nin: [numeric!]
}

"""column ordering options"""
enum order_by {
  """in ascending order, nulls last"""
  asc

  """in ascending order, nulls first"""
  asc_nulls_first

  """in ascending order, nulls last"""
  asc_nulls_last

  """in descending order, nulls first"""
  desc

  """in descending order, nulls first"""
  desc_nulls_first

  """in descending order, nulls last"""
  desc_nulls_last
}

"""
columns and relationships of "ownership"
"""
type ownership {
  balance: numeric!
  contract: String!
  created: timestamptz!
  id: uuid!
  owner: String!
  token_id: String!
  updated: timestamptz!
}

"""
aggregated selection of "ownership"
"""
type ownership_aggregate {
  aggregate: ownership_aggregate_fields
  nodes: [ownership!]!
}

"""
aggregate fields of "ownership"
"""
type ownership_aggregate_fields {
  avg: ownership_avg_fields
  count(columns: [ownership_select_column!], distinct: Boolean): Int!
  max: ownership_max_fields
  min: ownership_min_fields
  stddev: ownership_stddev_fields
  stddev_pop: ownership_stddev_pop_fields
  stddev_samp: ownership_stddev_samp_fields
  sum: ownership_sum_fields
  var_pop: ownership_var_pop_fields
  var_samp: ownership_var_samp_fields
  variance: ownership_variance_fields
}

"""aggregate avg on columns"""
type ownership_avg_fields {
  balance: Float
}

"""
Boolean expression to filter rows from the table "ownership". All fields are combined with a logical 'AND'.
"""
input ownership_bool_exp {
  _and: [ownership_bool_exp!]
  _not: ownership_bool_exp
  _or: [ownership_bool_exp!]
  balance: numeric_comparison_exp
  contract: String_comparison_exp
  created: timestamptz_comparison_exp
  id: uuid_comparison_exp
  owner: String_comparison_exp
  token_id: String_comparison_exp
  updated: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "ownership"
"""
enum ownership_constraint {
  """
  unique or primary key constraint on columns "token_id", "contract", "owner"
  """
  ownership_id

  """
  unique or primary key constraint on columns "id"
  """
  ownership_pkey
}

"""
input type for incrementing numeric columns in table "ownership"
"""
input ownership_inc_input {
  balance: numeric
}

"""
input type for inserting data into table "ownership"
"""
input ownership_insert_input {
  balance: numeric
  contract: String
  created: timestamptz
  id: uuid
  owner: String
  token_id: String
  updated: timestamptz
}

"""aggregate max on columns"""
type ownership_max_fields {
  balance: numeric
  contract: String
  created: timestamptz
  id: uuid
  owner: String
  token_id: String
  updated: timestamptz
}

"""aggregate min on columns"""
type ownership_min_fields {
  balance: numeric
  contract: String
  created: timestamptz
  id: uuid
  owner: String
  token_id: String
  updated: timestamptz
}

"""
response of any mutation on the table "ownership"
"""
type ownership_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [ownership!]!
}

"""
on_conflict condition type for table "ownership"
"""
input ownership_on_conflict {
  constraint: ownership_constraint!
  update_columns: [ownership_update_column!]! = []
  where: ownership_bool_exp
}

"""Ordering options when selecting data from "ownership"."""
input ownership_order_by {
  balance: order_by
  contract: order_by
  created: order_by
  id: order_by
  owner: order_by
  token_id: order_by
  updated: order_by
}

"""primary key columns input for table: ownership"""
input ownership_pk_columns_input {
  id: uuid!
}

"""
select columns of table "ownership"
"""
enum ownership_select_column {
  """column name"""
  balance

  """column name"""
  contract

  """column name"""
  created

  """column name"""
  id

  """column name"""
  owner

  """column name"""
  token_id

  """column name"""
  updated
}

"""
input type for updating data in table "ownership"
"""
input ownership_set_input {
  balance: numeric
  contract: String
  created: timestamptz
  id: uuid
  owner: String
  token_id: String
  updated: timestamptz
}

"""aggregate stddev on columns"""
type ownership_stddev_fields {
  balance: Float
}

"""aggregate stddev_pop on columns"""
type ownership_stddev_pop_fields {
  balance: Float
}

"""aggregate stddev_samp on columns"""
type ownership_stddev_samp_fields {
  balance: Float
}

"""
Streaming cursor of the table "ownership"
"""
input ownership_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: ownership_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input ownership_stream_cursor_value_input {
  balance: numeric
  contract: String
  created: timestamptz
  id: uuid
  owner: String
  token_id: String
  updated: timestamptz
}

"""aggregate sum on columns"""
type ownership_sum_fields {
  balance: numeric
}

"""
update columns of table "ownership"
"""
enum ownership_update_column {
  """column name"""
  balance

  """column name"""
  contract

  """column name"""
  created

  """column name"""
  id

  """column name"""
  owner

  """column name"""
  token_id

  """column name"""
  updated
}

input ownership_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: ownership_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: ownership_set_input

  """filter the rows which have to be updated"""
  where: ownership_bool_exp!
}

"""aggregate var_pop on columns"""
type ownership_var_pop_fields {
  balance: Float
}

"""aggregate var_samp on columns"""
type ownership_var_samp_fields {
  balance: Float
}

"""aggregate variance on columns"""
type ownership_variance_fields {
  balance: Float
}

type query_root {
  """
  fetch data from the table: "aggregator_event"
  """
  aggregator_event(
    """distinct select on columns"""
    distinct_on: [aggregator_event_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [aggregator_event_order_by!]

    """filter the rows returned"""
    where: aggregator_event_bool_exp
  ): [aggregator_event!]!

  """
  fetch aggregated fields from the table: "aggregator_event"
  """
  aggregator_event_aggregate(
    """distinct select on columns"""
    distinct_on: [aggregator_event_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [aggregator_event_order_by!]

    """filter the rows returned"""
    where: aggregator_event_bool_exp
  ): aggregator_event_aggregate!

  """
  fetch data from the table: "aggregator_event" using primary key columns
  """
  aggregator_event_by_pk(id: bigint!): aggregator_event

  """
  fetch data from the table: "collection"
  """
  collection(
    """distinct select on columns"""
    distinct_on: [collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_order_by!]

    """filter the rows returned"""
    where: collection_bool_exp
  ): [collection!]!

  """
  fetch aggregated fields from the table: "collection"
  """
  collection_aggregate(
    """distinct select on columns"""
    distinct_on: [collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_order_by!]

    """filter the rows returned"""
    where: collection_bool_exp
  ): collection_aggregate!

  """fetch data from the table: "collection" using primary key columns"""
  collection_by_pk(id: String!): collection

  """
  fetch data from the table: "collection_with_meta"
  """
  collection_with_meta(
    """distinct select on columns"""
    distinct_on: [collection_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_with_meta_order_by!]

    """filter the rows returned"""
    where: collection_with_meta_bool_exp
  ): [collection_with_meta!]!

  """
  fetch aggregated fields from the table: "collection_with_meta"
  """
  collection_with_meta_aggregate(
    """distinct select on columns"""
    distinct_on: [collection_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_with_meta_order_by!]

    """filter the rows returned"""
    where: collection_with_meta_bool_exp
  ): collection_with_meta_aggregate!

  """
  fetch data from the table: "dipdup_contract"
  """
  dipdup_contract(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_bool_exp
  ): [dipdup_contract!]!

  """
  fetch aggregated fields from the table: "dipdup_contract"
  """
  dipdup_contract_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_bool_exp
  ): dipdup_contract_aggregate!

  """fetch data from the table: "dipdup_contract" using primary key columns"""
  dipdup_contract_by_pk(name: String!): dipdup_contract

  """
  fetch data from the table: "dipdup_contract_metadata"
  """
  dipdup_contract_metadata(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_metadata_bool_exp
  ): [dipdup_contract_metadata!]!

  """
  fetch aggregated fields from the table: "dipdup_contract_metadata"
  """
  dipdup_contract_metadata_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_metadata_bool_exp
  ): dipdup_contract_metadata_aggregate!

  """
  fetch data from the table: "dipdup_contract_metadata" using primary key columns
  """
  dipdup_contract_metadata_by_pk(id: Int!): dipdup_contract_metadata

  """
  fetch data from the table: "dipdup_head"
  """
  dipdup_head(
    """distinct select on columns"""
    distinct_on: [dipdup_head_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_order_by!]

    """filter the rows returned"""
    where: dipdup_head_bool_exp
  ): [dipdup_head!]!

  """
  fetch aggregated fields from the table: "dipdup_head"
  """
  dipdup_head_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_head_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_order_by!]

    """filter the rows returned"""
    where: dipdup_head_bool_exp
  ): dipdup_head_aggregate!

  """fetch data from the table: "dipdup_head" using primary key columns"""
  dipdup_head_by_pk(name: String!): dipdup_head

  """
  fetch data from the table: "dipdup_head_status"
  """
  dipdup_head_status(
    """distinct select on columns"""
    distinct_on: [dipdup_head_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_status_order_by!]

    """filter the rows returned"""
    where: dipdup_head_status_bool_exp
  ): [dipdup_head_status!]!

  """
  fetch aggregated fields from the table: "dipdup_head_status"
  """
  dipdup_head_status_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_head_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_status_order_by!]

    """filter the rows returned"""
    where: dipdup_head_status_bool_exp
  ): dipdup_head_status_aggregate!

  """
  fetch data from the table: "dipdup_index"
  """
  dipdup_index(
    """distinct select on columns"""
    distinct_on: [dipdup_index_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_index_order_by!]

    """filter the rows returned"""
    where: dipdup_index_bool_exp
  ): [dipdup_index!]!

  """
  fetch aggregated fields from the table: "dipdup_index"
  """
  dipdup_index_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_index_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_index_order_by!]

    """filter the rows returned"""
    where: dipdup_index_bool_exp
  ): dipdup_index_aggregate!

  """fetch data from the table: "dipdup_index" using primary key columns"""
  dipdup_index_by_pk(name: String!): dipdup_index

  """
  fetch data from the table: "dipdup_model_update"
  """
  dipdup_model_update(
    """distinct select on columns"""
    distinct_on: [dipdup_model_update_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_model_update_order_by!]

    """filter the rows returned"""
    where: dipdup_model_update_bool_exp
  ): [dipdup_model_update!]!

  """
  fetch aggregated fields from the table: "dipdup_model_update"
  """
  dipdup_model_update_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_model_update_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_model_update_order_by!]

    """filter the rows returned"""
    where: dipdup_model_update_bool_exp
  ): dipdup_model_update_aggregate!

  """
  fetch data from the table: "dipdup_model_update" using primary key columns
  """
  dipdup_model_update_by_pk(id: Int!): dipdup_model_update

  """
  fetch data from the table: "dipdup_schema"
  """
  dipdup_schema(
    """distinct select on columns"""
    distinct_on: [dipdup_schema_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_schema_order_by!]

    """filter the rows returned"""
    where: dipdup_schema_bool_exp
  ): [dipdup_schema!]!

  """
  fetch aggregated fields from the table: "dipdup_schema"
  """
  dipdup_schema_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_schema_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_schema_order_by!]

    """filter the rows returned"""
    where: dipdup_schema_bool_exp
  ): dipdup_schema_aggregate!

  """fetch data from the table: "dipdup_schema" using primary key columns"""
  dipdup_schema_by_pk(name: String!): dipdup_schema

  """
  fetch data from the table: "dipdup_token_metadata"
  """
  dipdup_token_metadata(
    """distinct select on columns"""
    distinct_on: [dipdup_token_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_token_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_token_metadata_bool_exp
  ): [dipdup_token_metadata!]!

  """
  fetch aggregated fields from the table: "dipdup_token_metadata"
  """
  dipdup_token_metadata_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_token_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_token_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_token_metadata_bool_exp
  ): dipdup_token_metadata_aggregate!

  """
  fetch data from the table: "dipdup_token_metadata" using primary key columns
  """
  dipdup_token_metadata_by_pk(id: Int!): dipdup_token_metadata

  """
  fetch data from the table: "indexing_status"
  """
  indexing_status(
    """distinct select on columns"""
    distinct_on: [indexing_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [indexing_status_order_by!]

    """filter the rows returned"""
    where: indexing_status_bool_exp
  ): [indexing_status!]!

  """
  fetch aggregated fields from the table: "indexing_status"
  """
  indexing_status_aggregate(
    """distinct select on columns"""
    distinct_on: [indexing_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [indexing_status_order_by!]

    """filter the rows returned"""
    where: indexing_status_bool_exp
  ): indexing_status_aggregate!

  """fetch data from the table: "indexing_status" using primary key columns"""
  indexing_status_by_pk(
    """
    COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
    """
    index: String!
  ): indexing_status

  """
  fetch data from the table: "legacy_orders"
  """
  legacy_orders(
    """distinct select on columns"""
    distinct_on: [legacy_orders_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [legacy_orders_order_by!]

    """filter the rows returned"""
    where: legacy_orders_bool_exp
  ): [legacy_orders!]!

  """
  fetch aggregated fields from the table: "legacy_orders"
  """
  legacy_orders_aggregate(
    """distinct select on columns"""
    distinct_on: [legacy_orders_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [legacy_orders_order_by!]

    """filter the rows returned"""
    where: legacy_orders_bool_exp
  ): legacy_orders_aggregate!

  """fetch data from the table: "legacy_orders" using primary key columns"""
  legacy_orders_by_pk(hash: String!): legacy_orders

  """
  fetch data from the table: "marketplace_activity"
  """
  marketplace_activity(
    """distinct select on columns"""
    distinct_on: [marketplace_activity_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_activity_order_by!]

    """filter the rows returned"""
    where: marketplace_activity_bool_exp
  ): [marketplace_activity!]!

  """
  fetch aggregated fields from the table: "marketplace_activity"
  """
  marketplace_activity_aggregate(
    """distinct select on columns"""
    distinct_on: [marketplace_activity_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_activity_order_by!]

    """filter the rows returned"""
    where: marketplace_activity_bool_exp
  ): marketplace_activity_aggregate!

  """
  fetch data from the table: "marketplace_activity" using primary key columns
  """
  marketplace_activity_by_pk(id: uuid!): marketplace_activity

  """
  fetch data from the table: "marketplace_order"
  """
  marketplace_order(
    """distinct select on columns"""
    distinct_on: [marketplace_order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_order_order_by!]

    """filter the rows returned"""
    where: marketplace_order_bool_exp
  ): [marketplace_order!]!

  """
  fetch aggregated fields from the table: "marketplace_order"
  """
  marketplace_order_aggregate(
    """distinct select on columns"""
    distinct_on: [marketplace_order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_order_order_by!]

    """filter the rows returned"""
    where: marketplace_order_bool_exp
  ): marketplace_order_aggregate!

  """
  fetch data from the table: "marketplace_order" using primary key columns
  """
  marketplace_order_by_pk(id: uuid!): marketplace_order

  """
  fetch data from the table: "metadata_collection"
  """
  metadata_collection(
    """distinct select on columns"""
    distinct_on: [metadata_collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_collection_order_by!]

    """filter the rows returned"""
    where: metadata_collection_bool_exp
  ): [metadata_collection!]!

  """
  fetch aggregated fields from the table: "metadata_collection"
  """
  metadata_collection_aggregate(
    """distinct select on columns"""
    distinct_on: [metadata_collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_collection_order_by!]

    """filter the rows returned"""
    where: metadata_collection_bool_exp
  ): metadata_collection_aggregate!

  """
  fetch data from the table: "metadata_collection" using primary key columns
  """
  metadata_collection_by_pk(id: String!): metadata_collection

  """
  fetch data from the table: "metadata_token"
  """
  metadata_token(
    """distinct select on columns"""
    distinct_on: [metadata_token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_token_order_by!]

    """filter the rows returned"""
    where: metadata_token_bool_exp
  ): [metadata_token!]!

  """
  fetch aggregated fields from the table: "metadata_token"
  """
  metadata_token_aggregate(
    """distinct select on columns"""
    distinct_on: [metadata_token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_token_order_by!]

    """filter the rows returned"""
    where: metadata_token_bool_exp
  ): metadata_token_aggregate!

  """fetch data from the table: "metadata_token" using primary key columns"""
  metadata_token_by_pk(id: uuid!): metadata_token

  """
  fetch data from the table: "ownership"
  """
  ownership(
    """distinct select on columns"""
    distinct_on: [ownership_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [ownership_order_by!]

    """filter the rows returned"""
    where: ownership_bool_exp
  ): [ownership!]!

  """
  fetch aggregated fields from the table: "ownership"
  """
  ownership_aggregate(
    """distinct select on columns"""
    distinct_on: [ownership_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [ownership_order_by!]

    """filter the rows returned"""
    where: ownership_bool_exp
  ): ownership_aggregate!

  """fetch data from the table: "ownership" using primary key columns"""
  ownership_by_pk(id: uuid!): ownership

  """
  fetch data from the table: "royalties"
  """
  royalties(
    """distinct select on columns"""
    distinct_on: [royalties_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [royalties_order_by!]

    """filter the rows returned"""
    where: royalties_bool_exp
  ): [royalties!]!

  """
  fetch aggregated fields from the table: "royalties"
  """
  royalties_aggregate(
    """distinct select on columns"""
    distinct_on: [royalties_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [royalties_order_by!]

    """filter the rows returned"""
    where: royalties_bool_exp
  ): royalties_aggregate!

  """fetch data from the table: "royalties" using primary key columns"""
  royalties_by_pk(id: uuid!): royalties

  """
  fetch data from the table: "tasks"
  """
  tasks(
    """distinct select on columns"""
    distinct_on: [tasks_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tasks_order_by!]

    """filter the rows returned"""
    where: tasks_bool_exp
  ): [tasks!]!

  """
  fetch aggregated fields from the table: "tasks"
  """
  tasks_aggregate(
    """distinct select on columns"""
    distinct_on: [tasks_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tasks_order_by!]

    """filter the rows returned"""
    where: tasks_bool_exp
  ): tasks_aggregate!

  """fetch data from the table: "tasks" using primary key columns"""
  tasks_by_pk(id: bigint!): tasks

  """
  fetch data from the table: "tezos_domains_domain"
  """
  tezos_domains_domain(
    """distinct select on columns"""
    distinct_on: [tezos_domains_domain_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_domain_order_by!]

    """filter the rows returned"""
    where: tezos_domains_domain_bool_exp
  ): [tezos_domains_domain!]!

  """
  fetch aggregated fields from the table: "tezos_domains_domain"
  """
  tezos_domains_domain_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_domain_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_domain_order_by!]

    """filter the rows returned"""
    where: tezos_domains_domain_bool_exp
  ): tezos_domains_domain_aggregate!

  """
  fetch data from the table: "tezos_domains_domain" using primary key columns
  """
  tezos_domains_domain_by_pk(id: String!): tezos_domains_domain

  """
  fetch data from the table: "tezos_domains_record"
  """
  tezos_domains_record(
    """distinct select on columns"""
    distinct_on: [tezos_domains_record_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_record_order_by!]

    """filter the rows returned"""
    where: tezos_domains_record_bool_exp
  ): [tezos_domains_record!]!

  """
  fetch aggregated fields from the table: "tezos_domains_record"
  """
  tezos_domains_record_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_record_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_record_order_by!]

    """filter the rows returned"""
    where: tezos_domains_record_bool_exp
  ): tezos_domains_record_aggregate!

  """
  fetch data from the table: "tezos_domains_record" using primary key columns
  """
  tezos_domains_record_by_pk(id: String!): tezos_domains_record

  """
  fetch data from the table: "tezos_domains_tld"
  """
  tezos_domains_tld(
    """distinct select on columns"""
    distinct_on: [tezos_domains_tld_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_tld_order_by!]

    """filter the rows returned"""
    where: tezos_domains_tld_bool_exp
  ): [tezos_domains_tld!]!

  """
  fetch aggregated fields from the table: "tezos_domains_tld"
  """
  tezos_domains_tld_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_tld_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_tld_order_by!]

    """filter the rows returned"""
    where: tezos_domains_tld_bool_exp
  ): tezos_domains_tld_aggregate!

  """
  fetch data from the table: "tezos_domains_tld" using primary key columns
  """
  tezos_domains_tld_by_pk(id: String!): tezos_domains_tld

  """
  fetch data from the table: "token"
  """
  token(
    """distinct select on columns"""
    distinct_on: [token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_order_by!]

    """filter the rows returned"""
    where: token_bool_exp
  ): [token!]!

  """
  fetch aggregated fields from the table: "token"
  """
  token_aggregate(
    """distinct select on columns"""
    distinct_on: [token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_order_by!]

    """filter the rows returned"""
    where: token_bool_exp
  ): token_aggregate!

  """
  fetch data from the table: "token_by_owner"
  """
  token_by_owner(
    """distinct select on columns"""
    distinct_on: [token_by_owner_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_by_owner_order_by!]

    """filter the rows returned"""
    where: token_by_owner_bool_exp
  ): [token_by_owner!]!

  """
  fetch aggregated fields from the table: "token_by_owner"
  """
  token_by_owner_aggregate(
    """distinct select on columns"""
    distinct_on: [token_by_owner_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_by_owner_order_by!]

    """filter the rows returned"""
    where: token_by_owner_bool_exp
  ): token_by_owner_aggregate!

  """fetch data from the table: "token" using primary key columns"""
  token_by_pk(id: uuid!): token

  """
  fetch data from the table: "token_transfer"
  """
  token_transfer(
    """distinct select on columns"""
    distinct_on: [token_transfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_transfer_order_by!]

    """filter the rows returned"""
    where: token_transfer_bool_exp
  ): [token_transfer!]!

  """
  fetch aggregated fields from the table: "token_transfer"
  """
  token_transfer_aggregate(
    """distinct select on columns"""
    distinct_on: [token_transfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_transfer_order_by!]

    """filter the rows returned"""
    where: token_transfer_bool_exp
  ): token_transfer_aggregate!

  """fetch data from the table: "token_transfer" using primary key columns"""
  token_transfer_by_pk(id: bigint!): token_transfer

  """
  fetch data from the table: "token_with_meta"
  """
  token_with_meta(
    """distinct select on columns"""
    distinct_on: [token_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_with_meta_order_by!]

    """filter the rows returned"""
    where: token_with_meta_bool_exp
  ): [token_with_meta!]!

  """
  fetch aggregated fields from the table: "token_with_meta"
  """
  token_with_meta_aggregate(
    """distinct select on columns"""
    distinct_on: [token_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_with_meta_order_by!]

    """filter the rows returned"""
    where: token_with_meta_bool_exp
  ): token_with_meta_aggregate!

  """
  fetch data from the table: "tzprofiles"
  """
  tzprofiles(
    """distinct select on columns"""
    distinct_on: [tzprofiles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tzprofiles_order_by!]

    """filter the rows returned"""
    where: tzprofiles_bool_exp
  ): [tzprofiles!]!

  """
  fetch aggregated fields from the table: "tzprofiles"
  """
  tzprofiles_aggregate(
    """distinct select on columns"""
    distinct_on: [tzprofiles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tzprofiles_order_by!]

    """filter the rows returned"""
    where: tzprofiles_bool_exp
  ): tzprofiles_aggregate!

  """fetch data from the table: "tzprofiles" using primary key columns"""
  tzprofiles_by_pk(account: String!): tzprofiles
}

"""
columns and relationships of "royalties"
"""
type royalties {
  contract: String!
  db_updated_at: timestamptz!
  id: uuid!
  parts(
    """JSON select path"""
    path: String
  ): jsonb!
  royalties_retries: Int!
  royalties_synced: Boolean!
  token_id: String!
}

"""
aggregated selection of "royalties"
"""
type royalties_aggregate {
  aggregate: royalties_aggregate_fields
  nodes: [royalties!]!
}

"""
aggregate fields of "royalties"
"""
type royalties_aggregate_fields {
  avg: royalties_avg_fields
  count(columns: [royalties_select_column!], distinct: Boolean): Int!
  max: royalties_max_fields
  min: royalties_min_fields
  stddev: royalties_stddev_fields
  stddev_pop: royalties_stddev_pop_fields
  stddev_samp: royalties_stddev_samp_fields
  sum: royalties_sum_fields
  var_pop: royalties_var_pop_fields
  var_samp: royalties_var_samp_fields
  variance: royalties_variance_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input royalties_append_input {
  parts: jsonb
}

"""aggregate avg on columns"""
type royalties_avg_fields {
  royalties_retries: Float
}

"""
Boolean expression to filter rows from the table "royalties". All fields are combined with a logical 'AND'.
"""
input royalties_bool_exp {
  _and: [royalties_bool_exp!]
  _not: royalties_bool_exp
  _or: [royalties_bool_exp!]
  contract: String_comparison_exp
  db_updated_at: timestamptz_comparison_exp
  id: uuid_comparison_exp
  parts: jsonb_comparison_exp
  royalties_retries: Int_comparison_exp
  royalties_synced: Boolean_comparison_exp
  token_id: String_comparison_exp
}

"""
unique or primary key constraints on table "royalties"
"""
enum royalties_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  royalties_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input royalties_delete_at_path_input {
  parts: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input royalties_delete_elem_input {
  parts: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input royalties_delete_key_input {
  parts: String
}

"""
input type for incrementing numeric columns in table "royalties"
"""
input royalties_inc_input {
  royalties_retries: Int
}

"""
input type for inserting data into table "royalties"
"""
input royalties_insert_input {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  parts: jsonb
  royalties_retries: Int
  royalties_synced: Boolean
  token_id: String
}

"""aggregate max on columns"""
type royalties_max_fields {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  royalties_retries: Int
  token_id: String
}

"""aggregate min on columns"""
type royalties_min_fields {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  royalties_retries: Int
  token_id: String
}

"""
response of any mutation on the table "royalties"
"""
type royalties_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [royalties!]!
}

"""
on_conflict condition type for table "royalties"
"""
input royalties_on_conflict {
  constraint: royalties_constraint!
  update_columns: [royalties_update_column!]! = []
  where: royalties_bool_exp
}

"""Ordering options when selecting data from "royalties"."""
input royalties_order_by {
  contract: order_by
  db_updated_at: order_by
  id: order_by
  parts: order_by
  royalties_retries: order_by
  royalties_synced: order_by
  token_id: order_by
}

"""primary key columns input for table: royalties"""
input royalties_pk_columns_input {
  id: uuid!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input royalties_prepend_input {
  parts: jsonb
}

"""
select columns of table "royalties"
"""
enum royalties_select_column {
  """column name"""
  contract

  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  parts

  """column name"""
  royalties_retries

  """column name"""
  royalties_synced

  """column name"""
  token_id
}

"""
input type for updating data in table "royalties"
"""
input royalties_set_input {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  parts: jsonb
  royalties_retries: Int
  royalties_synced: Boolean
  token_id: String
}

"""aggregate stddev on columns"""
type royalties_stddev_fields {
  royalties_retries: Float
}

"""aggregate stddev_pop on columns"""
type royalties_stddev_pop_fields {
  royalties_retries: Float
}

"""aggregate stddev_samp on columns"""
type royalties_stddev_samp_fields {
  royalties_retries: Float
}

"""
Streaming cursor of the table "royalties"
"""
input royalties_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: royalties_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input royalties_stream_cursor_value_input {
  contract: String
  db_updated_at: timestamptz
  id: uuid
  parts: jsonb
  royalties_retries: Int
  royalties_synced: Boolean
  token_id: String
}

"""aggregate sum on columns"""
type royalties_sum_fields {
  royalties_retries: Int
}

"""
update columns of table "royalties"
"""
enum royalties_update_column {
  """column name"""
  contract

  """column name"""
  db_updated_at

  """column name"""
  id

  """column name"""
  parts

  """column name"""
  royalties_retries

  """column name"""
  royalties_synced

  """column name"""
  token_id
}

input royalties_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: royalties_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: royalties_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: royalties_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: royalties_delete_key_input

  """increments the numeric columns with given value of the filtered values"""
  _inc: royalties_inc_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: royalties_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: royalties_set_input

  """filter the rows which have to be updated"""
  where: royalties_bool_exp!
}

"""aggregate var_pop on columns"""
type royalties_var_pop_fields {
  royalties_retries: Float
}

"""aggregate var_samp on columns"""
type royalties_var_samp_fields {
  royalties_retries: Float
}

"""aggregate variance on columns"""
type royalties_variance_fields {
  royalties_retries: Float
}

type subscription_root {
  """
  fetch data from the table: "aggregator_event"
  """
  aggregator_event(
    """distinct select on columns"""
    distinct_on: [aggregator_event_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [aggregator_event_order_by!]

    """filter the rows returned"""
    where: aggregator_event_bool_exp
  ): [aggregator_event!]!

  """
  fetch aggregated fields from the table: "aggregator_event"
  """
  aggregator_event_aggregate(
    """distinct select on columns"""
    distinct_on: [aggregator_event_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [aggregator_event_order_by!]

    """filter the rows returned"""
    where: aggregator_event_bool_exp
  ): aggregator_event_aggregate!

  """
  fetch data from the table: "aggregator_event" using primary key columns
  """
  aggregator_event_by_pk(id: bigint!): aggregator_event

  """
  fetch data from the table in a streaming manner: "aggregator_event"
  """
  aggregator_event_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [aggregator_event_stream_cursor_input]!

    """filter the rows returned"""
    where: aggregator_event_bool_exp
  ): [aggregator_event!]!

  """
  fetch data from the table: "collection"
  """
  collection(
    """distinct select on columns"""
    distinct_on: [collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_order_by!]

    """filter the rows returned"""
    where: collection_bool_exp
  ): [collection!]!

  """
  fetch aggregated fields from the table: "collection"
  """
  collection_aggregate(
    """distinct select on columns"""
    distinct_on: [collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_order_by!]

    """filter the rows returned"""
    where: collection_bool_exp
  ): collection_aggregate!

  """fetch data from the table: "collection" using primary key columns"""
  collection_by_pk(id: String!): collection

  """
  fetch data from the table in a streaming manner: "collection"
  """
  collection_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [collection_stream_cursor_input]!

    """filter the rows returned"""
    where: collection_bool_exp
  ): [collection!]!

  """
  fetch data from the table: "collection_with_meta"
  """
  collection_with_meta(
    """distinct select on columns"""
    distinct_on: [collection_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_with_meta_order_by!]

    """filter the rows returned"""
    where: collection_with_meta_bool_exp
  ): [collection_with_meta!]!

  """
  fetch aggregated fields from the table: "collection_with_meta"
  """
  collection_with_meta_aggregate(
    """distinct select on columns"""
    distinct_on: [collection_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [collection_with_meta_order_by!]

    """filter the rows returned"""
    where: collection_with_meta_bool_exp
  ): collection_with_meta_aggregate!

  """
  fetch data from the table in a streaming manner: "collection_with_meta"
  """
  collection_with_meta_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [collection_with_meta_stream_cursor_input]!

    """filter the rows returned"""
    where: collection_with_meta_bool_exp
  ): [collection_with_meta!]!

  """
  fetch data from the table: "dipdup_contract"
  """
  dipdup_contract(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_bool_exp
  ): [dipdup_contract!]!

  """
  fetch aggregated fields from the table: "dipdup_contract"
  """
  dipdup_contract_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_bool_exp
  ): dipdup_contract_aggregate!

  """fetch data from the table: "dipdup_contract" using primary key columns"""
  dipdup_contract_by_pk(name: String!): dipdup_contract

  """
  fetch data from the table: "dipdup_contract_metadata"
  """
  dipdup_contract_metadata(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_metadata_bool_exp
  ): [dipdup_contract_metadata!]!

  """
  fetch aggregated fields from the table: "dipdup_contract_metadata"
  """
  dipdup_contract_metadata_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_contract_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_contract_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_contract_metadata_bool_exp
  ): dipdup_contract_metadata_aggregate!

  """
  fetch data from the table: "dipdup_contract_metadata" using primary key columns
  """
  dipdup_contract_metadata_by_pk(id: Int!): dipdup_contract_metadata

  """
  fetch data from the table in a streaming manner: "dipdup_contract_metadata"
  """
  dipdup_contract_metadata_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_contract_metadata_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_contract_metadata_bool_exp
  ): [dipdup_contract_metadata!]!

  """
  fetch data from the table in a streaming manner: "dipdup_contract"
  """
  dipdup_contract_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_contract_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_contract_bool_exp
  ): [dipdup_contract!]!

  """
  fetch data from the table: "dipdup_head"
  """
  dipdup_head(
    """distinct select on columns"""
    distinct_on: [dipdup_head_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_order_by!]

    """filter the rows returned"""
    where: dipdup_head_bool_exp
  ): [dipdup_head!]!

  """
  fetch aggregated fields from the table: "dipdup_head"
  """
  dipdup_head_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_head_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_order_by!]

    """filter the rows returned"""
    where: dipdup_head_bool_exp
  ): dipdup_head_aggregate!

  """fetch data from the table: "dipdup_head" using primary key columns"""
  dipdup_head_by_pk(name: String!): dipdup_head

  """
  fetch data from the table: "dipdup_head_status"
  """
  dipdup_head_status(
    """distinct select on columns"""
    distinct_on: [dipdup_head_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_status_order_by!]

    """filter the rows returned"""
    where: dipdup_head_status_bool_exp
  ): [dipdup_head_status!]!

  """
  fetch aggregated fields from the table: "dipdup_head_status"
  """
  dipdup_head_status_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_head_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_head_status_order_by!]

    """filter the rows returned"""
    where: dipdup_head_status_bool_exp
  ): dipdup_head_status_aggregate!

  """
  fetch data from the table in a streaming manner: "dipdup_head_status"
  """
  dipdup_head_status_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_head_status_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_head_status_bool_exp
  ): [dipdup_head_status!]!

  """
  fetch data from the table in a streaming manner: "dipdup_head"
  """
  dipdup_head_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_head_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_head_bool_exp
  ): [dipdup_head!]!

  """
  fetch data from the table: "dipdup_index"
  """
  dipdup_index(
    """distinct select on columns"""
    distinct_on: [dipdup_index_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_index_order_by!]

    """filter the rows returned"""
    where: dipdup_index_bool_exp
  ): [dipdup_index!]!

  """
  fetch aggregated fields from the table: "dipdup_index"
  """
  dipdup_index_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_index_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_index_order_by!]

    """filter the rows returned"""
    where: dipdup_index_bool_exp
  ): dipdup_index_aggregate!

  """fetch data from the table: "dipdup_index" using primary key columns"""
  dipdup_index_by_pk(name: String!): dipdup_index

  """
  fetch data from the table in a streaming manner: "dipdup_index"
  """
  dipdup_index_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_index_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_index_bool_exp
  ): [dipdup_index!]!

  """
  fetch data from the table: "dipdup_model_update"
  """
  dipdup_model_update(
    """distinct select on columns"""
    distinct_on: [dipdup_model_update_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_model_update_order_by!]

    """filter the rows returned"""
    where: dipdup_model_update_bool_exp
  ): [dipdup_model_update!]!

  """
  fetch aggregated fields from the table: "dipdup_model_update"
  """
  dipdup_model_update_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_model_update_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_model_update_order_by!]

    """filter the rows returned"""
    where: dipdup_model_update_bool_exp
  ): dipdup_model_update_aggregate!

  """
  fetch data from the table: "dipdup_model_update" using primary key columns
  """
  dipdup_model_update_by_pk(id: Int!): dipdup_model_update

  """
  fetch data from the table in a streaming manner: "dipdup_model_update"
  """
  dipdup_model_update_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_model_update_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_model_update_bool_exp
  ): [dipdup_model_update!]!

  """
  fetch data from the table: "dipdup_schema"
  """
  dipdup_schema(
    """distinct select on columns"""
    distinct_on: [dipdup_schema_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_schema_order_by!]

    """filter the rows returned"""
    where: dipdup_schema_bool_exp
  ): [dipdup_schema!]!

  """
  fetch aggregated fields from the table: "dipdup_schema"
  """
  dipdup_schema_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_schema_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_schema_order_by!]

    """filter the rows returned"""
    where: dipdup_schema_bool_exp
  ): dipdup_schema_aggregate!

  """fetch data from the table: "dipdup_schema" using primary key columns"""
  dipdup_schema_by_pk(name: String!): dipdup_schema

  """
  fetch data from the table in a streaming manner: "dipdup_schema"
  """
  dipdup_schema_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_schema_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_schema_bool_exp
  ): [dipdup_schema!]!

  """
  fetch data from the table: "dipdup_token_metadata"
  """
  dipdup_token_metadata(
    """distinct select on columns"""
    distinct_on: [dipdup_token_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_token_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_token_metadata_bool_exp
  ): [dipdup_token_metadata!]!

  """
  fetch aggregated fields from the table: "dipdup_token_metadata"
  """
  dipdup_token_metadata_aggregate(
    """distinct select on columns"""
    distinct_on: [dipdup_token_metadata_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [dipdup_token_metadata_order_by!]

    """filter the rows returned"""
    where: dipdup_token_metadata_bool_exp
  ): dipdup_token_metadata_aggregate!

  """
  fetch data from the table: "dipdup_token_metadata" using primary key columns
  """
  dipdup_token_metadata_by_pk(id: Int!): dipdup_token_metadata

  """
  fetch data from the table in a streaming manner: "dipdup_token_metadata"
  """
  dipdup_token_metadata_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [dipdup_token_metadata_stream_cursor_input]!

    """filter the rows returned"""
    where: dipdup_token_metadata_bool_exp
  ): [dipdup_token_metadata!]!

  """
  fetch data from the table: "indexing_status"
  """
  indexing_status(
    """distinct select on columns"""
    distinct_on: [indexing_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [indexing_status_order_by!]

    """filter the rows returned"""
    where: indexing_status_bool_exp
  ): [indexing_status!]!

  """
  fetch aggregated fields from the table: "indexing_status"
  """
  indexing_status_aggregate(
    """distinct select on columns"""
    distinct_on: [indexing_status_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [indexing_status_order_by!]

    """filter the rows returned"""
    where: indexing_status_bool_exp
  ): indexing_status_aggregate!

  """fetch data from the table: "indexing_status" using primary key columns"""
  indexing_status_by_pk(
    """
    COLLECTION: COLLECTION\nCOLLECTION_METADATA: COLLECTION_METADATA\nNFT: NFT\nNFT_METADATA: NFT_METADATA\nLEGACY_ORDERS: LEGACY_ORDERS\nV1_CLEANING: V1_CLEANING\nV1_FILL_FIX: V1_FILL_FIX
    """
    index: String!
  ): indexing_status

  """
  fetch data from the table in a streaming manner: "indexing_status"
  """
  indexing_status_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [indexing_status_stream_cursor_input]!

    """filter the rows returned"""
    where: indexing_status_bool_exp
  ): [indexing_status!]!

  """
  fetch data from the table: "legacy_orders"
  """
  legacy_orders(
    """distinct select on columns"""
    distinct_on: [legacy_orders_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [legacy_orders_order_by!]

    """filter the rows returned"""
    where: legacy_orders_bool_exp
  ): [legacy_orders!]!

  """
  fetch aggregated fields from the table: "legacy_orders"
  """
  legacy_orders_aggregate(
    """distinct select on columns"""
    distinct_on: [legacy_orders_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [legacy_orders_order_by!]

    """filter the rows returned"""
    where: legacy_orders_bool_exp
  ): legacy_orders_aggregate!

  """fetch data from the table: "legacy_orders" using primary key columns"""
  legacy_orders_by_pk(hash: String!): legacy_orders

  """
  fetch data from the table in a streaming manner: "legacy_orders"
  """
  legacy_orders_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [legacy_orders_stream_cursor_input]!

    """filter the rows returned"""
    where: legacy_orders_bool_exp
  ): [legacy_orders!]!

  """
  fetch data from the table: "marketplace_activity"
  """
  marketplace_activity(
    """distinct select on columns"""
    distinct_on: [marketplace_activity_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_activity_order_by!]

    """filter the rows returned"""
    where: marketplace_activity_bool_exp
  ): [marketplace_activity!]!

  """
  fetch aggregated fields from the table: "marketplace_activity"
  """
  marketplace_activity_aggregate(
    """distinct select on columns"""
    distinct_on: [marketplace_activity_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_activity_order_by!]

    """filter the rows returned"""
    where: marketplace_activity_bool_exp
  ): marketplace_activity_aggregate!

  """
  fetch data from the table: "marketplace_activity" using primary key columns
  """
  marketplace_activity_by_pk(id: uuid!): marketplace_activity

  """
  fetch data from the table in a streaming manner: "marketplace_activity"
  """
  marketplace_activity_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [marketplace_activity_stream_cursor_input]!

    """filter the rows returned"""
    where: marketplace_activity_bool_exp
  ): [marketplace_activity!]!

  """
  fetch data from the table: "marketplace_order"
  """
  marketplace_order(
    """distinct select on columns"""
    distinct_on: [marketplace_order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_order_order_by!]

    """filter the rows returned"""
    where: marketplace_order_bool_exp
  ): [marketplace_order!]!

  """
  fetch aggregated fields from the table: "marketplace_order"
  """
  marketplace_order_aggregate(
    """distinct select on columns"""
    distinct_on: [marketplace_order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [marketplace_order_order_by!]

    """filter the rows returned"""
    where: marketplace_order_bool_exp
  ): marketplace_order_aggregate!

  """
  fetch data from the table: "marketplace_order" using primary key columns
  """
  marketplace_order_by_pk(id: uuid!): marketplace_order

  """
  fetch data from the table in a streaming manner: "marketplace_order"
  """
  marketplace_order_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [marketplace_order_stream_cursor_input]!

    """filter the rows returned"""
    where: marketplace_order_bool_exp
  ): [marketplace_order!]!

  """
  fetch data from the table: "metadata_collection"
  """
  metadata_collection(
    """distinct select on columns"""
    distinct_on: [metadata_collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_collection_order_by!]

    """filter the rows returned"""
    where: metadata_collection_bool_exp
  ): [metadata_collection!]!

  """
  fetch aggregated fields from the table: "metadata_collection"
  """
  metadata_collection_aggregate(
    """distinct select on columns"""
    distinct_on: [metadata_collection_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_collection_order_by!]

    """filter the rows returned"""
    where: metadata_collection_bool_exp
  ): metadata_collection_aggregate!

  """
  fetch data from the table: "metadata_collection" using primary key columns
  """
  metadata_collection_by_pk(id: String!): metadata_collection

  """
  fetch data from the table in a streaming manner: "metadata_collection"
  """
  metadata_collection_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [metadata_collection_stream_cursor_input]!

    """filter the rows returned"""
    where: metadata_collection_bool_exp
  ): [metadata_collection!]!

  """
  fetch data from the table: "metadata_token"
  """
  metadata_token(
    """distinct select on columns"""
    distinct_on: [metadata_token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_token_order_by!]

    """filter the rows returned"""
    where: metadata_token_bool_exp
  ): [metadata_token!]!

  """
  fetch aggregated fields from the table: "metadata_token"
  """
  metadata_token_aggregate(
    """distinct select on columns"""
    distinct_on: [metadata_token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [metadata_token_order_by!]

    """filter the rows returned"""
    where: metadata_token_bool_exp
  ): metadata_token_aggregate!

  """fetch data from the table: "metadata_token" using primary key columns"""
  metadata_token_by_pk(id: uuid!): metadata_token

  """
  fetch data from the table in a streaming manner: "metadata_token"
  """
  metadata_token_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [metadata_token_stream_cursor_input]!

    """filter the rows returned"""
    where: metadata_token_bool_exp
  ): [metadata_token!]!

  """
  fetch data from the table: "ownership"
  """
  ownership(
    """distinct select on columns"""
    distinct_on: [ownership_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [ownership_order_by!]

    """filter the rows returned"""
    where: ownership_bool_exp
  ): [ownership!]!

  """
  fetch aggregated fields from the table: "ownership"
  """
  ownership_aggregate(
    """distinct select on columns"""
    distinct_on: [ownership_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [ownership_order_by!]

    """filter the rows returned"""
    where: ownership_bool_exp
  ): ownership_aggregate!

  """fetch data from the table: "ownership" using primary key columns"""
  ownership_by_pk(id: uuid!): ownership

  """
  fetch data from the table in a streaming manner: "ownership"
  """
  ownership_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [ownership_stream_cursor_input]!

    """filter the rows returned"""
    where: ownership_bool_exp
  ): [ownership!]!

  """
  fetch data from the table: "royalties"
  """
  royalties(
    """distinct select on columns"""
    distinct_on: [royalties_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [royalties_order_by!]

    """filter the rows returned"""
    where: royalties_bool_exp
  ): [royalties!]!

  """
  fetch aggregated fields from the table: "royalties"
  """
  royalties_aggregate(
    """distinct select on columns"""
    distinct_on: [royalties_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [royalties_order_by!]

    """filter the rows returned"""
    where: royalties_bool_exp
  ): royalties_aggregate!

  """fetch data from the table: "royalties" using primary key columns"""
  royalties_by_pk(id: uuid!): royalties

  """
  fetch data from the table in a streaming manner: "royalties"
  """
  royalties_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [royalties_stream_cursor_input]!

    """filter the rows returned"""
    where: royalties_bool_exp
  ): [royalties!]!

  """
  fetch data from the table: "tasks"
  """
  tasks(
    """distinct select on columns"""
    distinct_on: [tasks_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tasks_order_by!]

    """filter the rows returned"""
    where: tasks_bool_exp
  ): [tasks!]!

  """
  fetch aggregated fields from the table: "tasks"
  """
  tasks_aggregate(
    """distinct select on columns"""
    distinct_on: [tasks_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tasks_order_by!]

    """filter the rows returned"""
    where: tasks_bool_exp
  ): tasks_aggregate!

  """fetch data from the table: "tasks" using primary key columns"""
  tasks_by_pk(id: bigint!): tasks

  """
  fetch data from the table in a streaming manner: "tasks"
  """
  tasks_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [tasks_stream_cursor_input]!

    """filter the rows returned"""
    where: tasks_bool_exp
  ): [tasks!]!

  """
  fetch data from the table: "tezos_domains_domain"
  """
  tezos_domains_domain(
    """distinct select on columns"""
    distinct_on: [tezos_domains_domain_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_domain_order_by!]

    """filter the rows returned"""
    where: tezos_domains_domain_bool_exp
  ): [tezos_domains_domain!]!

  """
  fetch aggregated fields from the table: "tezos_domains_domain"
  """
  tezos_domains_domain_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_domain_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_domain_order_by!]

    """filter the rows returned"""
    where: tezos_domains_domain_bool_exp
  ): tezos_domains_domain_aggregate!

  """
  fetch data from the table: "tezos_domains_domain" using primary key columns
  """
  tezos_domains_domain_by_pk(id: String!): tezos_domains_domain

  """
  fetch data from the table in a streaming manner: "tezos_domains_domain"
  """
  tezos_domains_domain_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [tezos_domains_domain_stream_cursor_input]!

    """filter the rows returned"""
    where: tezos_domains_domain_bool_exp
  ): [tezos_domains_domain!]!

  """
  fetch data from the table: "tezos_domains_record"
  """
  tezos_domains_record(
    """distinct select on columns"""
    distinct_on: [tezos_domains_record_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_record_order_by!]

    """filter the rows returned"""
    where: tezos_domains_record_bool_exp
  ): [tezos_domains_record!]!

  """
  fetch aggregated fields from the table: "tezos_domains_record"
  """
  tezos_domains_record_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_record_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_record_order_by!]

    """filter the rows returned"""
    where: tezos_domains_record_bool_exp
  ): tezos_domains_record_aggregate!

  """
  fetch data from the table: "tezos_domains_record" using primary key columns
  """
  tezos_domains_record_by_pk(id: String!): tezos_domains_record

  """
  fetch data from the table in a streaming manner: "tezos_domains_record"
  """
  tezos_domains_record_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [tezos_domains_record_stream_cursor_input]!

    """filter the rows returned"""
    where: tezos_domains_record_bool_exp
  ): [tezos_domains_record!]!

  """
  fetch data from the table: "tezos_domains_tld"
  """
  tezos_domains_tld(
    """distinct select on columns"""
    distinct_on: [tezos_domains_tld_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_tld_order_by!]

    """filter the rows returned"""
    where: tezos_domains_tld_bool_exp
  ): [tezos_domains_tld!]!

  """
  fetch aggregated fields from the table: "tezos_domains_tld"
  """
  tezos_domains_tld_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_tld_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_tld_order_by!]

    """filter the rows returned"""
    where: tezos_domains_tld_bool_exp
  ): tezos_domains_tld_aggregate!

  """
  fetch data from the table: "tezos_domains_tld" using primary key columns
  """
  tezos_domains_tld_by_pk(id: String!): tezos_domains_tld

  """
  fetch data from the table in a streaming manner: "tezos_domains_tld"
  """
  tezos_domains_tld_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [tezos_domains_tld_stream_cursor_input]!

    """filter the rows returned"""
    where: tezos_domains_tld_bool_exp
  ): [tezos_domains_tld!]!

  """
  fetch data from the table: "token"
  """
  token(
    """distinct select on columns"""
    distinct_on: [token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_order_by!]

    """filter the rows returned"""
    where: token_bool_exp
  ): [token!]!

  """
  fetch aggregated fields from the table: "token"
  """
  token_aggregate(
    """distinct select on columns"""
    distinct_on: [token_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_order_by!]

    """filter the rows returned"""
    where: token_bool_exp
  ): token_aggregate!

  """
  fetch data from the table: "token_by_owner"
  """
  token_by_owner(
    """distinct select on columns"""
    distinct_on: [token_by_owner_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_by_owner_order_by!]

    """filter the rows returned"""
    where: token_by_owner_bool_exp
  ): [token_by_owner!]!

  """
  fetch aggregated fields from the table: "token_by_owner"
  """
  token_by_owner_aggregate(
    """distinct select on columns"""
    distinct_on: [token_by_owner_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_by_owner_order_by!]

    """filter the rows returned"""
    where: token_by_owner_bool_exp
  ): token_by_owner_aggregate!

  """
  fetch data from the table in a streaming manner: "token_by_owner"
  """
  token_by_owner_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [token_by_owner_stream_cursor_input]!

    """filter the rows returned"""
    where: token_by_owner_bool_exp
  ): [token_by_owner!]!

  """fetch data from the table: "token" using primary key columns"""
  token_by_pk(id: uuid!): token

  """
  fetch data from the table in a streaming manner: "token"
  """
  token_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [token_stream_cursor_input]!

    """filter the rows returned"""
    where: token_bool_exp
  ): [token!]!

  """
  fetch data from the table: "token_transfer"
  """
  token_transfer(
    """distinct select on columns"""
    distinct_on: [token_transfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_transfer_order_by!]

    """filter the rows returned"""
    where: token_transfer_bool_exp
  ): [token_transfer!]!

  """
  fetch aggregated fields from the table: "token_transfer"
  """
  token_transfer_aggregate(
    """distinct select on columns"""
    distinct_on: [token_transfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_transfer_order_by!]

    """filter the rows returned"""
    where: token_transfer_bool_exp
  ): token_transfer_aggregate!

  """fetch data from the table: "token_transfer" using primary key columns"""
  token_transfer_by_pk(id: bigint!): token_transfer

  """
  fetch data from the table in a streaming manner: "token_transfer"
  """
  token_transfer_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [token_transfer_stream_cursor_input]!

    """filter the rows returned"""
    where: token_transfer_bool_exp
  ): [token_transfer!]!

  """
  fetch data from the table: "token_with_meta"
  """
  token_with_meta(
    """distinct select on columns"""
    distinct_on: [token_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_with_meta_order_by!]

    """filter the rows returned"""
    where: token_with_meta_bool_exp
  ): [token_with_meta!]!

  """
  fetch aggregated fields from the table: "token_with_meta"
  """
  token_with_meta_aggregate(
    """distinct select on columns"""
    distinct_on: [token_with_meta_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [token_with_meta_order_by!]

    """filter the rows returned"""
    where: token_with_meta_bool_exp
  ): token_with_meta_aggregate!

  """
  fetch data from the table in a streaming manner: "token_with_meta"
  """
  token_with_meta_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [token_with_meta_stream_cursor_input]!

    """filter the rows returned"""
    where: token_with_meta_bool_exp
  ): [token_with_meta!]!

  """
  fetch data from the table: "tzprofiles"
  """
  tzprofiles(
    """distinct select on columns"""
    distinct_on: [tzprofiles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tzprofiles_order_by!]

    """filter the rows returned"""
    where: tzprofiles_bool_exp
  ): [tzprofiles!]!

  """
  fetch aggregated fields from the table: "tzprofiles"
  """
  tzprofiles_aggregate(
    """distinct select on columns"""
    distinct_on: [tzprofiles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tzprofiles_order_by!]

    """filter the rows returned"""
    where: tzprofiles_bool_exp
  ): tzprofiles_aggregate!

  """fetch data from the table: "tzprofiles" using primary key columns"""
  tzprofiles_by_pk(account: String!): tzprofiles

  """
  fetch data from the table in a streaming manner: "tzprofiles"
  """
  tzprofiles_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [tzprofiles_stream_cursor_input]!

    """filter the rows returned"""
    where: tzprofiles_bool_exp
  ): [tzprofiles!]!
}

"""
columns and relationships of "tasks"
"""
type tasks {
  created: timestamptz!
  error: String
  id: bigint!
  name: String!
  param: String
  sample: String
  status: String!
  updated: timestamptz!
  version: Int!
}

"""
aggregated selection of "tasks"
"""
type tasks_aggregate {
  aggregate: tasks_aggregate_fields
  nodes: [tasks!]!
}

"""
aggregate fields of "tasks"
"""
type tasks_aggregate_fields {
  avg: tasks_avg_fields
  count(columns: [tasks_select_column!], distinct: Boolean): Int!
  max: tasks_max_fields
  min: tasks_min_fields
  stddev: tasks_stddev_fields
  stddev_pop: tasks_stddev_pop_fields
  stddev_samp: tasks_stddev_samp_fields
  sum: tasks_sum_fields
  var_pop: tasks_var_pop_fields
  var_samp: tasks_var_samp_fields
  variance: tasks_variance_fields
}

"""aggregate avg on columns"""
type tasks_avg_fields {
  id: Float
  version: Float
}

"""
Boolean expression to filter rows from the table "tasks". All fields are combined with a logical 'AND'.
"""
input tasks_bool_exp {
  _and: [tasks_bool_exp!]
  _not: tasks_bool_exp
  _or: [tasks_bool_exp!]
  created: timestamptz_comparison_exp
  error: String_comparison_exp
  id: bigint_comparison_exp
  name: String_comparison_exp
  param: String_comparison_exp
  sample: String_comparison_exp
  status: String_comparison_exp
  updated: timestamptz_comparison_exp
  version: Int_comparison_exp
}

"""
unique or primary key constraints on table "tasks"
"""
enum tasks_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  tasks_pkey
}

"""
input type for incrementing numeric columns in table "tasks"
"""
input tasks_inc_input {
  id: bigint
  version: Int
}

"""
input type for inserting data into table "tasks"
"""
input tasks_insert_input {
  created: timestamptz
  error: String
  id: bigint
  name: String
  param: String
  sample: String
  status: String
  updated: timestamptz
  version: Int
}

"""aggregate max on columns"""
type tasks_max_fields {
  created: timestamptz
  error: String
  id: bigint
  name: String
  param: String
  sample: String
  status: String
  updated: timestamptz
  version: Int
}

"""aggregate min on columns"""
type tasks_min_fields {
  created: timestamptz
  error: String
  id: bigint
  name: String
  param: String
  sample: String
  status: String
  updated: timestamptz
  version: Int
}

"""
response of any mutation on the table "tasks"
"""
type tasks_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [tasks!]!
}

"""
on_conflict condition type for table "tasks"
"""
input tasks_on_conflict {
  constraint: tasks_constraint!
  update_columns: [tasks_update_column!]! = []
  where: tasks_bool_exp
}

"""Ordering options when selecting data from "tasks"."""
input tasks_order_by {
  created: order_by
  error: order_by
  id: order_by
  name: order_by
  param: order_by
  sample: order_by
  status: order_by
  updated: order_by
  version: order_by
}

"""primary key columns input for table: tasks"""
input tasks_pk_columns_input {
  id: bigint!
}

"""
select columns of table "tasks"
"""
enum tasks_select_column {
  """column name"""
  created

  """column name"""
  error

  """column name"""
  id

  """column name"""
  name

  """column name"""
  param

  """column name"""
  sample

  """column name"""
  status

  """column name"""
  updated

  """column name"""
  version
}

"""
input type for updating data in table "tasks"
"""
input tasks_set_input {
  created: timestamptz
  error: String
  id: bigint
  name: String
  param: String
  sample: String
  status: String
  updated: timestamptz
  version: Int
}

"""aggregate stddev on columns"""
type tasks_stddev_fields {
  id: Float
  version: Float
}

"""aggregate stddev_pop on columns"""
type tasks_stddev_pop_fields {
  id: Float
  version: Float
}

"""aggregate stddev_samp on columns"""
type tasks_stddev_samp_fields {
  id: Float
  version: Float
}

"""
Streaming cursor of the table "tasks"
"""
input tasks_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: tasks_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input tasks_stream_cursor_value_input {
  created: timestamptz
  error: String
  id: bigint
  name: String
  param: String
  sample: String
  status: String
  updated: timestamptz
  version: Int
}

"""aggregate sum on columns"""
type tasks_sum_fields {
  id: bigint
  version: Int
}

"""
update columns of table "tasks"
"""
enum tasks_update_column {
  """column name"""
  created

  """column name"""
  error

  """column name"""
  id

  """column name"""
  name

  """column name"""
  param

  """column name"""
  sample

  """column name"""
  status

  """column name"""
  updated

  """column name"""
  version
}

input tasks_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: tasks_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: tasks_set_input

  """filter the rows which have to be updated"""
  where: tasks_bool_exp!
}

"""aggregate var_pop on columns"""
type tasks_var_pop_fields {
  id: Float
  version: Float
}

"""aggregate var_samp on columns"""
type tasks_var_samp_fields {
  id: Float
  version: Float
}

"""aggregate variance on columns"""
type tasks_variance_fields {
  id: Float
  version: Float
}

"""
columns and relationships of "tezos_domains_domain"
"""
type tezos_domains_domain {
  id: String!
  owner: String!

  """An array relationship"""
  records(
    """distinct select on columns"""
    distinct_on: [tezos_domains_record_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_record_order_by!]

    """filter the rows returned"""
    where: tezos_domains_record_bool_exp
  ): [tezos_domains_record!]!

  """An aggregate relationship"""
  records_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_record_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_record_order_by!]

    """filter the rows returned"""
    where: tezos_domains_record_bool_exp
  ): tezos_domains_record_aggregate!

  """An object relationship"""
  tld: tezos_domains_tld!
  tld_id: String!
  token_id: bigint
}

"""
aggregated selection of "tezos_domains_domain"
"""
type tezos_domains_domain_aggregate {
  aggregate: tezos_domains_domain_aggregate_fields
  nodes: [tezos_domains_domain!]!
}

input tezos_domains_domain_aggregate_bool_exp {
  count: tezos_domains_domain_aggregate_bool_exp_count
}

input tezos_domains_domain_aggregate_bool_exp_count {
  arguments: [tezos_domains_domain_select_column!]
  distinct: Boolean
  filter: tezos_domains_domain_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "tezos_domains_domain"
"""
type tezos_domains_domain_aggregate_fields {
  avg: tezos_domains_domain_avg_fields
  count(columns: [tezos_domains_domain_select_column!], distinct: Boolean): Int!
  max: tezos_domains_domain_max_fields
  min: tezos_domains_domain_min_fields
  stddev: tezos_domains_domain_stddev_fields
  stddev_pop: tezos_domains_domain_stddev_pop_fields
  stddev_samp: tezos_domains_domain_stddev_samp_fields
  sum: tezos_domains_domain_sum_fields
  var_pop: tezos_domains_domain_var_pop_fields
  var_samp: tezos_domains_domain_var_samp_fields
  variance: tezos_domains_domain_variance_fields
}

"""
order by aggregate values of table "tezos_domains_domain"
"""
input tezos_domains_domain_aggregate_order_by {
  avg: tezos_domains_domain_avg_order_by
  count: order_by
  max: tezos_domains_domain_max_order_by
  min: tezos_domains_domain_min_order_by
  stddev: tezos_domains_domain_stddev_order_by
  stddev_pop: tezos_domains_domain_stddev_pop_order_by
  stddev_samp: tezos_domains_domain_stddev_samp_order_by
  sum: tezos_domains_domain_sum_order_by
  var_pop: tezos_domains_domain_var_pop_order_by
  var_samp: tezos_domains_domain_var_samp_order_by
  variance: tezos_domains_domain_variance_order_by
}

"""
input type for inserting array relation for remote table "tezos_domains_domain"
"""
input tezos_domains_domain_arr_rel_insert_input {
  data: [tezos_domains_domain_insert_input!]!

  """upsert condition"""
  on_conflict: tezos_domains_domain_on_conflict
}

"""aggregate avg on columns"""
type tezos_domains_domain_avg_fields {
  token_id: Float
}

"""
order by avg() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_avg_order_by {
  token_id: order_by
}

"""
Boolean expression to filter rows from the table "tezos_domains_domain". All fields are combined with a logical 'AND'.
"""
input tezos_domains_domain_bool_exp {
  _and: [tezos_domains_domain_bool_exp!]
  _not: tezos_domains_domain_bool_exp
  _or: [tezos_domains_domain_bool_exp!]
  id: String_comparison_exp
  owner: String_comparison_exp
  records: tezos_domains_record_bool_exp
  records_aggregate: tezos_domains_record_aggregate_bool_exp
  tld: tezos_domains_tld_bool_exp
  tld_id: String_comparison_exp
  token_id: bigint_comparison_exp
}

"""
unique or primary key constraints on table "tezos_domains_domain"
"""
enum tezos_domains_domain_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  tezos_domains_domain_pkey
}

"""
input type for incrementing numeric columns in table "tezos_domains_domain"
"""
input tezos_domains_domain_inc_input {
  token_id: bigint
}

"""
input type for inserting data into table "tezos_domains_domain"
"""
input tezos_domains_domain_insert_input {
  id: String
  owner: String
  records: tezos_domains_record_arr_rel_insert_input
  tld: tezos_domains_tld_obj_rel_insert_input
  tld_id: String
  token_id: bigint
}

"""aggregate max on columns"""
type tezos_domains_domain_max_fields {
  id: String
  owner: String
  tld_id: String
  token_id: bigint
}

"""
order by max() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_max_order_by {
  id: order_by
  owner: order_by
  tld_id: order_by
  token_id: order_by
}

"""aggregate min on columns"""
type tezos_domains_domain_min_fields {
  id: String
  owner: String
  tld_id: String
  token_id: bigint
}

"""
order by min() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_min_order_by {
  id: order_by
  owner: order_by
  tld_id: order_by
  token_id: order_by
}

"""
response of any mutation on the table "tezos_domains_domain"
"""
type tezos_domains_domain_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [tezos_domains_domain!]!
}

"""
input type for inserting object relation for remote table "tezos_domains_domain"
"""
input tezos_domains_domain_obj_rel_insert_input {
  data: tezos_domains_domain_insert_input!

  """upsert condition"""
  on_conflict: tezos_domains_domain_on_conflict
}

"""
on_conflict condition type for table "tezos_domains_domain"
"""
input tezos_domains_domain_on_conflict {
  constraint: tezos_domains_domain_constraint!
  update_columns: [tezos_domains_domain_update_column!]! = []
  where: tezos_domains_domain_bool_exp
}

"""Ordering options when selecting data from "tezos_domains_domain"."""
input tezos_domains_domain_order_by {
  id: order_by
  owner: order_by
  records_aggregate: tezos_domains_record_aggregate_order_by
  tld: tezos_domains_tld_order_by
  tld_id: order_by
  token_id: order_by
}

"""primary key columns input for table: tezos_domains_domain"""
input tezos_domains_domain_pk_columns_input {
  id: String!
}

"""
select columns of table "tezos_domains_domain"
"""
enum tezos_domains_domain_select_column {
  """column name"""
  id

  """column name"""
  owner

  """column name"""
  tld_id

  """column name"""
  token_id
}

"""
input type for updating data in table "tezos_domains_domain"
"""
input tezos_domains_domain_set_input {
  id: String
  owner: String
  tld_id: String
  token_id: bigint
}

"""aggregate stddev on columns"""
type tezos_domains_domain_stddev_fields {
  token_id: Float
}

"""
order by stddev() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_stddev_order_by {
  token_id: order_by
}

"""aggregate stddev_pop on columns"""
type tezos_domains_domain_stddev_pop_fields {
  token_id: Float
}

"""
order by stddev_pop() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_stddev_pop_order_by {
  token_id: order_by
}

"""aggregate stddev_samp on columns"""
type tezos_domains_domain_stddev_samp_fields {
  token_id: Float
}

"""
order by stddev_samp() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_stddev_samp_order_by {
  token_id: order_by
}

"""
Streaming cursor of the table "tezos_domains_domain"
"""
input tezos_domains_domain_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: tezos_domains_domain_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input tezos_domains_domain_stream_cursor_value_input {
  id: String
  owner: String
  tld_id: String
  token_id: bigint
}

"""aggregate sum on columns"""
type tezos_domains_domain_sum_fields {
  token_id: bigint
}

"""
order by sum() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_sum_order_by {
  token_id: order_by
}

"""
update columns of table "tezos_domains_domain"
"""
enum tezos_domains_domain_update_column {
  """column name"""
  id

  """column name"""
  owner

  """column name"""
  tld_id

  """column name"""
  token_id
}

input tezos_domains_domain_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: tezos_domains_domain_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: tezos_domains_domain_set_input

  """filter the rows which have to be updated"""
  where: tezos_domains_domain_bool_exp!
}

"""aggregate var_pop on columns"""
type tezos_domains_domain_var_pop_fields {
  token_id: Float
}

"""
order by var_pop() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_var_pop_order_by {
  token_id: order_by
}

"""aggregate var_samp on columns"""
type tezos_domains_domain_var_samp_fields {
  token_id: Float
}

"""
order by var_samp() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_var_samp_order_by {
  token_id: order_by
}

"""aggregate variance on columns"""
type tezos_domains_domain_variance_fields {
  token_id: Float
}

"""
order by variance() on columns of table "tezos_domains_domain"
"""
input tezos_domains_domain_variance_order_by {
  token_id: order_by
}

"""
columns and relationships of "tezos_domains_record"
"""
type tezos_domains_record {
  address: String

  """An object relationship"""
  domain: tezos_domains_domain!
  domain_id: String!
  id: String!
}

"""
aggregated selection of "tezos_domains_record"
"""
type tezos_domains_record_aggregate {
  aggregate: tezos_domains_record_aggregate_fields
  nodes: [tezos_domains_record!]!
}

input tezos_domains_record_aggregate_bool_exp {
  count: tezos_domains_record_aggregate_bool_exp_count
}

input tezos_domains_record_aggregate_bool_exp_count {
  arguments: [tezos_domains_record_select_column!]
  distinct: Boolean
  filter: tezos_domains_record_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "tezos_domains_record"
"""
type tezos_domains_record_aggregate_fields {
  count(columns: [tezos_domains_record_select_column!], distinct: Boolean): Int!
  max: tezos_domains_record_max_fields
  min: tezos_domains_record_min_fields
}

"""
order by aggregate values of table "tezos_domains_record"
"""
input tezos_domains_record_aggregate_order_by {
  count: order_by
  max: tezos_domains_record_max_order_by
  min: tezos_domains_record_min_order_by
}

"""
input type for inserting array relation for remote table "tezos_domains_record"
"""
input tezos_domains_record_arr_rel_insert_input {
  data: [tezos_domains_record_insert_input!]!

  """upsert condition"""
  on_conflict: tezos_domains_record_on_conflict
}

"""
Boolean expression to filter rows from the table "tezos_domains_record". All fields are combined with a logical 'AND'.
"""
input tezos_domains_record_bool_exp {
  _and: [tezos_domains_record_bool_exp!]
  _not: tezos_domains_record_bool_exp
  _or: [tezos_domains_record_bool_exp!]
  address: String_comparison_exp
  domain: tezos_domains_domain_bool_exp
  domain_id: String_comparison_exp
  id: String_comparison_exp
}

"""
unique or primary key constraints on table "tezos_domains_record"
"""
enum tezos_domains_record_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  tezos_domains_record_pkey
}

"""
input type for inserting data into table "tezos_domains_record"
"""
input tezos_domains_record_insert_input {
  address: String
  domain: tezos_domains_domain_obj_rel_insert_input
  domain_id: String
  id: String
}

"""aggregate max on columns"""
type tezos_domains_record_max_fields {
  address: String
  domain_id: String
  id: String
}

"""
order by max() on columns of table "tezos_domains_record"
"""
input tezos_domains_record_max_order_by {
  address: order_by
  domain_id: order_by
  id: order_by
}

"""aggregate min on columns"""
type tezos_domains_record_min_fields {
  address: String
  domain_id: String
  id: String
}

"""
order by min() on columns of table "tezos_domains_record"
"""
input tezos_domains_record_min_order_by {
  address: order_by
  domain_id: order_by
  id: order_by
}

"""
response of any mutation on the table "tezos_domains_record"
"""
type tezos_domains_record_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [tezos_domains_record!]!
}

"""
on_conflict condition type for table "tezos_domains_record"
"""
input tezos_domains_record_on_conflict {
  constraint: tezos_domains_record_constraint!
  update_columns: [tezos_domains_record_update_column!]! = []
  where: tezos_domains_record_bool_exp
}

"""Ordering options when selecting data from "tezos_domains_record"."""
input tezos_domains_record_order_by {
  address: order_by
  domain: tezos_domains_domain_order_by
  domain_id: order_by
  id: order_by
}

"""primary key columns input for table: tezos_domains_record"""
input tezos_domains_record_pk_columns_input {
  id: String!
}

"""
select columns of table "tezos_domains_record"
"""
enum tezos_domains_record_select_column {
  """column name"""
  address

  """column name"""
  domain_id

  """column name"""
  id
}

"""
input type for updating data in table "tezos_domains_record"
"""
input tezos_domains_record_set_input {
  address: String
  domain_id: String
  id: String
}

"""
Streaming cursor of the table "tezos_domains_record"
"""
input tezos_domains_record_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: tezos_domains_record_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input tezos_domains_record_stream_cursor_value_input {
  address: String
  domain_id: String
  id: String
}

"""
update columns of table "tezos_domains_record"
"""
enum tezos_domains_record_update_column {
  """column name"""
  address

  """column name"""
  domain_id

  """column name"""
  id
}

input tezos_domains_record_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: tezos_domains_record_set_input

  """filter the rows which have to be updated"""
  where: tezos_domains_record_bool_exp!
}

"""
columns and relationships of "tezos_domains_tld"
"""
type tezos_domains_tld {
  """An array relationship"""
  domains(
    """distinct select on columns"""
    distinct_on: [tezos_domains_domain_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_domain_order_by!]

    """filter the rows returned"""
    where: tezos_domains_domain_bool_exp
  ): [tezos_domains_domain!]!

  """An aggregate relationship"""
  domains_aggregate(
    """distinct select on columns"""
    distinct_on: [tezos_domains_domain_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [tezos_domains_domain_order_by!]

    """filter the rows returned"""
    where: tezos_domains_domain_bool_exp
  ): tezos_domains_domain_aggregate!
  id: String!
  owner: String!
}

"""
aggregated selection of "tezos_domains_tld"
"""
type tezos_domains_tld_aggregate {
  aggregate: tezos_domains_tld_aggregate_fields
  nodes: [tezos_domains_tld!]!
}

"""
aggregate fields of "tezos_domains_tld"
"""
type tezos_domains_tld_aggregate_fields {
  count(columns: [tezos_domains_tld_select_column!], distinct: Boolean): Int!
  max: tezos_domains_tld_max_fields
  min: tezos_domains_tld_min_fields
}

"""
Boolean expression to filter rows from the table "tezos_domains_tld". All fields are combined with a logical 'AND'.
"""
input tezos_domains_tld_bool_exp {
  _and: [tezos_domains_tld_bool_exp!]
  _not: tezos_domains_tld_bool_exp
  _or: [tezos_domains_tld_bool_exp!]
  domains: tezos_domains_domain_bool_exp
  domains_aggregate: tezos_domains_domain_aggregate_bool_exp
  id: String_comparison_exp
  owner: String_comparison_exp
}

"""
unique or primary key constraints on table "tezos_domains_tld"
"""
enum tezos_domains_tld_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  tezos_domains_tld_pkey
}

"""
input type for inserting data into table "tezos_domains_tld"
"""
input tezos_domains_tld_insert_input {
  domains: tezos_domains_domain_arr_rel_insert_input
  id: String
  owner: String
}

"""aggregate max on columns"""
type tezos_domains_tld_max_fields {
  id: String
  owner: String
}

"""aggregate min on columns"""
type tezos_domains_tld_min_fields {
  id: String
  owner: String
}

"""
response of any mutation on the table "tezos_domains_tld"
"""
type tezos_domains_tld_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [tezos_domains_tld!]!
}

"""
input type for inserting object relation for remote table "tezos_domains_tld"
"""
input tezos_domains_tld_obj_rel_insert_input {
  data: tezos_domains_tld_insert_input!

  """upsert condition"""
  on_conflict: tezos_domains_tld_on_conflict
}

"""
on_conflict condition type for table "tezos_domains_tld"
"""
input tezos_domains_tld_on_conflict {
  constraint: tezos_domains_tld_constraint!
  update_columns: [tezos_domains_tld_update_column!]! = []
  where: tezos_domains_tld_bool_exp
}

"""Ordering options when selecting data from "tezos_domains_tld"."""
input tezos_domains_tld_order_by {
  domains_aggregate: tezos_domains_domain_aggregate_order_by
  id: order_by
  owner: order_by
}

"""primary key columns input for table: tezos_domains_tld"""
input tezos_domains_tld_pk_columns_input {
  id: String!
}

"""
select columns of table "tezos_domains_tld"
"""
enum tezos_domains_tld_select_column {
  """column name"""
  id

  """column name"""
  owner
}

"""
input type for updating data in table "tezos_domains_tld"
"""
input tezos_domains_tld_set_input {
  id: String
  owner: String
}

"""
Streaming cursor of the table "tezos_domains_tld"
"""
input tezos_domains_tld_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: tezos_domains_tld_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input tezos_domains_tld_stream_cursor_value_input {
  id: String
  owner: String
}

"""
update columns of table "tezos_domains_tld"
"""
enum tezos_domains_tld_update_column {
  """column name"""
  id

  """column name"""
  owner
}

input tezos_domains_tld_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: tezos_domains_tld_set_input

  """filter the rows which have to be updated"""
  where: tezos_domains_tld_bool_exp!
}

scalar timestamptz

"""
Boolean expression to compare columns of type "timestamptz". All fields are combined with logical 'AND'.
"""
input timestamptz_comparison_exp {
  _eq: timestamptz
  _gt: timestamptz
  _gte: timestamptz
  _in: [timestamptz!]
  _is_null: Boolean
  _lt: timestamptz
  _lte: timestamptz
  _neq: timestamptz
  _nin: [timestamptz!]
}

"""
columns and relationships of "token"
"""
type token {
  contract: String!
  creator: String
  db_updated_at: timestamptz!
  deleted: Boolean!
  id: uuid!
  minted: numeric!
  minted_at: timestamptz!
  supply: numeric!
  token_id: String!
  tzkt_id: bigint!
  updated: timestamptz!
}

"""
aggregated selection of "token"
"""
type token_aggregate {
  aggregate: token_aggregate_fields
  nodes: [token!]!
}

"""
aggregate fields of "token"
"""
type token_aggregate_fields {
  avg: token_avg_fields
  count(columns: [token_select_column!], distinct: Boolean): Int!
  max: token_max_fields
  min: token_min_fields
  stddev: token_stddev_fields
  stddev_pop: token_stddev_pop_fields
  stddev_samp: token_stddev_samp_fields
  sum: token_sum_fields
  var_pop: token_var_pop_fields
  var_samp: token_var_samp_fields
  variance: token_variance_fields
}

"""aggregate avg on columns"""
type token_avg_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
Boolean expression to filter rows from the table "token". All fields are combined with a logical 'AND'.
"""
input token_bool_exp {
  _and: [token_bool_exp!]
  _not: token_bool_exp
  _or: [token_bool_exp!]
  contract: String_comparison_exp
  creator: String_comparison_exp
  db_updated_at: timestamptz_comparison_exp
  deleted: Boolean_comparison_exp
  id: uuid_comparison_exp
  minted: numeric_comparison_exp
  minted_at: timestamptz_comparison_exp
  supply: numeric_comparison_exp
  token_id: String_comparison_exp
  tzkt_id: bigint_comparison_exp
  updated: timestamptz_comparison_exp
}

"""
columns and relationships of "token_by_owner"
"""
type token_by_owner {
  contract: String
  creator: String
  db_updated_at: timestamptz
  deleted: Boolean
  id: uuid
  item_id: String
  minted: numeric
  minted_at: timestamptz
  owner: String
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""
aggregated selection of "token_by_owner"
"""
type token_by_owner_aggregate {
  aggregate: token_by_owner_aggregate_fields
  nodes: [token_by_owner!]!
}

"""
aggregate fields of "token_by_owner"
"""
type token_by_owner_aggregate_fields {
  avg: token_by_owner_avg_fields
  count(columns: [token_by_owner_select_column!], distinct: Boolean): Int!
  max: token_by_owner_max_fields
  min: token_by_owner_min_fields
  stddev: token_by_owner_stddev_fields
  stddev_pop: token_by_owner_stddev_pop_fields
  stddev_samp: token_by_owner_stddev_samp_fields
  sum: token_by_owner_sum_fields
  var_pop: token_by_owner_var_pop_fields
  var_samp: token_by_owner_var_samp_fields
  variance: token_by_owner_variance_fields
}

"""aggregate avg on columns"""
type token_by_owner_avg_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
Boolean expression to filter rows from the table "token_by_owner". All fields are combined with a logical 'AND'.
"""
input token_by_owner_bool_exp {
  _and: [token_by_owner_bool_exp!]
  _not: token_by_owner_bool_exp
  _or: [token_by_owner_bool_exp!]
  contract: String_comparison_exp
  creator: String_comparison_exp
  db_updated_at: timestamptz_comparison_exp
  deleted: Boolean_comparison_exp
  id: uuid_comparison_exp
  item_id: String_comparison_exp
  minted: numeric_comparison_exp
  minted_at: timestamptz_comparison_exp
  owner: String_comparison_exp
  supply: numeric_comparison_exp
  token_id: String_comparison_exp
  tzkt_id: bigint_comparison_exp
  updated: timestamptz_comparison_exp
}

"""aggregate max on columns"""
type token_by_owner_max_fields {
  contract: String
  creator: String
  db_updated_at: timestamptz
  id: uuid
  item_id: String
  minted: numeric
  minted_at: timestamptz
  owner: String
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate min on columns"""
type token_by_owner_min_fields {
  contract: String
  creator: String
  db_updated_at: timestamptz
  id: uuid
  item_id: String
  minted: numeric
  minted_at: timestamptz
  owner: String
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""Ordering options when selecting data from "token_by_owner"."""
input token_by_owner_order_by {
  contract: order_by
  creator: order_by
  db_updated_at: order_by
  deleted: order_by
  id: order_by
  item_id: order_by
  minted: order_by
  minted_at: order_by
  owner: order_by
  supply: order_by
  token_id: order_by
  tzkt_id: order_by
  updated: order_by
}

"""
select columns of table "token_by_owner"
"""
enum token_by_owner_select_column {
  """column name"""
  contract

  """column name"""
  creator

  """column name"""
  db_updated_at

  """column name"""
  deleted

  """column name"""
  id

  """column name"""
  item_id

  """column name"""
  minted

  """column name"""
  minted_at

  """column name"""
  owner

  """column name"""
  supply

  """column name"""
  token_id

  """column name"""
  tzkt_id

  """column name"""
  updated
}

"""aggregate stddev on columns"""
type token_by_owner_stddev_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate stddev_pop on columns"""
type token_by_owner_stddev_pop_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate stddev_samp on columns"""
type token_by_owner_stddev_samp_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
Streaming cursor of the table "token_by_owner"
"""
input token_by_owner_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: token_by_owner_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input token_by_owner_stream_cursor_value_input {
  contract: String
  creator: String
  db_updated_at: timestamptz
  deleted: Boolean
  id: uuid
  item_id: String
  minted: numeric
  minted_at: timestamptz
  owner: String
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate sum on columns"""
type token_by_owner_sum_fields {
  minted: numeric
  supply: numeric
  tzkt_id: bigint
}

"""aggregate var_pop on columns"""
type token_by_owner_var_pop_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate var_samp on columns"""
type token_by_owner_var_samp_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate variance on columns"""
type token_by_owner_variance_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
unique or primary key constraints on table "token"
"""
enum token_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  token_pkey
}

"""
input type for incrementing numeric columns in table "token"
"""
input token_inc_input {
  minted: numeric
  supply: numeric
  tzkt_id: bigint
}

"""
input type for inserting data into table "token"
"""
input token_insert_input {
  contract: String
  creator: String
  db_updated_at: timestamptz
  deleted: Boolean
  id: uuid
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate max on columns"""
type token_max_fields {
  contract: String
  creator: String
  db_updated_at: timestamptz
  id: uuid
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate min on columns"""
type token_min_fields {
  contract: String
  creator: String
  db_updated_at: timestamptz
  id: uuid
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""
response of any mutation on the table "token"
"""
type token_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [token!]!
}

"""
on_conflict condition type for table "token"
"""
input token_on_conflict {
  constraint: token_constraint!
  update_columns: [token_update_column!]! = []
  where: token_bool_exp
}

"""Ordering options when selecting data from "token"."""
input token_order_by {
  contract: order_by
  creator: order_by
  db_updated_at: order_by
  deleted: order_by
  id: order_by
  minted: order_by
  minted_at: order_by
  supply: order_by
  token_id: order_by
  tzkt_id: order_by
  updated: order_by
}

"""primary key columns input for table: token"""
input token_pk_columns_input {
  id: uuid!
}

"""
select columns of table "token"
"""
enum token_select_column {
  """column name"""
  contract

  """column name"""
  creator

  """column name"""
  db_updated_at

  """column name"""
  deleted

  """column name"""
  id

  """column name"""
  minted

  """column name"""
  minted_at

  """column name"""
  supply

  """column name"""
  token_id

  """column name"""
  tzkt_id

  """column name"""
  updated
}

"""
input type for updating data in table "token"
"""
input token_set_input {
  contract: String
  creator: String
  db_updated_at: timestamptz
  deleted: Boolean
  id: uuid
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate stddev on columns"""
type token_stddev_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate stddev_pop on columns"""
type token_stddev_pop_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate stddev_samp on columns"""
type token_stddev_samp_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
Streaming cursor of the table "token"
"""
input token_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: token_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input token_stream_cursor_value_input {
  contract: String
  creator: String
  db_updated_at: timestamptz
  deleted: Boolean
  id: uuid
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate sum on columns"""
type token_sum_fields {
  minted: numeric
  supply: numeric
  tzkt_id: bigint
}

"""
columns and relationships of "token_transfer"
"""
type token_transfer {
  amount: numeric!
  contract: String!
  date: timestamptz!
  db_updated_at: timestamptz
  from_address: String
  hash: String
  id: bigint!
  to_address: String
  token_id: String!

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String!
  tzkt_origination_id: bigint
  tzkt_token_id: bigint!
  tzkt_transaction_id: bigint
}

"""
aggregated selection of "token_transfer"
"""
type token_transfer_aggregate {
  aggregate: token_transfer_aggregate_fields
  nodes: [token_transfer!]!
}

"""
aggregate fields of "token_transfer"
"""
type token_transfer_aggregate_fields {
  avg: token_transfer_avg_fields
  count(columns: [token_transfer_select_column!], distinct: Boolean): Int!
  max: token_transfer_max_fields
  min: token_transfer_min_fields
  stddev: token_transfer_stddev_fields
  stddev_pop: token_transfer_stddev_pop_fields
  stddev_samp: token_transfer_stddev_samp_fields
  sum: token_transfer_sum_fields
  var_pop: token_transfer_var_pop_fields
  var_samp: token_transfer_var_samp_fields
  variance: token_transfer_variance_fields
}

"""aggregate avg on columns"""
type token_transfer_avg_fields {
  amount: Float
  id: Float
  tzkt_origination_id: Float
  tzkt_token_id: Float
  tzkt_transaction_id: Float
}

"""
Boolean expression to filter rows from the table "token_transfer". All fields are combined with a logical 'AND'.
"""
input token_transfer_bool_exp {
  _and: [token_transfer_bool_exp!]
  _not: token_transfer_bool_exp
  _or: [token_transfer_bool_exp!]
  amount: numeric_comparison_exp
  contract: String_comparison_exp
  date: timestamptz_comparison_exp
  db_updated_at: timestamptz_comparison_exp
  from_address: String_comparison_exp
  hash: String_comparison_exp
  id: bigint_comparison_exp
  to_address: String_comparison_exp
  token_id: String_comparison_exp
  type: String_comparison_exp
  tzkt_origination_id: bigint_comparison_exp
  tzkt_token_id: bigint_comparison_exp
  tzkt_transaction_id: bigint_comparison_exp
}

"""
unique or primary key constraints on table "token_transfer"
"""
enum token_transfer_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  token_transfer_pkey
}

"""
input type for incrementing numeric columns in table "token_transfer"
"""
input token_transfer_inc_input {
  amount: numeric
  id: bigint
  tzkt_origination_id: bigint
  tzkt_token_id: bigint
  tzkt_transaction_id: bigint
}

"""
input type for inserting data into table "token_transfer"
"""
input token_transfer_insert_input {
  amount: numeric
  contract: String
  date: timestamptz
  db_updated_at: timestamptz
  from_address: String
  hash: String
  id: bigint
  to_address: String
  token_id: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
  tzkt_origination_id: bigint
  tzkt_token_id: bigint
  tzkt_transaction_id: bigint
}

"""aggregate max on columns"""
type token_transfer_max_fields {
  amount: numeric
  contract: String
  date: timestamptz
  db_updated_at: timestamptz
  from_address: String
  hash: String
  id: bigint
  to_address: String
  token_id: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
  tzkt_origination_id: bigint
  tzkt_token_id: bigint
  tzkt_transaction_id: bigint
}

"""aggregate min on columns"""
type token_transfer_min_fields {
  amount: numeric
  contract: String
  date: timestamptz
  db_updated_at: timestamptz
  from_address: String
  hash: String
  id: bigint
  to_address: String
  token_id: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
  tzkt_origination_id: bigint
  tzkt_token_id: bigint
  tzkt_transaction_id: bigint
}

"""
response of any mutation on the table "token_transfer"
"""
type token_transfer_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [token_transfer!]!
}

"""
on_conflict condition type for table "token_transfer"
"""
input token_transfer_on_conflict {
  constraint: token_transfer_constraint!
  update_columns: [token_transfer_update_column!]! = []
  where: token_transfer_bool_exp
}

"""Ordering options when selecting data from "token_transfer"."""
input token_transfer_order_by {
  amount: order_by
  contract: order_by
  date: order_by
  db_updated_at: order_by
  from_address: order_by
  hash: order_by
  id: order_by
  to_address: order_by
  token_id: order_by
  type: order_by
  tzkt_origination_id: order_by
  tzkt_token_id: order_by
  tzkt_transaction_id: order_by
}

"""primary key columns input for table: token_transfer"""
input token_transfer_pk_columns_input {
  id: bigint!
}

"""
select columns of table "token_transfer"
"""
enum token_transfer_select_column {
  """column name"""
  amount

  """column name"""
  contract

  """column name"""
  date

  """column name"""
  db_updated_at

  """column name"""
  from_address

  """column name"""
  hash

  """column name"""
  id

  """column name"""
  to_address

  """column name"""
  token_id

  """column name"""
  type

  """column name"""
  tzkt_origination_id

  """column name"""
  tzkt_token_id

  """column name"""
  tzkt_transaction_id
}

"""
input type for updating data in table "token_transfer"
"""
input token_transfer_set_input {
  amount: numeric
  contract: String
  date: timestamptz
  db_updated_at: timestamptz
  from_address: String
  hash: String
  id: bigint
  to_address: String
  token_id: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
  tzkt_origination_id: bigint
  tzkt_token_id: bigint
  tzkt_transaction_id: bigint
}

"""aggregate stddev on columns"""
type token_transfer_stddev_fields {
  amount: Float
  id: Float
  tzkt_origination_id: Float
  tzkt_token_id: Float
  tzkt_transaction_id: Float
}

"""aggregate stddev_pop on columns"""
type token_transfer_stddev_pop_fields {
  amount: Float
  id: Float
  tzkt_origination_id: Float
  tzkt_token_id: Float
  tzkt_transaction_id: Float
}

"""aggregate stddev_samp on columns"""
type token_transfer_stddev_samp_fields {
  amount: Float
  id: Float
  tzkt_origination_id: Float
  tzkt_token_id: Float
  tzkt_transaction_id: Float
}

"""
Streaming cursor of the table "token_transfer"
"""
input token_transfer_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: token_transfer_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input token_transfer_stream_cursor_value_input {
  amount: numeric
  contract: String
  date: timestamptz
  db_updated_at: timestamptz
  from_address: String
  hash: String
  id: bigint
  to_address: String
  token_id: String

  """
  GET_BID: GET_BID\nGET_FLOOR_BID: GET_FLOOR_BID\nORDER_LIST: LIST\nORDER_MATCH: SELL\nORDER_CANCEL: CANCEL_LIST\nCANCEL_BID: CANCEL_BID\nCANCEL_FLOOR_BID: CANCEL_FLOOR_BID\nMAKE_BID: MAKE_BID\nMAKE_FLOOR_BID: MAKE_FLOOR_BID\nTOKEN_MINT: MINT\nTOKEN_TRANSFER: TRANSFER\nTOKEN_BURN: BURN
  """
  type: String
  tzkt_origination_id: bigint
  tzkt_token_id: bigint
  tzkt_transaction_id: bigint
}

"""aggregate sum on columns"""
type token_transfer_sum_fields {
  amount: numeric
  id: bigint
  tzkt_origination_id: bigint
  tzkt_token_id: bigint
  tzkt_transaction_id: bigint
}

"""
update columns of table "token_transfer"
"""
enum token_transfer_update_column {
  """column name"""
  amount

  """column name"""
  contract

  """column name"""
  date

  """column name"""
  db_updated_at

  """column name"""
  from_address

  """column name"""
  hash

  """column name"""
  id

  """column name"""
  to_address

  """column name"""
  token_id

  """column name"""
  type

  """column name"""
  tzkt_origination_id

  """column name"""
  tzkt_token_id

  """column name"""
  tzkt_transaction_id
}

input token_transfer_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: token_transfer_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: token_transfer_set_input

  """filter the rows which have to be updated"""
  where: token_transfer_bool_exp!
}

"""aggregate var_pop on columns"""
type token_transfer_var_pop_fields {
  amount: Float
  id: Float
  tzkt_origination_id: Float
  tzkt_token_id: Float
  tzkt_transaction_id: Float
}

"""aggregate var_samp on columns"""
type token_transfer_var_samp_fields {
  amount: Float
  id: Float
  tzkt_origination_id: Float
  tzkt_token_id: Float
  tzkt_transaction_id: Float
}

"""aggregate variance on columns"""
type token_transfer_variance_fields {
  amount: Float
  id: Float
  tzkt_origination_id: Float
  tzkt_token_id: Float
  tzkt_transaction_id: Float
}

"""
update columns of table "token"
"""
enum token_update_column {
  """column name"""
  contract

  """column name"""
  creator

  """column name"""
  db_updated_at

  """column name"""
  deleted

  """column name"""
  id

  """column name"""
  minted

  """column name"""
  minted_at

  """column name"""
  supply

  """column name"""
  token_id

  """column name"""
  tzkt_id

  """column name"""
  updated
}

input token_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: token_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: token_set_input

  """filter the rows which have to be updated"""
  where: token_bool_exp!
}

"""aggregate var_pop on columns"""
type token_var_pop_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate var_samp on columns"""
type token_var_samp_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate variance on columns"""
type token_variance_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
columns and relationships of "token_with_meta"
"""
type token_with_meta {
  contract: String
  creator: String
  deleted: Boolean
  id: uuid
  metadata: String
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""
aggregated selection of "token_with_meta"
"""
type token_with_meta_aggregate {
  aggregate: token_with_meta_aggregate_fields
  nodes: [token_with_meta!]!
}

"""
aggregate fields of "token_with_meta"
"""
type token_with_meta_aggregate_fields {
  avg: token_with_meta_avg_fields
  count(columns: [token_with_meta_select_column!], distinct: Boolean): Int!
  max: token_with_meta_max_fields
  min: token_with_meta_min_fields
  stddev: token_with_meta_stddev_fields
  stddev_pop: token_with_meta_stddev_pop_fields
  stddev_samp: token_with_meta_stddev_samp_fields
  sum: token_with_meta_sum_fields
  var_pop: token_with_meta_var_pop_fields
  var_samp: token_with_meta_var_samp_fields
  variance: token_with_meta_variance_fields
}

"""aggregate avg on columns"""
type token_with_meta_avg_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
Boolean expression to filter rows from the table "token_with_meta". All fields are combined with a logical 'AND'.
"""
input token_with_meta_bool_exp {
  _and: [token_with_meta_bool_exp!]
  _not: token_with_meta_bool_exp
  _or: [token_with_meta_bool_exp!]
  contract: String_comparison_exp
  creator: String_comparison_exp
  deleted: Boolean_comparison_exp
  id: uuid_comparison_exp
  metadata: String_comparison_exp
  minted: numeric_comparison_exp
  minted_at: timestamptz_comparison_exp
  supply: numeric_comparison_exp
  token_id: String_comparison_exp
  tzkt_id: bigint_comparison_exp
  updated: timestamptz_comparison_exp
}

"""aggregate max on columns"""
type token_with_meta_max_fields {
  contract: String
  creator: String
  id: uuid
  metadata: String
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate min on columns"""
type token_with_meta_min_fields {
  contract: String
  creator: String
  id: uuid
  metadata: String
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""Ordering options when selecting data from "token_with_meta"."""
input token_with_meta_order_by {
  contract: order_by
  creator: order_by
  deleted: order_by
  id: order_by
  metadata: order_by
  minted: order_by
  minted_at: order_by
  supply: order_by
  token_id: order_by
  tzkt_id: order_by
  updated: order_by
}

"""
select columns of table "token_with_meta"
"""
enum token_with_meta_select_column {
  """column name"""
  contract

  """column name"""
  creator

  """column name"""
  deleted

  """column name"""
  id

  """column name"""
  metadata

  """column name"""
  minted

  """column name"""
  minted_at

  """column name"""
  supply

  """column name"""
  token_id

  """column name"""
  tzkt_id

  """column name"""
  updated
}

"""aggregate stddev on columns"""
type token_with_meta_stddev_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate stddev_pop on columns"""
type token_with_meta_stddev_pop_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate stddev_samp on columns"""
type token_with_meta_stddev_samp_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
Streaming cursor of the table "token_with_meta"
"""
input token_with_meta_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: token_with_meta_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input token_with_meta_stream_cursor_value_input {
  contract: String
  creator: String
  deleted: Boolean
  id: uuid
  metadata: String
  minted: numeric
  minted_at: timestamptz
  supply: numeric
  token_id: String
  tzkt_id: bigint
  updated: timestamptz
}

"""aggregate sum on columns"""
type token_with_meta_sum_fields {
  minted: numeric
  supply: numeric
  tzkt_id: bigint
}

"""aggregate var_pop on columns"""
type token_with_meta_var_pop_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate var_samp on columns"""
type token_with_meta_var_samp_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""aggregate variance on columns"""
type token_with_meta_variance_fields {
  minted: Float
  supply: Float
  tzkt_id: Float
}

"""
columns and relationships of "tzprofiles"
"""
type tzprofiles {
  account: String!
  alias: String
  contract: String!
  description: String
  discord: String
  domain_name: String
  errored: Boolean!
  ethereum: String
  github: String
  invalid_claims(
    """JSON select path"""
    path: String
  ): jsonb!
  logo: String
  twitter: String
  valid_claims(
    """JSON select path"""
    path: String
  ): jsonb!
  website: String
}

"""
aggregated selection of "tzprofiles"
"""
type tzprofiles_aggregate {
  aggregate: tzprofiles_aggregate_fields
  nodes: [tzprofiles!]!
}

"""
aggregate fields of "tzprofiles"
"""
type tzprofiles_aggregate_fields {
  count(columns: [tzprofiles_select_column!], distinct: Boolean): Int!
  max: tzprofiles_max_fields
  min: tzprofiles_min_fields
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input tzprofiles_append_input {
  invalid_claims: jsonb
  valid_claims: jsonb
}

"""
Boolean expression to filter rows from the table "tzprofiles". All fields are combined with a logical 'AND'.
"""
input tzprofiles_bool_exp {
  _and: [tzprofiles_bool_exp!]
  _not: tzprofiles_bool_exp
  _or: [tzprofiles_bool_exp!]
  account: String_comparison_exp
  alias: String_comparison_exp
  contract: String_comparison_exp
  description: String_comparison_exp
  discord: String_comparison_exp
  domain_name: String_comparison_exp
  errored: Boolean_comparison_exp
  ethereum: String_comparison_exp
  github: String_comparison_exp
  invalid_claims: jsonb_comparison_exp
  logo: String_comparison_exp
  twitter: String_comparison_exp
  valid_claims: jsonb_comparison_exp
  website: String_comparison_exp
}

"""
unique or primary key constraints on table "tzprofiles"
"""
enum tzprofiles_constraint {
  """
  unique or primary key constraint on columns "account"
  """
  tzprofiles_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input tzprofiles_delete_at_path_input {
  invalid_claims: [String!]
  valid_claims: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input tzprofiles_delete_elem_input {
  invalid_claims: Int
  valid_claims: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input tzprofiles_delete_key_input {
  invalid_claims: String
  valid_claims: String
}

"""
input type for inserting data into table "tzprofiles"
"""
input tzprofiles_insert_input {
  account: String
  alias: String
  contract: String
  description: String
  discord: String
  domain_name: String
  errored: Boolean
  ethereum: String
  github: String
  invalid_claims: jsonb
  logo: String
  twitter: String
  valid_claims: jsonb
  website: String
}

"""aggregate max on columns"""
type tzprofiles_max_fields {
  account: String
  alias: String
  contract: String
  description: String
  discord: String
  domain_name: String
  ethereum: String
  github: String
  logo: String
  twitter: String
  website: String
}

"""aggregate min on columns"""
type tzprofiles_min_fields {
  account: String
  alias: String
  contract: String
  description: String
  discord: String
  domain_name: String
  ethereum: String
  github: String
  logo: String
  twitter: String
  website: String
}

"""
response of any mutation on the table "tzprofiles"
"""
type tzprofiles_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [tzprofiles!]!
}

"""
on_conflict condition type for table "tzprofiles"
"""
input tzprofiles_on_conflict {
  constraint: tzprofiles_constraint!
  update_columns: [tzprofiles_update_column!]! = []
  where: tzprofiles_bool_exp
}

"""Ordering options when selecting data from "tzprofiles"."""
input tzprofiles_order_by {
  account: order_by
  alias: order_by
  contract: order_by
  description: order_by
  discord: order_by
  domain_name: order_by
  errored: order_by
  ethereum: order_by
  github: order_by
  invalid_claims: order_by
  logo: order_by
  twitter: order_by
  valid_claims: order_by
  website: order_by
}

"""primary key columns input for table: tzprofiles"""
input tzprofiles_pk_columns_input {
  account: String!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input tzprofiles_prepend_input {
  invalid_claims: jsonb
  valid_claims: jsonb
}

"""
select columns of table "tzprofiles"
"""
enum tzprofiles_select_column {
  """column name"""
  account

  """column name"""
  alias

  """column name"""
  contract

  """column name"""
  description

  """column name"""
  discord

  """column name"""
  domain_name

  """column name"""
  errored

  """column name"""
  ethereum

  """column name"""
  github

  """column name"""
  invalid_claims

  """column name"""
  logo

  """column name"""
  twitter

  """column name"""
  valid_claims

  """column name"""
  website
}

"""
input type for updating data in table "tzprofiles"
"""
input tzprofiles_set_input {
  account: String
  alias: String
  contract: String
  description: String
  discord: String
  domain_name: String
  errored: Boolean
  ethereum: String
  github: String
  invalid_claims: jsonb
  logo: String
  twitter: String
  valid_claims: jsonb
  website: String
}

"""
Streaming cursor of the table "tzprofiles"
"""
input tzprofiles_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: tzprofiles_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input tzprofiles_stream_cursor_value_input {
  account: String
  alias: String
  contract: String
  description: String
  discord: String
  domain_name: String
  errored: Boolean
  ethereum: String
  github: String
  invalid_claims: jsonb
  logo: String
  twitter: String
  valid_claims: jsonb
  website: String
}

"""
update columns of table "tzprofiles"
"""
enum tzprofiles_update_column {
  """column name"""
  account

  """column name"""
  alias

  """column name"""
  contract

  """column name"""
  description

  """column name"""
  discord

  """column name"""
  domain_name

  """column name"""
  errored

  """column name"""
  ethereum

  """column name"""
  github

  """column name"""
  invalid_claims

  """column name"""
  logo

  """column name"""
  twitter

  """column name"""
  valid_claims

  """column name"""
  website
}

input tzprofiles_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: tzprofiles_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: tzprofiles_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: tzprofiles_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: tzprofiles_delete_key_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: tzprofiles_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: tzprofiles_set_input

  """filter the rows which have to be updated"""
  where: tzprofiles_bool_exp!
}

scalar uuid

"""
Boolean expression to compare columns of type "uuid". All fields are combined with logical 'AND'.
"""
input uuid_comparison_exp {
  _eq: uuid
  _gt: uuid
  _gte: uuid
  _in: [uuid!]
  _is_null: Boolean
  _lt: uuid
  _lte: uuid
  _neq: uuid
  _nin: [uuid!]
}

